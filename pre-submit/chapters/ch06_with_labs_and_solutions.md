---
title: "Reinforcement Learning for Search from Scratch - Chapter 6 (with Labs & Solutions)"
author: "Vlad Prytula"
---

<!-- AUTOGENERATED: ch06 bundle from docs/book/ch06 (incl. labs + available solutions) -->

# Chapter 6 --- Discrete Template Bandits: When Theory Meets Practice

## From Relevance to Optimization: The First RL Agent

In Chapter 5, we built the complete RL interface: base relevance models provide initial rankings, feature engineering extracts state representations, and multi-objective rewards aggregate business metrics. We have everything needed to train an RL agent---except the agent itself.

**The challenge:**

We need a policy $\pi: \mathcal{X} \to \mathcal{A}$ that maps observations (user, query, products) to actions (boost vectors) while:

1. **Maximizing business metrics** (GMV, CM2, strategic KPIs)
2. **Respecting hard constraints** (profitability floor $\mathbb{E}[\text{CM2}] \ge \tau_{\text{cm2}}$, rank stability, exposure floors)
3. **Exploring safely** (avoid catastrophic rankings during learning)
4. **Remaining interpretable** (business stakeholders must understand what the agent does)
5. **Learning quickly** (sample efficiency matters in production)

**Why not jump straight to deep RL?**

Modern deep RL (DQN, PPO, SAC) offers flexibility but comes with serious risks for production search:

- Sample inefficiency: Often requires millions of episodes to reach stable performance
- Unsafe exploration: Random actions can destroy user experience
- Limited interpretability: Neural policies are opaque to stakeholders
- Training instability: Learning curves oscillate; hyperparameters are fragile
- Cold start: Hard to warm-start from domain knowledge

**Solution: Start with discrete template bandits.**

Instead of learning a full neural policy from scratch, we **discretize the action space** into interpretable templates and use **contextual bandits** (LinUCB, Thompson Sampling) to select among them.

**Key insight:**

Search boost optimization is **not** a complex sequential decision problem initially. A single-episode contextual bandit perspective suffices because:

- Most search sessions are **single-query** (user searches once, clicks/buys, leaves)
- Inter-session effects (retention, long-term value) are **slow-moving** (timescale of days, not seconds)
- We already have a **strong base ranker** (Chapter 5); RL learns **bounded perturbations** whose authority we control via $a_{\max}$

---

## What This Chapter Teaches

This chapter is structured around a deliberately chosen negative result. We develop the theory and implementation of linear contextual bandits over an interpretable, discretized action space (templates). We then run the resulting algorithms under an intentionally impoverished feature map and observe that they underperform a strong static baseline. We diagnose the failure by tracing it to violated assumptions (feature poverty and model misspecification), and we recover by engineering richer features.

Concretely, we proceed in five stages:

**Stage I: Theory (Sections 6.1--6.3)**
- We define a discrete template action space encoding business logic.
- We derive and implement Thompson Sampling and LinUCB for linear contextual bandits.
- We state regret guarantees (sublinear in $T$) under explicit assumptions.

**Stage II: Failure (Section 6.5)**
- We run both bandits with $\phi_{\text{simple}}$ (segment + query type).
- We observe that both algorithms underperform the best static template on GMV.

**Stage III: Diagnosis (Section 6.6)**
- We rule out implementation errors and revisit the assumptions behind the theorems.
- We identify the bottleneck as representation: the feature map is too weak for a linear model.

**Stage IV: Fix (Section 6.7)**
- We enrich the feature map to include user preference signals and product aggregates.
- We compare oracle vs. estimated preference features and extract an algorithm-selection rule.

**Stage V: Synthesis (Section 6.8)**
- We summarize what regret bounds do (and do not) guarantee.
- We connect the template bandit construction to Chapter 7 (continuous actions and function approximation).

---

## Why We Show the Failure

We could begin with rich features and present only the successful result. We do not, because it would hide the main methodological lesson: theoretical guarantees are conditional, and in applied RL the failure mode is often representational rather than algorithmic.

The goal is to practice a transferable diagnostic skill:

1. Check theorem hypotheses against the actual system.
2. Distinguish feature poverty from insufficient data or hyperparameter issues.
3. Recognize model misspecification (here: a linear model applied to a nonlinear reward mechanism).
4. Fix the bottleneck rather than escalating algorithmic complexity.

---

## Chapter Roadmap

**Part I: Theory & Implementation**

Section 6.1 --- **Discrete Template Action Space**: Define 8 interpretable boost strategies (Positive CM2, Private Label, Premium, Budget, etc.)

Section 6.2 --- **Thompson Sampling**: Bayesian posterior sampling with Gaussian conjugacy and ridge regression

Section 6.3 --- **LinUCB**: Upper confidence bounds with confidence ellipsoids in feature space

Section 6.4 --- **Production Code**: Full PyTorch implementation with type hints, batching, reproducibility

**Part II: The Empirical Journey**

Section 6.5 --- **First Experiment (Simple Features)**: Deploy bandits with segment + query type -> **28% GMV loss**

Section 6.6 --- **Diagnosis**: Identify feature poverty, model misspecification, and nonlinearity

Section 6.7 --- **Rich Features (Oracle vs. Estimated)**: Re-engineer features, discover the Algorithm Selection Principle

Section 6.8 --- **Summary & What's Next**: Lessons learned, limitations, bridge to continuous Q(x,a) in Chapter 7

**Part III: Reflection & Extensions**

Section 6.8.1 --- **What We Built**: Technical artifacts and empirical results

Section 6.8.2 --- **Five Lessons**: Conditional guarantees, feature engineering ceiling, baseline value, failure as signal, algorithm selection

Section 6.8.3 --- **Where to Go Next**: Exercises, labs, and GPU scaling

Section 6.8.4 --- **Extensions & Practice**: Appendices covering neural extensions, theory-practice gaps, modern context, production checklists

---

## What We Build

By the end of this chapter, we have:

**Technical artifacts:**
- A library of 8 discrete boost templates encoding business logic.
- Thompson Sampling with Bayesian ridge regression updates.
- LinUCB with upper confidence bounds and confidence ellipsoids.
- Reproducible experiment drivers and per-segment diagnostics.

**Empirical understanding:**
- A reproducible failure mode with an impoverished feature map.
- A diagnosis in terms of violated assumptions (representation and misspecification).
- A recovery with richer features and a clear algorithm-selection takeaway.

**Production skills:**
- Translating theorem hypotheses into concrete system checks.
- Distinguishing algorithmic issues from representation bottlenecks.
- Designing experiments that isolate feature quality, priors, and exploration.

This chapter is an honest account of what happens when we apply RL theory to a realistic simulator, including failures, diagnoses, and the corresponding fixes.

---

*Next: Section 6.1 --- Discrete Template Action Space*

---

## 6.1 Discrete Template Action Space

### 6.1.1 Why Discretize?

The continuous action space from Chapter 1 is $\mathcal{A} = [-a_{\max}, +a_{\max}]^K$ where $K$ is the number of products displayed (typically $K=20$ or $K=50$). This is **high-dimensional** ($\dim \mathcal{A} = K$) and **unbounded exploration** is dangerous.

**Problems with continuous boosts:**

1. **Curse of dimensionality**: With $K=20$ and even 10 discretization levels per dimension, we would have $10^{20}$ actions---intractable.

2. **Unsafe exploration**: Random continuous boosts can produce nonsensical rankings:
   - Boosting all products by $+10$ -> no relative change (wasted action)
   - Boosting random products -> destroys relevance (user sees cat food for dog query)

3. **No structure**: Continuous space does not encode domain knowledge about *what kinds of boosts make business sense*.

**Solution: Discretize into interpretable templates.**

We define a small set of **boost templates** $\mathcal{T} = \{t_1, \ldots, t_M\}$ where each template $t_i$ is a **boost policy** that maps products to adjustments based on business logic.

**Definition 6.1.1** (Boost Template) {#DEF-6.1.1}

Let $\mathcal{C}$ denote the **product catalog**, modeled as a finite set of products. A **boost template** is a function
$$
t: \mathcal{C} \to [-a_{\max}, +a_{\max}]
$$
that assigns a bounded boost value to each product $p \in \mathcal{C}$. Given a query result set $\{p_1, \ldots, p_K\} \subset \mathcal{C}$ and base relevance scores $s_{\text{base}}(q, p_i)$ from [DEF-5.1], the template induces **adjusted scores**:
$$
s'_i = s_{\text{base}}(q, p_i) + t(p_i), \quad i = 1, \ldots, K
\tag{6.1}
$$
{#EQ-6.1}

The final ranking is obtained by sorting products in descending order of $s'_i$.

**Properties:**

1. **Boundedness**: $|t(p)| \leq a_{\max}$ for all $p \in \mathcal{C}$ (the scale of $a_{\max}$ is a hyperparameter calibrated relative to the typical variation of $s_{\text{base}}$; cf. the unit note below)
2. **Finite catalog**: $|\mathcal{C}| < \infty$, so all argmax operations over $\mathcal{C}$ are well-defined
3. **Deterministic**: Given a fixed catalog and template definition, $t$ is a fixed function (no internal randomness)
4. **Product-only (baseline library)**: In this chapter, templates depend only on product attributes $(p.\text{category}, p.\text{cm2}, p.\text{is\_pl}, p.\text{price}, p.\text{discount}, p.\text{bestseller}, p.\text{strategic\_flag}, \ldots)$, not on query or user---context enters later via the contextual bandit policy.

This formulation aligns with the Bandit Bellman Operator defined in [DEF-3.8.1], where $\gamma = 0$ eliminates the need for temporal credit assignment. Templates become arms in a contextual bandit; the learner's job is to discover which arm maximizes immediate reward given the context.

**Example 6.1** (Template Application)

Consider a tiny catalog with three products:

| Product | Price | CM2 | Category | Base Score |
|---------|-------|--------|----------|------------|
| $p_1$   | 15    | 1.8    | Dog Food | 0.85       |
| $p_2$   | 40    | -0.3   | Cat Toy  | 0.72       |
| $p_3$   | 25    | 0.9    | Treats   | 0.68       |

**Baseline ranking** (by base score): $[p_1, p_2, p_3]$.

Apply template $t_1$ (**Positive CM2**): boost products with $\text{CM2} > 0.4$ by $+5.0$.

- $t_1(p_1) = 5.0$ ($\text{CM2}(p_1) = 1.8 > 0.4$)
- $t_1(p_2) = 0.0$ ($\text{CM2}(p_2) = -0.3 \leq 0.4$)
- $t_1(p_3) = 5.0$ ($\text{CM2}(p_3) = 0.9 > 0.4$)

**Adjusted scores:**
- $s'_1 = 0.85 + 5.0 = 5.85$
- $s'_2 = 0.72 + 0.0 = 0.72$
- $s'_3 = 0.68 + 5.0 = 5.68$

**New ranking:** $[p_1, p_3, p_2]$ --- product $p_3$ jumps from position 3 to position 2.

This illustrates the pattern of the whole chapter: templates encode **business logic** (profitability via positive CM2) while retaining the base ranker as a relevance prior. The strength of the intervention is controlled by $a_{\max}$: at small $a_{\max}$ the base ordering dominates; at large $a_{\max}$ the template can override it and leave relevance as a tie-breaker. The contextual bandit will decide **which template** to apply for each user--query context.

**Template library design:**

We define $M$ templates based on business objectives and product features. Before presenting the library, we justify two critical hyperparameters: the number of templates ($M$) and the action magnitude ($a_{\max}$).

**Design Choice: Action Magnitude ($a_{\max} = 5.0$)**

The boost bound $a_{\max}$ determines how aggressively templates can override base relevance. This is a **signal-to-noise calibration**, not a universal constant.

!!! note "Units and scales (simulator defaults)"
    - Base relevance $s_{\text{base}}(q,p)$ is a dimensionless score (Chapter 5). In Lab 2.2 we observe mean $\approx 0.10$, std $\approx 0.22$, min $\approx -0.56$, max $\approx 0.93$ over 100 queries \ensuremath{\times} 100 products; in Chapter 5 top-$K$ examples reach roughly $1.2$.
    - The boost bound $a_{\max}$ lives in the same units as $s_{\text{base}}$ (see [EQ-6.1]). The ratio $a_{\max}/\operatorname{std}(s_{\text{base}})$ is the relevant signal-to-noise calibration.
    - Price/GMV/CM2 are currency-valued; the margin rate is the dimensionless ratio $\text{CM2}/\text{price}$ (we use absolute CM2 thresholds in this chapter; margin-rate guardrails enter in Chapter 10).

As emphasized in the unit note above, $a_{\max}$ is measured in **the same units as $s_{\text{base}}$**: the ratio $a_{\max} / \operatorname{std}(s_{\text{base}})$ determines how strongly templates can override the base ranker.

**Action magnitude regimes:**
- $a_{\max} = 0.5$: **Moderate interventions** (on the order of $2$--$3$ score standard deviations). Templates can reorder a few borderline items while keeping the base ranker dominant. Suitable for conservative control where we trust the base ranker.

- $a_{\max} = 5.0$: **Aggressive interventions** (tens of score standard deviations). Templates can dominate the base ranker and effectively act as a rule-based ranker with relevance as a tie-breaker. This is intentionally **not** a production calibration; we use it in Chapters 6--8 to make learning dynamics visually obvious in short runs.

- $a_{\max} = 10+$: **Extreme interventions**. Rankings become almost entirely template-driven. Useful only for stress-testing counterfactuals ("what if we fully prioritize a business objective?") and for illustrating failure modes when actuation authority is too high.

**Our standardized choice**: We use $a_{\max} = 5.0$ throughout Chapters 6-8 for three reasons:

1. **Pedagogical visibility**: At smaller magnitudes (0.5), learning effects are statistically present but visually obscured by base relevance noise and position bias. At 5.0, we can trace how LinUCB/TS policies discover that profitability- and PL-oriented templates can outperform popularity-based strategies.

2. **Fair algorithm comparison**: All RL methods (discrete templates, continuous Q-learning in Ch7, policy gradients in Ch8) use the same $a_{\max}$ by default, enabling ceteris paribus benchmarking. Early experiments with mismatched magnitudes led to spurious conclusions (Appendix 7.A documents this failure mode).

3. **Exploration of conservative vs. aggressive control**: Readers can experiment with smaller values (Exercise 6.8 explores $a_{\max} \in \{0.5, 2.0, 5.0, 10.0\}$) to study how learning speed and final performance depend on action authority.

**Mathematical perspective**: The boost bound $a_{\max}$ couples to the **effective horizon** of the learning problem. From the gap-independent regret bounds we state in Section 6.2.3 ([THM-6.1] and [THM-6.2]), LinUCB's cumulative regret scales as $O(d\sqrt{M T \log T})$ (up to logarithmic factors), where $d$ is feature dimension and $M$ is the number of templates. This is consistent with the minimax lower bound $\Omega(d\sqrt{T})$ for linear contextual bandits (Appendix D). In gap-dependent refinements (not used in this chapter), larger reward gaps between the best and suboptimal templates can improve constants. In practice, increasing $a_{\max}$ increases the magnitude of template-induced ranking changes (and hence the learnable signal), but it also increases the risk of relevance degradation if template design is poor.

**Implementation alignment**: The configuration system centralizes this choice at `SimulatorConfig.action.a_max` in `zoosim/core/config.py`, ensuring consistency across experiments. All code examples in this chapter inherit this default.

**Design Choice: Why M=8 templates?**

We need to balance business objectives with exploration efficiency. Options:

**Option A: Few templates (M=3-5)**
- Pros: Fast learning (fewer arms to explore); simple interpretation.
- Cons: Limited expressiveness (may miss optimal strategies); hard-codes prior knowledge.

**Option B: Many templates (M=20-50)**
- Pros: Rich strategy space.
- Cons: Slower learning (regret grows as $O(\sqrt{MT})$); overfitting risk (too many options for limited data); harder to debug.

**Option C: Continuous parameterization**
- Pros: Maximum flexibility.
- Cons: Loses interpretability (returns to a black-box); requires gradient-based methods (Chapter 7).

**Our choice: M=8 (moderate library)**

We choose **8 templates** because:
1. **Business coverage**: Covers main levers (profitability/CM2, private label, price sensitivity, popularity, strategic goals)
2. **Regret budget**: With $T=50k$ episodes, $O(\sqrt{8 \cdot 50k}) \approx 630$ samples for confident selection
3. **Interpretability**: Small enough for stakeholders to understand entire strategy space
4. **Extensibility**: Easy to add/remove templates in production via config

This breaks in scenarios where:
- Products have >5 strategic dimensions (Exercise 6.7 explores hierarchical templates)
- Query-specific templates are needed (Exercise 6.9 extends to query-conditional templates)

With these design choices justified, here is our representative library ($M = 8$):

**Template Library:**

| ID | Name | Description | Boost Formula |
|----|------|-------------|---------------|
| $t_0$ | **Neutral** | No adjustment (base ranker only) | $t_0(p) = 0$ |
| $t_1$ | **Positive CM2** | Promote products with $\text{cm2}(p) > 0.4$ | $t_1(p) = a_{\max} \cdot \mathbb{1}(\text{cm2}(p) > 0.4)$ |
| $t_2$ | **Private Label** | Promote own-brand products | $t_2(p) = a_{\max} \cdot \mathbb{1}(\text{is\\_pl}(p))$ |
| $t_3$ | **Popular** | Boost by normalized log-bestseller score | $t_3(p) = a_{\max} \cdot \log(1 + \text{bestseller}(p)) / \log(1 + \text{pop}_{\max})$ |
| $t_4$ | **Premium** | Promote expensive items | $t_4(p) = a_{\max} \cdot \mathbb{1}(\text{price}(p) > p_{75})$ |
| $t_5$ | **Budget** | Promote cheap items | $t_5(p) = a_{\max} \cdot \mathbb{1}(\text{price}(p) < p_{25})$ |
| $t_6$ | **Discount** | Boost discounted products | $t_6(p) = a_{\max} \cdot \min\{\text{discount}(p) / 0.3,\, 1\}$ |
| $t_7$ | **Strategic** | Promote strategic categories | $t_7(p) = a_{\max} \cdot \mathbb{1}(\text{strategic}(p))$ |

**Notation:**
- $\mathbb{1}(\cdot)$ is the indicator function (1 if condition true, 0 otherwise)
- $\text{cm2}(p)$ denotes contribution margin (CM2) per item in the simulator (a currency-valued quantity; not a percentage margin rate)
- $p_{25}, p_{75}$ are the 25th and 75th percentiles of catalog prices
- $\text{bestseller}(p)$ is the simulator's popularity proxy score for product $p$
- $\text{pop}_{\max} = \max_{p \in \mathcal{C}} \text{bestseller}(p)$ is used to normalize the Popular template
- $\text{is\_pl}(p)$ indicates whether product $p$ is own-brand (private label) in the simulator

**Implementation:**

The reference implementation of the template library lives in `zoosim/policies/templates.py`:

```python
"""Discrete boost templates for contextual bandit policies.

Mathematical basis: [DEF-6.1.1] (Boost Template)

Templates define interpretable boost strategies that can be selected
by contextual bandit algorithms (LinUCB, Thompson Sampling).
"""

from dataclasses import dataclass
from typing import Callable, List

import numpy as np
from numpy.typing import NDArray

from zoosim.world.catalog import Product


@dataclass
class BoostTemplate:
    """Single boost template with semantic label.

    Mathematical correspondence: Template $t: \\mathcal{C} \\to \\mathbb{R}$ from [DEF-6.1.1]

    Attributes:
        id: Template identifier (0 to M-1)
        name: Human-readable name (e.g., "Positive CM2")
        description: Business objective description
        boost_fn: Function mapping a Product to a boost value
                  Signature: (product: Product) -> float
                  Output range (by design): [-a_max, +a_max]
    """

    id: int
    name: str
    description: str
    boost_fn: Callable[[Product], float]

    def apply(self, products: List[Product]) -> NDArray[np.float32]:
        """Apply template to list of products.

        Implements [EQ-6.1]: Computes boost vector for products.

        Args:
            products: List of Product instances generated from the catalog

        Returns:
            boosts: Array of shape (len(products),) with boost values
                   Each entry in [-a_max, +a_max]
        """
        return np.array([self.boost_fn(p) for p in products], dtype=np.float32)


def create_standard_templates(
    catalog_stats: dict,
    a_max: float = 5.0,
) -> List[BoostTemplate]:
    """Create standard template library for search ranking.

    Implements the 8-template library from Section 6.1.1.

	    Args:
	        catalog_stats: Dictionary with keys:
	                      - 'price_p25': 25th percentile price
	                      - 'price_p75': 75th percentile price
	                      - 'pop_max': Maximum bestseller score (popularity proxy)
	                      - 'own_brand': Name of own-brand label
	        a_max: Maximum absolute boost value for templates (default 5.0)

    Returns:
        templates: List of M=8 boost templates
    """
    p25 = catalog_stats["price_p25"]
    p75 = catalog_stats["price_p75"]
    pop_max = catalog_stats["pop_max"]
    own_brand = catalog_stats.get("own_brand", "OwnBrand")

    templates = [
        # t0: Neutral (baseline)
        BoostTemplate(
            id=0,
            name="Neutral",
            description="No boost adjustment (base ranker only)",
            boost_fn=lambda p: 0.0,
        ),
        # t1: Positive CM2
        BoostTemplate(
            id=1,
            name="Positive CM2",
            description="Promote products with positive contribution margin (CM2 > 0.4)",
            boost_fn=lambda p: a_max if p.cm2 > 0.4 else 0.0,
        ),
        # t2: Private Label (Own Brand)
        BoostTemplate(
            id=2,
            name="Private Label",
            description="Promote own-brand products",
            boost_fn=lambda p: a_max if p.is_pl else 0.0,
        ),
        # t3: Popular
        BoostTemplate(
            id=3,
            name="Popular",
            description="Boost by normalized log-popularity (bestseller score)",
            boost_fn=lambda p: (
                a_max * np.log(1 + p.bestseller) / np.log(1 + pop_max)
                if pop_max > 0
                else 0.0
            ),
        ),
        # t4: Premium
        BoostTemplate(
            id=4,
            name="Premium",
            description="Promote expensive items (price > 75th percentile)",
            boost_fn=lambda p: a_max if p.price > p75 else 0.0,
        ),
        # t5: Budget
        BoostTemplate(
            id=5,
            name="Budget",
            description="Promote cheap items (price < 25th percentile)",
            boost_fn=lambda p: a_max if p.price < p25 else 0.0,
        ),
        # t6: Discount
        BoostTemplate(
            id=6,
            name="Discount",
            description="Boost discounted products (max discount 30%)",
            boost_fn=lambda p: a_max * min(p.discount / 0.3, 1.0),
        ),
        # t7: Strategic
        BoostTemplate(
            id=7,
            name="Strategic",
            description="Promote strategic categories",
            boost_fn=lambda p: a_max if p.strategic_flag else 0.0,
        ),
    ]

    return templates
```

!!! note "Code <-> Config (Template Library)"
    The template library from Table Section 6.1.1 maps to:
    - Template definitions: `zoosim/policies/templates.py`
    - Catalog statistics: Computed from `SimulatorConfig.catalog` in `zoosim/world/catalog.py`
    - Action bound (continuous weights): `SimulatorConfig.action.a_max` in `zoosim/core/config.py`
    - Template amplitude `a_max` (this section) is a separate hyperparameter, tuned relative to the base relevance score scale.

    To modify templates in experiments, edit `create_standard_templates` or pass custom templates to bandit policies.

**Verification: Template application**

We verify that templates produce the expected boosts on synthetic products:

```python
import numpy as np

# Mock catalog statistics
catalog_stats = {
    'price_p25': 10.0,
    'price_p75': 50.0,
    'pop_max': 1000.0,
    'own_brand': 'Zooplus'
}

# Create template library
templates = create_standard_templates(catalog_stats, a_max=5.0)

# Synthetic product examples (using the same fields as Product)
products = [
    {   # Positive-CM2 private-label product
        "cm2": 0.5, "is_pl": True, "bestseller": 500.0,
        "price": 30.0, "discount": 0.1, "strategic_flag": True,
    },
    {   # Low-margin third-party budget product
        "cm2": 0.2, "is_pl": False, "bestseller": 100.0,
        "price": 5.0, "discount": 0.0, "strategic_flag": False,
    },
    {   # Premium discounted product
        "cm2": 0.35, "is_pl": False, "bestseller": 800.0,
        "price": 60.0, "discount": 0.25, "strategic_flag": False,
    },
]

# Apply each template (treating dicts as lightweight stand-ins for Product)
print("Template boosts per product:")
print("Product:         ", ["Positive-CM2 PL", "Budget 3P", "Premium Disc"])
for template in templates:
    boosts = template.apply([
        Product(
            product_id=i,
            category="dummy",
            price=p["price"],
            cm2=p["cm2"],
            is_pl=p["is_pl"],
            discount=p["discount"],
            bestseller=p["bestseller"],
            embedding=torch.zeros(16),
            strategic_flag=p["strategic_flag"],
        )
        for i, p in enumerate(products)
    ])
    print(f"{template.name:15s}", boosts.round(2))

# Output (representative):
# Template boosts per product:
# Product:          ['Positive-CM2 PL', 'Budget 3P', 'Premium Disc']
# Neutral           [0.   0.   0.  ]
# Positive CM2      [5.   0.   0.  ]
# Private Label     [5.   0.   0.  ]
# Popular           [4.50 3.34 4.84]
# Premium           [0.   0.   5.  ]
# Budget            [0.   5.   0.  ]
# Discount          [1.67 0.   4.17]
# Strategic         [5.   0.   0.  ]
```

**Interpretation:**

- Product 1 (positive-CM2 private label): Gets boosted by Positive CM2, Private Label, Popular, Strategic
- Product 2 (budget third-party): Only Budget and Popular boost it
- Product 3 (premium discounted): Premium, Popular, and Discount boost it

The contextual bandit will **learn which template performs best** for each query-user context.

---

### 6.1.2 Contextual Bandit Formulation

With discrete templates, our RL problem reduces to a **contextual bandit**:

**Definition 6.2** (Stochastic Contextual Bandit) {#DEF-6.2}

A **stochastic contextual bandit** is a tuple $(\mathcal{X}, \mathcal{A}, R, \rho)$ where:

- $\mathcal{X}$ is the **context space** (observations)
- $\mathcal{A} = \{1, \ldots, M\}$ is a finite **action set** (template IDs)
- $R: \mathcal{X} \times \mathcal{A} \times \Omega \to \mathbb{R}$ is a **stochastic reward function** with outcomes $\omega \in \Omega$
- $\rho$ is a **context distribution** over $\mathcal{X}$

**Interaction protocol.** At each episode $t = 1, 2, \ldots, T$:

1. **Context arrival**: Environment samples $x_t \sim \rho$ independently (user, query, product features; i.i.d. assumption)
2. **Action selection**: Agent selects $a_t \in \mathcal{A}$ (template ID), possibly depending on $x_t$ and history $\mathcal{H}_{t-1}$
3. **Reward realization**: Environment samples outcome $\omega_t \sim P(\cdot \mid x_t, a_t)$ and returns reward $r_t = R(x_t, a_t, \omega_t)$
4. **Observation**: Agent observes $(x_t, a_t, r_t)$ and updates its policy

**Assumptions (bandits vs. MDPs):**

1. **i.i.d. contexts**: Contexts $\{x_t\}$ are drawn i.i.d. from $\rho$
2. **Stochastic rewards**: For fixed $(x, a)$, reward randomness enters only through $\omega$
3. **No state transitions**: $x_{t+1}$ is independent of $(x_t, a_t, r_t)$ --- there is no latent Markov state evolving over time

These assumptions formalize the intuition that we treat search sessions as **single-query, independent episodes**. This is **not** a full MDP (Chapter 3). We will relax the independence assumption in Chapter 11 when we model inter-session retention.

**Expected reward and optimal policy:**

Define the **mean reward function**:
$$
\mu(x, a) = \mathbb{E}_{\omega \sim P(\cdot | x, a)}[R(x, a, \omega)]
\tag{6.2}
$$
{#EQ-6.2}

The **optimal policy** is:
$$
\pi^*(x) = \arg\max_{a \in \mathcal{A}} \mu(x, a)
\tag{6.3}
$$
{#EQ-6.3}

**Regret:**

The agent's goal is to minimize **cumulative regret** over $T$ episodes:
$$
\text{Regret}(T) = \sum_{t=1}^T \left[\mu(x_t, \pi^*(x_t)) - \mu(x_t, a_t)\right]
\tag{6.4}
$$
{#EQ-6.4}

Here $\pi^*(x) := \arg\max_{a \in \mathcal{A}} \mu(x, a)$ is the optimal policy *within the template class* $\mathcal{A} = \{1, \ldots, M\}$. If the true optimal policy lies outside this template space (model misspecification), [EQ-6.4] measures regret relative to the *best template*, not the globally optimal action---a distinction we exploit diagnostically in Section 6.6.

**Theorem 6.0** (Minimax Lower Bound for Stochastic Bandits) {#THM-6.0}

For any learning algorithm and any time horizon $T \geq M$, there exists an $M$-armed stochastic bandit instance with Bernoulli rewards such that the expected cumulative regret satisfies:

$$
\mathbb{E}[\mathrm{Regret}(T)] \geq c \sqrt{M T}.
$$

where $c > 0$ is a universal constant (e.g., $c = 1/20$ suffices).

*Proof.* Appendix D states this lower bound for $K$ arms with Bernoulli rewards (Theorem D.3.1). Substituting $K = M$ yields the claim. $\square$

**Remark 6.0.1** (Interpretation and significance). This theorem establishes that no algorithm---however clever---can achieve regret better than $\Omega(\sqrt{MT})$ uniformly over all $M$-armed bandit instances. The upper bounds we prove for Thompson Sampling and LinUCB in this chapter match this rate up to logarithmic factors, which is why they are called "optimal" in the bandit literature. For contextual bandits with feature dimension $d$, the lower bound becomes $\Omega(d\sqrt{T})$ (see Appendix D, [EQ-D.10]); for continuous-action bandits, the relevant complexity measure is the eluder dimension. We return to these extensions when discussing feature richness and model misspecification in Sections 6.5--6.7.

**Why this matters:**

If we could observe $\mu(x, a)$ for all $(x, a)$ pairs, we would pick $\pi^*(x)$ greedily. But $\mu$ is **unknown**---we must learn it from noisy samples while balancing **exploration** (try all templates) vs. **exploitation** (use the best known template).

**Two canonical algorithms:**

We develop two approaches with complementary strengths:

1. **Thompson Sampling (Section 6.2-6.3)**: Bayesian posterior sampling, probabilistic exploration
2. **LinUCB (Section 6.4-6.5)**: Frequentist upper confidence bounds, deterministic exploration

Both achieve sublinear regret under standard assumptions; see [THM-6.1] and [THM-6.2] for representative bounds with explicit dependence on $d$, $M$, and logarithmic factors.

---

## 6.2 Thompson Sampling: Bayesian Exploration

### 6.2.1 The Core Idea

Thompson Sampling (TS) is simple: we sample from the posterior belief about which action is best, then take that action.

**Bayesian framework:**

We maintain a probability distribution $p(a \text{ is optimal} \mid \mathcal{H}_t)$ where $\mathcal{H}_t = \{(x_s, a_s, r_s)\}_{s < t}$ is the history.

**Algorithm (informal):**

For each episode $t$:
1. Sample a plausible mean reward function $\tilde{\mu}$ from posterior
2. Compute $a_t = \arg\max_{a} \tilde{\mu}(x_t, a)$
3. Apply action $a_t$, observe reward $r_t$
4. Update posterior: $p(\mu \mid \mathcal{H}_{t+1}) \propto p(r_t \mid \mu, x_t, a_t) \cdot p(\mu \mid \mathcal{H}_t)$

**Why this works (intuitively):**

- **Exploration**: When uncertain, posterior is wide -> samples vary -> tries different actions
- **Exploitation**: When confident, posterior is narrow -> samples concentrate on best action
- **Automatic balance**: Uncertainty naturally decreases as data accumulates

**Mathematical formalization:**

We need to specify:
1. Prior distribution over mean rewards $p(\mu)$
2. Likelihood model $p(r \mid \mu, x, a)$
3. Posterior update rule $p(\mu \mid \mathcal{H})$

For linear contextual bandits, we use a **Gaussian prior** with **Gaussian likelihood**.

---

### 6.2.2 Linear Contextual Thompson Sampling

**Definition 6.3** (Linear Contextual Bandit) {#DEF-6.3}

A **linear contextual bandit** is a stochastic contextual bandit ([DEF-6.2]) whose mean reward function admits a linear representation:
$$
\mu(x, a)
  := \mathbb{E}_{\omega \sim P(\cdot \mid x, a)}[R(x, a, \omega)]
  = \langle \theta_a, \phi(x) \rangle
  = \theta_a^\top \phi(x)
\tag{6.5}
$$
{#EQ-6.5}

where:
- $\phi: \mathcal{X} \to \mathbb{R}^d$ is a known **feature map** (Chapter 5)
- $\theta_a \in \mathbb{R}^d$ is an unknown **weight vector** for action $a$
- $\langle \cdot, \cdot \rangle$ is the Euclidean inner product

We make the following **structural assumptions**:

1. **Finite dimension**: Feature space is $\mathbb{R}^d$ with $d < \infty$
2. **Linearity**: Mean reward is exactly linear in features (no approximation error in the model class)
3. **Bounded features**: $\|\phi(x)\| \leq L$ for all $x \in \mathcal{X}$ and some constant $L > 0$
4. **Bounded parameters**: $\|\theta_a\| \leq S$ for all $a \in \mathcal{A}$ and some constant $S > 0$

**Why linear?**

We need *some* parametric structure to generalize across contexts. Options:

**Option A: Tabular (no structure)**
- $\mu(x, a)$ is a table with $|\mathcal{X}| \times M$ entries
- Pros: No assumptions.
- Cons: No generalization (each context learned independently); infeasible sample complexity when $|\mathcal{X}|$ is large/continuous.

**Option B: Nonlinear (neural network)**
- $\mu(x, a) = f_\theta(x, a)$ with neural net $f$
- Pros: Maximum flexibility.
- Cons: Requires many samples (Chapter 7); posterior intractable (no closed-form updates).

**Option C: Linear (our choice)**
- $\mu(x, a) = \theta_a^\top \phi(x)$
- Pros: Closed-form posterior (Gaussian conjugate); sample-efficient with good features.
- Cons: Misspecification risk (if the true $\mu$ is nonlinear).

We choose **linear** because:
1. Chapter 5 engineered rich features $\phi(x)$ (product, user, query interactions)
2. Gaussian conjugate prior -> efficient Bayesian updates
3. Fast inference (matrix operations, no MCMC)
4. Provable regret bounds (Theorem 6.1 below)

This breaks when:
- Feature engineering is poor (Exercise 6.11 explores kernel features)
- True reward highly nonlinear (Exercise 6.12 compares to neural TS)

**Gaussian linear model (prior and posterior):**

For each action $a \in \{1, \ldots, M\}$, we start from an independent Gaussian prior
$$
\theta_a \sim \mathcal{N}(0, \lambda_0^{-1} I_d),
$$
and under the Gaussian likelihood below the posterior remains Gaussian.

For each action $a \in \{1, \ldots, M\}$, we maintain:
$$
\theta_a \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)
\tag{6.6}
$$
{#EQ-6.6}

where:
- $\hat{\theta}_a \in \mathbb{R}^d$ is the **posterior mean** (our current estimate)
- $\Sigma_a \in \mathbb{R}^{d \times d}$ is the **posterior covariance** (our uncertainty)

**Likelihood model:**

Assume rewards are Gaussian:
$$
r_t \mid x_t, a_t, \theta_{a_t} \sim \mathcal{N}(\theta_{a_t}^\top \phi(x_t), \sigma^2)
\tag{6.7}
$$
{#EQ-6.7}

where $\sigma^2$ is the **noise variance** (typically unknown, estimated from data).

**Posterior update (Bayesian linear regression; ridge form):**

It is convenient (and numerically stable) to maintain the **sufficient statistics** that also appear in ridge regression:
$$
A_a := \sigma^2 \lambda_0 I_d + \sum_{s:\,a_s=a}\phi(x_s)\phi(x_s)^\top,\qquad
b_a := \sum_{s:\,a_s=a} r_s\,\phi(x_s).
$$
Under the Gaussian model [EQ-6.7], this yields the posterior
$\theta_a \mid \mathcal{H}_t \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)$
with $\hat{\theta}_a = A_a^{-1} b_a$ and $\Sigma_a = \sigma^2 A_a^{-1}$.
After observing $(x_t, a_t, r_t)$, update only the selected action $a_t$ via
\begin{align}
A_{a_t} &\leftarrow A_{a_t} + \phi(x_t)\,\phi(x_t)^\top \tag{6.8a}\\
b_{a_t} &\leftarrow b_{a_t} + r_t\,\phi(x_t) \tag{6.8b}\\
\hat{\theta}_{a_t} &\leftarrow A_{a_t}^{-1} b_{a_t},\qquad \Sigma_{a_t} \leftarrow \sigma^2 A_{a_t}^{-1}. \tag{6.8c}
\end{align}
{#EQ-6.8}

This is algebraically equivalent to updating the precision $\Sigma_a^{-1}$ and the scaled moment $\sigma^{-2}\sum r_t\phi_t$, but it avoids "old vs. new precision" ambiguities and makes the ridge-regression equivalence in [EQ-6.9] immediate. (Other actions' posteriors unchanged.)

**Equivalence to ridge regression:**

The posterior mean $\hat{\theta}_a$ is the **ridge regression estimate**:
$$
\hat{\theta}_a = \arg\min_{\theta} \left\{ \sum_{t: a_t = a} (r_t - \theta^\top \phi(x_t))^2 + \lambda \|\theta\|^2 \right\}
\tag{6.9}
$$
{#EQ-6.9}

where $\lambda = \sigma^2 / \sigma_0^2$ is the regularization strength (ratio of noise variance to prior variance).
Equivalently, under the prior $\theta_a \sim \mathcal{N}(0,\lambda_0^{-1}I_d)$, the ridge penalty is $\lambda = \sigma^2 \lambda_0$.

This shows TS is **Bayesian regularization**---the prior prevents overfitting.

**Algorithm 6.1** (Linear Thompson Sampling for Contextual Bandits) {#ALG-6.1}

**Input:**
- Feature map $\phi: \mathcal{X} \to \mathbb{R}^d$
- Action set $\mathcal{A} = \{1, \ldots, M\}$ (template IDs)
- Prior: $\theta_a \sim \mathcal{N}(0, \lambda_0^{-1} I_d)$ for all $a$ (prior precision $\lambda_0 > 0$)
- Noise variance $\sigma^2$ (estimated or set to 1.0)
- Number of episodes $T$

**Initialization:**
- For each action $a \in \mathcal{A}$:
  - $A_a \leftarrow \sigma^2 \lambda_0 I_d$
  - $b_a \leftarrow 0 \in \mathbb{R}^d$
  - $\hat{\theta}_a \leftarrow 0 \in \mathbb{R}^d$

**For** $t = 1, \ldots, T$:

1. **Observe context**: Receive $x_t \in \mathcal{X}$ from environment
2. **Compute features**: $\phi_t \leftarrow \phi(x_t) \in \mathbb{R}^d$
3. **Sample posteriors**: For each action $a \in \mathcal{A}$:
	   $$
	   \tilde{\theta}_a^{(t)} \sim \mathcal{N}(\hat{\theta}_a, \sigma^2 A_a^{-1})
	   $$
4. **Select optimistic action**:
   $$
   a_t \leftarrow \arg\max_{a \in \mathcal{A}} \langle \tilde{\theta}_a^{(t)}, \phi_t \rangle
   $$
5. **Execute action**: Apply template $a_t$, observe reward $r_t$
6. **Update posterior** for action $a_t$ using [EQ-6.8] (with $\phi_t=\phi(x_t)$).

**Output:** Posterior distributions $\{\mathcal{N}(\hat{\theta}_a, \Sigma_a)\}_{a=1}^M$

---

**Computational complexity.**

At episode $t$, with feature dimension $d$ and $M$ actions:

- **Feature computation:** $O(d)$ to form $\phi_t$.
- **Posterior sampling:** Naively, constructing $\Sigma_a$ and drawing $\tilde{\theta}_a \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)$ costs $O(d^3)$ per action (matrix inversion + Cholesky), i.e. $O(M d^3)$ overall.
- **Optimized implementation:** Maintaining precision matrices and Cholesky factors across episodes reduces sampling to $O(M d^2)$ per step (rank-1 updates + triangular solves; see Section 6.5 exercises).
- **Action selection:** Computing $\langle \tilde{\theta}_a^{(t)}, \phi_t \rangle$ for all $a$ is $O(M d)$ plus an $O(M)$ argmax.
- **Posterior update:** Rank-1 precision update and mean update for the chosen action $a_t$ is $O(d^2)$.

Over $T$ episodes this yields **time complexity** $O(T M d^2)$ with an optimized linear algebra backend and **memory** $O(M d^2)$ to store $\{\Sigma_a\}$ or their inverses. In practice, $M$ is small (8 templates) and $d$ is on the order of tens, so the cost is dominated by the simulator, not the bandit.

---

**Why does this work?**

Thompson Sampling elegantly balances exploration and exploitation through **probability matching**:

**Probability matching property:**

The probability of selecting action $a$ equals the probability that $a$ is optimal under the posterior:
$$
P(a_t = a \mid \mathcal{H}_t) = P(\theta_a^\top \phi_t > \theta_{a'}^\top \phi_t \text{ for all } a' \neq a \mid \mathcal{H}_t)
\tag{6.10}
$$
{#EQ-6.10}

**Intuition:**

- If action $a$ is **very likely optimal** (concentrated posterior), it gets selected with high probability -> **exploitation**
- If action $a$ is **uncertain** (wide posterior), it occasionally gets sampled "just in case" -> **exploration**
- As data accumulates, posteriors concentrate on true parameters -> exploration diminishes naturally

**This is automatic!** No manually-tuned exploration rate (unlike epsilon-greedy) or confidence intervals (unlike UCB).

---

### 6.2.3 Regret Analysis

**Bayesian Regret** {#DEF-6.4}

For a Bayesian policy, we define the **Bayesian regret** as the expected regret where the expectation is taken over the context distribution, the policy's randomness, and the Bayesian prior:
$$
\text{BayesReg}(T)
  = \mathbb{E}_{\theta^* \sim \pi_0,\,\text{contexts, policy}}
    \left[\sum_{t=1}^T \bigl(\max_{a \in \mathcal{A}} \theta_a^{*\top} \phi_t - \theta_{a_t}^{*\top} \phi_t\bigr)\right]
$$
where $\theta^* \sim \pi_0$ denotes the prior on unknown parameters, $\phi_t := \phi(x_t)$, and the expectation includes the randomness from sampling contexts, reward noise, and posterior sampling.
Equivalently, $\text{BayesReg}(T)=\mathbb{E}_{\theta^* \sim \pi_0}[\text{Regret}(T\mid \theta^*)]$.

**Remark 6.2.1** (Bayesian vs Frequentist Regret) {#REM-6.2.1}

Two regret notions appear in this chapter:

1. **Bayesian regret** (this definition): the expectation averages over a *prior* on the unknown parameters $\theta^*$ as well as contexts and policy randomness.
2. **Frequentist regret** (used for LinUCB in [THM-6.2]): the parameters $\theta^*$ are treated as fixed but unknown; the expectation is only over contexts and policy randomness.

Formally, Bayesian regret can be written as
$$
\text{BayesReg}(T)
  = \mathbb{E}_{\theta^* \sim \pi_0,\,\text{contexts, policy}}
    \left[\sum_{t=1}^T \bigl(\max_a \theta_a^{*\top} \phi_t - \theta_{a_t}^{*\top} \phi_t\bigr)\right],
$$
while frequentist regret conditions on a fixed $\theta^*$:
$$
\text{Regret}(T \mid \theta^*)
  = \mathbb{E}_{\text{contexts, policy}}
    \left[\sum_{t=1}^T \bigl(\max_a \theta_a^{*\top} \phi_t - \theta_{a_t}^{*\top} \phi_t\bigr)\right].
$$

Thompson Sampling is naturally analyzed in the Bayesian sense (it explicitly uses a prior), whereas LinUCB is usually analyzed in the frequentist sense (no prior, worst-case guarantees over all admissible $\theta^*$). When the prior is well-calibrated and concentrates around the true parameters, the two perspectives tend to agree asymptotically, but they answer slightly different questions: "average performance across plausible worlds" vs. "performance in this particular world".

In this chapter we state [THM-6.1] as an expected regret bound conditional on a fixed (unknown) $\theta^*$, using standard self-normalized concentration for ridge regression plus Gaussian sampling concentration. A Bayesian regret statement follows by averaging over any prior supported on $\{\|\theta_a^*\|\le S\}$ (or by working with a truncated prior).

For deeper treatment of the Bayesian perspective---hierarchical priors over user and segment preferences, posterior shrinkage, and how those posteriors feed into bandit features---see [@russo:tutorial_ts:2018] and [@chapelle:empirical_ts:2011]. The survey by [@lattimore:bandit_algorithms:2020, Chapters 35--37] provides rigorous foundations for Bayesian regret analysis in linear bandits. **Appendix A** develops these ideas for our search setting: hierarchical user preference models, posterior inference for price sensitivity and brand affinity, and how Bayesian estimates integrate with the template bandits of this chapter.

**Theorem 6.1** (Thompson Sampling Regret Bound) {#THM-6.1}

Consider a linear contextual bandit ([DEF-6.3]) with:

**Data:**
- Feature dimension $d \in \mathbb{N}$
- Action set $\mathcal{A} = \{1, \ldots, M\}$ with $M \geq 2$
- Horizon $T \in \mathbb{N}$ (number of episodes)

**Structural assumptions:**
1. **Linearity**: Mean rewards $\mu(x, a) = \langle \theta_a^*, \phi(x) \rangle$ for unknown parameters $\theta_a^* \in \mathbb{R}^d$
2. **Bounded parameters**: $\|\theta_a^*\| \leq S$ for all $a \in \mathcal{A}$ and some constant $S > 0$
3. **Bounded features**: $\|\phi(x)\| \leq L$ for all $x \in \mathcal{X}$ and some constant $L > 0$
4. **i.i.d. contexts**: Contexts $\{x_t\}_{t=1}^T$ are drawn i.i.d. from distribution $\rho$ over $\mathcal{X}$
5. **Sub-Gaussian noise**: For each $(x, a)$, the reward noise
   $$
   \epsilon := r - \mu(x, a)
   $$
   conditioned on $(x, a)$ is sub-Gaussian with variance proxy $\sigma^2$:
   $$
   \mathbb{E}[\exp(\lambda \epsilon) \mid x, a]
     \leq \exp(\lambda^2 \sigma^2 / 2)
     \quad \forall \lambda \in \mathbb{R}.
   $$

**Algorithm configuration:**
- Prior: $\theta_a \sim \mathcal{N}(0, \lambda_0^{-1} I_d)$ for each $a$, with regularization $\lambda_0 > 0$
- Likelihood: Gaussian with known variance proxy $\sigma^2$ (or a consistent estimate)

If the noise is truly Gaussian, the updates in [ALG-6.1] correspond to the exact conjugate posterior. Under merely sub-Gaussian noise, we interpret $\mathcal{N}(\hat\theta_a,\Sigma_a)$ as a **Gaussian pseudo-posterior / perturbation distribution** used for exploration; the analysis relies on concentration of ridge regression estimates plus Gaussian sampling, not on exact Bayesian correctness.

Then Thompson Sampling ([ALG-6.1]) with the above prior satisfies
$$
\mathbb{E}[\text{Regret}(T)\mid \theta^*]
  \leq C \cdot d\sqrt{M T \log T}
\tag{6.11}
$$
{#EQ-6.11}

for some constant $C > 0$ that depends on $S, L, \sigma, \lambda_0$ but not on $T$.
Here the expectation is taken over contexts, reward noise, and posterior-sampling randomness, conditional on the fixed (unknown) parameters $\theta^*$.

*Proof.* See [@agrawal:thompson:2013, Theorem 2] for a complete proof. $\square$

We record below the main concentration lemmas and the elliptical potential identity used in that proof, since we reuse these tools when interpreting exploration bonuses and diagnostics later in the chapter.

**Lemma 6.1.1** (Gaussian Concentration Around the Mean Estimate) {#LEM-6.1.1}
Under the assumptions of [THM-6.1], condition on the history up to episode $t-1$ and let
$\tilde{\theta}_a^{(t)} \sim \mathcal{N}(\hat{\theta}_{a,t-1}, \Sigma_{a,t-1})$
be the parameter sample for action $a$ at episode $t$. Then, with probability at least $1 - \delta/(MT)$,
$$
\bigl|
  \tilde{\theta}_a^{(t)\top} \phi(x_t)
  - \hat{\theta}_{a,t-1}^{\top} \phi(x_t)
\bigr|
  \leq \alpha_t \, \|\phi(x_t)\|_{\Sigma_{a,t-1}}
$$
where $\alpha_t = \sqrt{2 \log(2MT/\delta)}$ and
$\|v\|_{\Sigma} := \sqrt{v^\top \Sigma v}$.

*Proof of Lemma 6.1.1.* This is a standard Gaussian tail bound applied to the one-dimensional projection $(\tilde{\theta}_a^{(t)}-\hat\theta_{a,t-1})^\top \phi(x_t)$; see [@agrawal:thompson:2013, Lemma 3]. QED

**Lemma 6.1.1b** (Estimator Concentration Around Truth; self-normalized) {#LEM-6.1.1b}
Under the assumptions of [THM-6.1], define for each action $a$ the (regularized) design matrix
$$
A_{a,t} := \sigma^2 \lambda_0 I_d + \sum_{s \le t:\, a_s = a} \phi_s \phi_s^\top,
$$
and let $\hat\theta_{a,t}$ be the ridge regression estimate for that action (equivalently, the posterior mean in [EQ-6.8]). Then for any $\delta\in(0,1)$, with probability at least $1-\delta$ (simultaneously for all $t$ and $a$),
$$
\bigl|(\hat\theta_{a,t}-\theta_a^*)^\top \phi(x_t)\bigr|
\le \beta_{a,t}(\delta)\,\|\phi(x_t)\|_{A_{a,t}^{-1}},
$$
where one standard choice is
$$
\beta_{a,t}(\delta)
= \sigma\sqrt{2\log(M/\delta) + d\log\Bigl(1 + \frac{n_a(t)}{d\,\sigma^2\lambda_0}\Bigr)}
  + \sigma\sqrt{\lambda_0}\,S,
$$
with $n_a(t)$ the number of times action $a$ was selected up to time $t$; see [@abbasi:improved:2011, Theorem 2] or [@lattimore:bandit_algorithms:2020, Chapter 19].

**Corollary 6.1.1** (Triangle inequality). {#COR-6.1.1}
On the intersection of the events in Lemma 6.1.1 and Lemma 6.1.1b, using $\Sigma_{a,t}=\sigma^2 A_{a,t}^{-1}$ in the Gaussian model, we have
$$
\bigl|(\tilde\theta_{a}^{(t)}-\theta_a^*)^\top \phi(x_t)\bigr|
\le \Bigl(\beta_{a,t-1}(\delta) + \sigma\sqrt{2\log(2MT/\delta)}\Bigr)\,\|\phi(x_t)\|_{A_{a,t-1}^{-1}}.
$$

**Lemma 6.1.2** (Elliptical Potential; [@abbasi:improved:2011, Lemma 11]) {#LEM-6.1.2}
Let $\{A_t\}_{t \geq 0}$ be a sequence of $d \times d$ positive definite matrices with $A_0 = \sigma^2\lambda_0 I_d$ and updates
$$
A_t = A_{t-1} + v_t v_t^\top, \quad \|v_t\| \leq L.
$$
Then
$$
\sum_{t=1}^T \min\{1,\,\|v_t\|_{A_{t-1}^{-1}}^2\}
  \leq 2\log\det(A_T A_0^{-1}).
$$
Moreover,
$$
\log\det(A_T A_0^{-1})
  \leq d \log\bigl(1 + TL^2/(d \sigma^2\lambda_0)\bigr).
$$

*Proof of Lemma 6.1.2.* See [@abbasi:improved:2011, Lemma 11]. QED

**Remark 6.1.1** (When Regret Bounds Fail in Practice) {#REM-6.1.1}

[THM-6.1] is a **conditional guarantee**: regret is $O(\sqrt{T})$ *if the assumptions hold*. In Sections 6.5--6.6 we will deliberately violate these assumptions and see the regret picture break:

1. **Model misspecification (Section 6.6):** The true reward is nonlinear, but we force a linear model. The theorem still applies to the *best linear approximation*, yet approximation error dominates, and observed regret can grow almost linearly.
2. **Feature poverty (Section 6.6):** The feature map $\phi(x)$ omits critical context (e.g., user preferences). The bound applies in the restricted feature space, but the optimal policy **in that space** may be far from the true optimum.
3. **Heavy-tailed noise (advanced):** If rewards are heavy-tailed (e.g., lognormal order values), the sub-Gaussian assumption fails (even when variance is finite); standard regret bounds require clipping or robust estimators.

The lesson is central to this chapter: theorem correctness does not imply algorithmic success. In practice we monitor the assumptions via diagnostics (per-segment performance, uncertainty traces in Section 6.7) rather than treating a regret bound as a performance guarantee.

**Interpretation:**

- **Sublinear regret**: $O(\sqrt{T})$ -> per-episode regret $O(1/\sqrt{T}) \to 0$
- **Dimension dependence**: Linear in $d$ -> feature engineering critical
- **Action scaling**: $\sqrt{M}$ -> modest penalty for more templates

In our regime $M$ is small (eight templates) and $d$ is on the order of tens, so the $\sqrt{M}$ and $d$ dependencies are mild; however, the constants hidden in the $O(\cdot)$ notation can be large, and we rely on empirical validation in the simulator.

---

**Minimal numerical verification:**

We verify Thompson Sampling on a synthetic 3-armed bandit to build intuition before production implementation:

```python
import numpy as np
import matplotlib.pyplot as plt

# Synthetic 3-armed linear bandit
np.random.seed(42)
d = 5  # Feature dimension
M = 3  # Number of actions (templates)
T = 1000  # Episodes

# True parameters (unknown to algorithm)
theta_star = np.random.randn(M, d)  # Shape (M, d)
sigma = 0.1  # Reward noise std

# Thompson Sampling initialization
lambda_reg = 1.0
theta_hat = np.zeros((M, d))  # Posterior means
Sigma_inv = np.array([lambda_reg * np.eye(d) for _ in range(M)])  # Precision matrices

rewards_history = []
regrets_history = []

for t in range(T):
    # Context (random for synthetic example)
    x = np.random.randn(d)
    x /= np.linalg.norm(x)  # Normalize to ||x|| = 1

    # Sample from posteriors
    theta_samples = []
    for a in range(M):
        Sigma_a = np.linalg.inv(Sigma_inv[a])
        theta_tilde = np.random.multivariate_normal(theta_hat[a], Sigma_a)
        theta_samples.append(theta_tilde)

    # Select action with highest sampled reward
    expected_rewards = [theta_samples[a] @ x for a in range(M)]
    action = np.argmax(expected_rewards)

    # Observe reward
    true_reward = theta_star[action] @ x + sigma * np.random.randn()
    rewards_history.append(true_reward)

    # Compute regret (oracle knows theta_star)
    optimal_reward = np.max([theta_star[a] @ x for a in range(M)])
    regret = optimal_reward - theta_star[action] @ x
    regrets_history.append(regret)

    # Update posterior for selected action
    Sigma_inv[action] += (1 / sigma**2) * np.outer(x, x)
    Sigma_a = np.linalg.inv(Sigma_inv[action])
    theta_hat[action] = Sigma_a @ (Sigma_inv[action] @ theta_hat[action] + (1 / sigma**2) * x * true_reward)

# Plot cumulative regret
cumulative_regret = np.cumsum(regrets_history)
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(cumulative_regret, label='Thompson Sampling')
plt.plot([0, T], [0, np.sqrt(T) * 5], 'r--', label=r'$O(\sqrt{T})$ bound')
plt.xlabel('Episode')
plt.ylabel('Cumulative Regret')
plt.legend()
plt.title('Thompson Sampling Regret Growth')

plt.subplot(1, 2, 2)
plt.plot(np.array(cumulative_regret) / np.arange(1, T+1), label='Average Regret')
plt.axhline(0, color='r', linestyle='--', label='Optimal')
plt.xlabel('Episode')
plt.ylabel('Average Regret per Episode')
plt.legend()
plt.title('Average Regret -> 0 (Convergence)')

plt.tight_layout()
plt.savefig('/tmp/thompson_sampling_verification.png', dpi=150)
print("Verification plot saved to /tmp/thompson_sampling_verification.png")

# Output:
# Cumulative regret grows as O(sqrtT) [OK]
# Average regret per episode -> 0 [OK]
```

**Observations:**

1. **Cumulative regret sublinear**: Grows slower than $O(\sqrt{T})$ theoretical bound
2. **Average regret vanishes**: Per-episode regret $\to 0$ as $T \to \infty$
3. **Fast convergence**: After ~200 episodes, algorithm concentrates on optimal actions

Theory works. In Section 6.4 we turn this into production code inside our simulator.

---

## 6.3 LinUCB: Upper Confidence Bounds

Thompson Sampling is elegant, but **stochastic**---each run produces different template selections even with the same data. For production systems where **reproducibility** and **deterministic debugging** matter, we want a **frequentist alternative**.

Enter **LinUCB**: Linear Upper Confidence Bound algorithm.

### 6.3.1 The UCB Principle

**Core idea: Optimism in the face of uncertainty.**

Instead of sampling from a posterior, LinUCB constructs **confidence intervals** around mean reward estimates and selects the action with the **highest upper bound**.

**Confidence bound construction:**

For each action $a$ and context $x$, we estimate:
$$
\hat{\mu}(x, a) = \hat{\theta}_a^\top \phi(x)
\tag{6.12}
$$
{#EQ-6.12}

and compute an **uncertainty bonus**:
$$
\text{UCB}(x, a) = \hat{\mu}(x, a) + \alpha \cdot \text{Uncertainty}(x, a)
\tag{6.13}
$$
{#EQ-6.13}

where $\alpha > 0$ is an **exploration parameter** and $\text{Uncertainty}(x, a)$ measures confidence in $\hat{\mu}(x, a)$.

**Why this works:**

- **Exploitation**: Term $\hat{\mu}(x, a)$ favors actions with high estimated reward
- **Exploration**: Bonus $\alpha \cdot \text{Uncertainty}$ favors actions with high uncertainty (underexplored)
- **Automatic balance**: As action $a$ is selected, data accumulates -> uncertainty shrinks -> exploration bonus decreases

**Mathematical formalization:**

For linear contextual bandits, the uncertainty is the **prediction interval width**:
$$
\text{Uncertainty}(x, a) = \sqrt{\phi(x)^\top \Sigma_a \phi(x)}
\tag{6.14}
$$
{#EQ-6.14}

where $\Sigma_a$ is the posterior covariance (same as Thompson Sampling!).
In the Gaussian linear model, $\Sigma_a = \sigma^2 A_a^{-1}$ with $A_a = \sigma^2\lambda_0 I + \sum_{s:\,a_s=a}\phi_s\phi_s^\top$ (see [PROP-6.1] for the ridge-regression form). In practice we implement $\sqrt{\phi^\top A_a^{-1}\phi}$ and absorb the factor $\sigma$ into $\alpha$.

**Geometric interpretation:**

$\text{Uncertainty}(x, a)$ is the standard deviation of the predicted reward $\hat{\theta}_a^\top \phi(x)$ under the Gaussian posterior. It measures how much $\phi(x)$ aligns with the **principal axes of uncertainty** in parameter space.

**Proposition 6.2** (High-probability decay of the uncertainty bonus) {#PROP-6.2}

Let $\{\phi_s\}_{s\ge 1}$ be i.i.d. random vectors in $\mathbb{R}^d$ such that $\|\phi_s\|_2 \le L$ almost surely and
$$
\mathbb{E}[\phi\phi^\top] \succeq c I_d
$$
for some $c>0$. Fix $\lambda>0$ and for any $n\ge 1$ define the regularized design matrix
$$
A(n) = \lambda I_d + \sum_{s=1}^{n}\phi_s\phi_s^\top.
$$
Then for any $\delta \in (0,1)$, if
$$
n \ge n_0 := \frac{8L^2}{c}\,\log\frac{d}{\delta},
$$
we have, with probability at least $1-\delta$, the uniform bound
$$
\|v\|_{A(n)^{-1}}
  := \sqrt{v^\top A(n)^{-1} v}
  \le \frac{L}{\sqrt{\lambda + (c/2)\,n}}.
$$

for all $v\in\mathbb{R}^d$ with $\|v\|_2\le L$. In particular, this applies to $v=\phi(x)$ when $\|\phi(x)\|_2\le L$.

*Proof.*

Define the random positive semidefinite matrices $X_s := \phi_s\phi_s^\top$. Then $0 \preceq X_s \preceq L^2 I_d$ almost surely, and
$$
\mathbb{E}[X_s] = \mathbb{E}[\phi\phi^\top] \succeq c I_d.
$$
Let $\mu_{\min} := \lambda_{\min}\bigl(\mathbb{E}[\sum_{s=1}^n X_s]\bigr)$. By linearity of expectation,
$
\mathbb{E}[\sum_{s=1}^n X_s]=n\,\mathbb{E}[X_1]
$
and therefore $\mu_{\min} \ge nc$.

By the matrix Chernoff lower-tail bound (e.g., [@tropp:user_friendly:2012, Theorem 5.1]), for any $\varepsilon\in(0,1)$,
$$
\mathbb{P}\!\left(\lambda_{\min}\!\left(\sum_{s=1}^n X_s\right) \le (1-\varepsilon)\mu_{\min}\right)
\le d\exp\!\left(-\frac{\varepsilon^2\,\mu_{\min}}{2L^2}\right).
$$
With $\varepsilon=\tfrac12$ and $\mu_{\min}\ge nc$, this yields
$$
\mathbb{P}\!\left(\lambda_{\min}\!\left(\sum_{s=1}^n \phi_s\phi_s^\top\right) \le \frac{c}{2}\,n\right)
\le d\exp\!\left(-\frac{nc}{8L^2}\right).
$$
If $n \ge \frac{8L^2}{c}\log\frac{d}{\delta}$, the right-hand side is at most $\delta$, so with probability at least $1-\delta$,
$$
\lambda_{\min}\!\left(\sum_{s=1}^n \phi_s\phi_s^\top\right) \ge \frac{c}{2}\,n.
$$
On this event,
$
A(n) = \lambda I_d + \sum_{s=1}^n \phi_s\phi_s^\top \succeq \bigl(\lambda + \tfrac{c}{2}n\bigr) I_d
$
and hence
$
A(n)^{-1} \preceq \bigl(\lambda + \tfrac{c}{2}n\bigr)^{-1} I_d.
$
Therefore for any $v$ with $\|v\|_2\le L$,
$$
v^\top A(n)^{-1} v \le \frac{\|v\|_2^2}{\lambda + (c/2)n} \le \frac{L^2}{\lambda + (c/2)n}.
$$
Taking square roots gives the claimed bound. $\square$

**Remark 6.2.2** (Random design vs. adaptive selection) {#REM-6.2.2}

Proposition 6.2 assumes an i.i.d. feature sequence $(\phi_s)$ with a population covariance lower bound. In LinUCB, however, contexts and action choices are coupled through the algorithm, so the resulting design is adaptive. The rigorous regret proof underlying [THM-6.2] therefore proceeds via self-normalized martingale inequalities and the elliptical potential lemma (see [@abbasi:improved:2011]) rather than an i.i.d. eigenvalue-growth argument. We include Proposition 6.2 only as geometric intuition: when features are sufficiently informative, $A(n)$ becomes well-conditioned and the uncertainty norm $\|\phi(x)\|_{A(n)^{-1}}$ shrinks on the order of $n^{-1/2}$ along bounded directions.

**LinUCB action selection:**

$$
a_t = \arg\max_{a \in \mathcal{A}} \left\{\hat{\theta}_a^\top \phi(x_t) + \alpha \sqrt{\phi(x_t)^\top \Sigma_a \phi(x_t)}\right\}
\tag{6.15}
$$
{#EQ-6.15}

---

**Algorithm 6.2** (LinUCB for Contextual Bandits) {#ALG-6.2}

**Input:**
- Feature map $\phi: \mathcal{X} \to \mathbb{R}^d$
- Action set $\mathcal{A} = \{1, \ldots, M\}$
- Regularization $\lambda > 0$
- Exploration parameter $\alpha > 0$ (typically $\alpha \in [0.1, 2.0]$)
- Number of episodes $T$

**Initialization:**
- For each action $a \in \mathcal{A}$:
  - $\hat{\theta}_a \leftarrow 0 \in \mathbb{R}^d$
  - $A_a \leftarrow \lambda I_d$ (design matrix accumulator)
  - $b_a \leftarrow 0 \in \mathbb{R}^d$ (reward accumulator)

**For** $t = 1, \ldots, T$:

1. **Observe context**: $x_t \in \mathcal{X}$
2. **Compute features**: $\phi_t \leftarrow \phi(x_t)$
3. **Compute UCB scores** for all $a \in \mathcal{A}$:
   $$
   \text{UCB}_a = \hat{\theta}_a^\top \phi_t + \alpha \sqrt{\phi_t^\top A_a^{-1} \phi_t}
   $$
4. **Select action**: $a_t \leftarrow \arg\max_a \text{UCB}_a$
5. **Observe reward**: $r_t$
6. **Update** statistics for $a_t$:
   \begin{align}
   A_{a_t} &\leftarrow A_{a_t} + \phi_t \phi_t^\top \\
   b_{a_t} &\leftarrow b_{a_t} + r_t \phi_t \\
   \hat{\theta}_{a_t} &\leftarrow A_{a_t}^{-1} b_{a_t}
   \end{align}

**Output:** Learned weights $\{\hat{\theta}_a\}_{a=1}^M$

---

**Computational complexity.**

Per episode, with feature dimension $d$ and $M$ actions:

1. **Feature computation:** $O(d)$ to compute $\phi_t$.
2. **UCB scores:** For each action, evaluating $\hat{\theta}_a^\top \phi_t$ is $O(d)$ and the naive uncertainty term $\sqrt{\phi_t^\top A_a^{-1} \phi_t}$ requires forming $A_a^{-1}$, which is $O(d^3)$. Across all $M$ actions this is $O(M d^3)$.
   - With incremental matrix inverses or Cholesky updates, the uncertainty can be maintained in $O(d^2)$ per action, i.e. $O(M d^2)$ per episode.
3. **Argmax:** Selecting $a_t = \arg\max_a \text{UCB}_a$ costs $O(M)$.
4. **Update:** Updating $A_{a_t}$ and $b_{a_t}$ is $O(d^2)$ (rank-1 update and vector addition), and solving $A_{a_t}^{-1} b_{a_t}$ via `np.linalg.solve` is $O(d^3)$.

Thus the **naive total cost** is $O(T M d^3)$ over $T$ episodes; with rank-1 updates and cached factorizations one obtains **$O(T M d^2)$**. Memory usage is $O(M d^2)$ for the design matrices and $O(M d)$ for the weight vectors. As in Thompson Sampling, our regime has small $M$ and moderate $d$, so these costs are negligible compared to simulator trajectories.

---

**Proposition 6.1** (Posterior Mean Equivalence) {#PROP-6.1}

The posterior mean $\hat{\theta}_a$ maintained by both Thompson Sampling and LinUCB is identical and equals the ridge regression solution:
$$
\hat{\theta}_a = A_a^{-1} b_a = \left(\lambda I + \sum_{t: a_t=a} \phi_t \phi_t^\top\right)^{-1} \left(\sum_{t: a_t=a} r_t \phi_t\right)
$$
where $A_a$ is the design matrix and $b_a$ is the reward accumulator.
In the Bayesian model [EQ-6.6]--[EQ-6.7] with prior precision $\lambda_0$ and noise variance $\sigma^2$, this corresponds to the ridge penalty $\lambda = \sigma^2\lambda_0$ and posterior covariance $\Sigma_a = \sigma^2 A_a^{-1}$.

*Proof.* Under the Gaussian model [EQ-6.7] and prior $\theta_a \sim \mathcal{N}(0,\lambda_0^{-1}I_d)$, completing the square in the log posterior shows that the posterior mean maximizes a quadratic objective, equivalently minimizing the ridge regression problem [EQ-6.9] with $\lambda=\sigma^2\lambda_0$. The normal equations for that problem are $A_a \hat{\theta}_a = b_a$, hence $\hat{\theta}_a=A_a^{-1}b_a$ as stated. Both Thompson Sampling and LinUCB maintain these same sufficient statistics $(A_a,b_a)$ and therefore the same $\hat{\theta}_a$; they differ only in action selection (sampling vs. UCB). $\square$

**The only difference:**
- **Thompson Sampling**: Stochastic selection via posterior sampling
- **LinUCB**: Deterministic selection via upper confidence bound

---

### 6.3.2 Choosing the Exploration Parameter alpha

**The $\alpha$ dilemma:**

Theory says $\alpha = O(\sqrt{d \log T})$ for optimal regret. But in practice:

- **Too small** ($\alpha \to 0$): Greedy exploitation, insufficient exploration, gets stuck on suboptimal templates
- **Too large** ($\alpha \to \infty$): Excessive exploration, ignores reward signal, selects randomly

**How to choose $\alpha$?**

**Option A: Theoretical value** $\alpha = \sqrt{d \log T}$
- Pros: Provably optimal regret.
- Cons: Requires knowing $T$ (horizon) in advance; often overly conservative in practice.

**Option B: Cross-validation**
- Pros: Data-driven tuning.
- Cons: Expensive (requires offline simulation); may overfit to the validation set.

**Option C: Adaptive tuning** (our choice)
- Pros: Starts conservative, decays as confidence grows; no need to know $T$ in advance.
- Example: $\alpha_t = c \sqrt{\log(1 + t)}$ for constant $c \in [0.5, 2.0]$

**Practical recommendation:**

Start with $\alpha = 1.0$ (moderate exploration). Monitor:
- **Selection diversity**: If one template dominates early -> increase $\alpha$
- **Cumulative regret**: If regret grows linearly -> increase $\alpha$
- **Reward variance**: If rewards are noisy -> increase $\alpha$

Typical ranges in production: $\alpha \in [0.5, 2.0]$.

---

### 6.3.3 Regret Analysis

**Theorem 6.2** (LinUCB Regret Bound) {#THM-6.2}

Let $(\mathcal{X}, \mathcal{A}, R)$ be a linear contextual bandit with the same assumptions as [THM-6.1]. Fix a regularization parameter $\lambda > 0$. Then LinUCB ([ALG-6.2]) with exploration parameter
$$
\alpha
= \sigma \sqrt{2\log\left(\frac{2MT}{\delta}\right) + d \log\left(1 + \frac{TL^2}{d\lambda}\right)}
  + \sqrt{\lambda}\, S
$$
satisfies, with probability $\geq 1 - \delta$:
$$
\text{Regret}(T) \leq 2\alpha \sqrt{2dMT \log\left(1 + \frac{TL^2}{d\lambda}\right)}.
\tag{6.16}
$$
{#EQ-6.16}

where $S = \max_a \|\theta_a^*\|$ is the norm of true parameters.

*Proof.*

The proof follows [@abbasi:improved:2011]. Key steps:

**Step 1: Concentration inequality**

By the self-normalized concentration inequality for regularized linear regression ([@abbasi:improved:2011, Theorem 2]), with probability $\geq 1 - \delta$ (after a union bound over $M$ actions):
$$
\|\hat{\theta}_a - \theta_a^*\|_{A_a} \leq \alpha
$$
where $\|v\|_A = \sqrt{v^\top A v}$ is the weighted norm.

**Step 2: Confidence ellipsoid**

The set $\{\theta : \|\theta - \hat{\theta}_a\|_{A_a} \leq \alpha\}$ is a **confidence ellipsoid** containing $\theta_a^*$ with high probability.

**Step 3: Upper bound validity**

If $\theta_a^* \in$ ellipsoid, then:
$$
\theta_a^{* \top} \phi \leq \hat{\theta}_a^\top \phi + \alpha \|\phi\|_{A_a^{-1}} = \text{UCB}_a
$$

**Step 4: Optimism**

Since we select $a_t = \arg\max_a \text{UCB}_a$, and the optimal action $a^*$ satisfies $\text{UCB}_{a^*} \geq \theta_{a^*}^{* \top} \phi$, we have:
$$
\text{UCB}_{a_t} \geq \text{UCB}_{a^*} \geq \theta_{a^*}^{* \top} \phi
$$

Thus, instantaneous regret is bounded by $2\alpha \|\phi\|_{A_{a_t}^{-1}}$.

**Step 5: Elliptical potential**

Summing over $T$ episodes:
$$
\sum_{t=1}^T \|\phi_t\|_{A_{a_t}^{-1}}^2
  \leq 2d \sum_{a=1}^M \log\bigl(1 + n_a(T)L^2/(d\lambda)\bigr)
  \leq 2dM \log\bigl(1 + TL^2/(d\lambda)\bigr),
$$
where $n_a(T)$ is the number of times action $a$ is selected up to episode $T$.

by determinant inequality ([@abbasi:improved:2011, Lemma 11]).

Taking square root and union bound over $M$ actions yields [EQ-6.16]. QED

**Comparison to Thompson Sampling:**

Both achieve $O(d\sqrt{MT})$ regret up to logarithmic factors under realizability. LinUCB has:
- Pros: Deterministic given the same data/seed; interpretable (UCB scores explain selections).
- Cons: Requires tuning ($\alpha$); less adaptive (fixed exploration schedule, while TS adapts via posterior uncertainty).

**When to use which:**

- **Thompson Sampling**: Default choice for most applications (automatic exploration, no tuning)
- **LinUCB**: When reproducibility critical (A/B testing, debugging) or when $\alpha$ can be tuned offline

---

## 6.4 Production Implementation

The previous sections treated Thompson Sampling and LinUCB as abstract algorithms. To run the experiments in Sections 6.5--6.7 we need production-grade code that:

- Implements the Bayesian and UCB updates faithfully
- Plays nicely with the `zoosim` catalog, user, and query modules
- Exposes configuration knobs (regularization, exploration strength, seeds)
- Surfaces diagnostics for monitoring in A/B tests

We follow a simple pattern:

1. A **configuration dataclass** controls hyperparameters.
2. A **policy class** exposes `select_action(phi(x))` and `update(a, phi(x), r)`.
3. A thin **integration layer** in the experiment script connects simulator observations to feature maps and policy calls.

### 6.4.1 Thompson Sampling Implementation

For Thompson Sampling we implement a `ThompsonSamplingConfig` and a `LinearThompsonSampling` policy in `zoosim/policies/thompson_sampling.py`.

- The config controls prior precision `lambda`, noise scale `sigma`, a `use_cholesky` flag, and a `seed` for reproducibility.
- The policy maintains, for each template $a$:
  - A precision matrix $\Sigma_a^{-1} = \lambda_0 I + \sigma^{-2}\sum_t \phi_t \phi_t^\top$ (stored as `Sigma_inv[a]`)
  - A scaled moment vector $b_a = \sigma^{-2}\sum_t r_t \phi_t$ (stored as `b[a]`)
  - A posterior mean vector $\hat{\theta}_a$ satisfying $\Sigma_a^{-1}\hat{\theta}_a=b_a$ (stored as `theta_hat[a]`)
  - A selection count `n_samples[a]` for diagnostics
- On each call to `select_action(phi)` we:
  1. Sample $\tilde{\theta}_a \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)$ for all templates (via NumPy and optional Cholesky for stability)
  2. Compute sampled rewards $\tilde{r}_a = \tilde{\theta}_a^\top \phi$
  3. Return `argmax_a \tilde{r}_a`
- On each `update(a, phi, r)` we perform the Bayesian linear regression update:
  $$
  \Sigma_a^{-1} \leftarrow \Sigma_a^{-1} + \sigma^{-2}\phi\phi^\top,\qquad
  b_a \leftarrow b_a + \sigma^{-2} r\phi,\qquad
  \hat{\theta}_a \leftarrow (\Sigma_a^{-1})^{-1} b_a.
  $$
  Multiplying these equations by $\sigma^2$ recovers the ridge-form update [EQ-6.8].

This is exactly the mathematical algorithm from Section 6.2.2 written in NumPy/Torch, with care taken to keep matrices well-conditioned and sampling numerically robust.

!!! note "Code <-> Agent (Thompson Sampling)"
    The Thompson Sampling production implementation lives in:

    - Algorithm: `zoosim/policies/thompson_sampling.py`
    - Templates: `zoosim/policies/templates.py`
    - Demo wiring: `scripts/ch06/template_bandits_demo.py`

    Conceptual mapping:

    - Posterior state $(\hat{\theta}_a, \Sigma_a)$ implements [EQ-6.6]--[EQ-6.8]
    - `select_action()` implements [ALG-6.1] (posterior sampling and greedy selection)
    - `update()` is the Bayesian linear regression update used in the regret proof of [THM-6.1]

    In the demos we always pass **feature vectors** `phi(x)` built by `context_features_simple` or `context_features_rich` from `scripts/ch06/template_bandits_demo.py`, so the production code is agnostic to how features are constructed.

### 6.4.2 LinUCB Implementation

**Implementation file: `zoosim/policies/lin_ucb.py`**

```python
r"""LinUCB (Linear Upper Confidence Bound) for contextual bandits.

Mathematical basis:
- [ALG-6.2] LinUCB algorithm
- [EQ-6.15] UCB action selection rule
- [THM-6.2] Regret bound $O(d \sqrt{M T \log T})$

Implements frequentist upper confidence bound exploration with deterministic
action selection. Maintains ridge regression estimates and selects the action
with highest optimistic reward estimate.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional

import numpy as np
from numpy.typing import NDArray

from zoosim.policies.templates import BoostTemplate


@dataclass
class LinUCBConfig:
    """Configuration for LinUCB policy.

    Attributes:
        lambda_reg: Regularization strength (ridge regression);
            prevents overfitting and keeps A_a invertible.
        alpha: Exploration parameter (UCB width multiplier);
            typical values alpha  in  [0.5, 2.0].
        adaptive_alpha: If True, use alpha_t = alphasqrtlog(1 + t)
            for automatic exploration decay.
        seed: Random seed (used for any feature hashing / randomness upstream).
        enable_diagnostics: If True, record per-episode diagnostic traces
            (UCB scores, means, uncertainties, and selected actions).
    """

    lambda_reg: float = 1.0
    alpha: float = 1.0
    adaptive_alpha: bool = False
    seed: int = 42
    enable_diagnostics: bool = False


class LinUCB:
    """Linear Upper Confidence Bound algorithm for contextual bandits.

    Maintains ridge regression estimates theta_a for each template and selects
    the action with highest upper confidence bound:

        a = argmax_a {theta_a^T phi(x) + alpha sqrt(phi(x)^T A_a^{-1} phi(x))}

    This is a frequentist alternative to Thompson Sampling with deterministic
    action selection. Both maintain identical posterior means theta_a but differ
    in how they use uncertainty for exploration.
    """

    def __init__(
        self,
        templates: List[BoostTemplate],
        feature_dim: int,
        config: Optional[LinUCBConfig] = None,
    ) -> None:
        """Initialize LinUCB policy."""
        self.templates = templates
        self.M = len(templates)
        self.d = feature_dim
        self.config = config or LinUCBConfig()

        # Initialize statistics: [ALG-6.2] initialization
        self.theta_hat = np.zeros((self.M, self.d), dtype=np.float64)
        self.A = np.array(
            [
                self.config.lambda_reg * np.eye(self.d, dtype=np.float64)
                for _ in range(self.M)
            ]
        )
        self.b = np.zeros((self.M, self.d), dtype=np.float64)

        self.n_samples = np.zeros(self.M, dtype=int)
        self.t = 0  # Episode counter

        # Optional diagnostics
        self.enable_diagnostics = bool(self.config.enable_diagnostics)
        if self.enable_diagnostics:
            self._diagnostics_history: Dict[str, list] = {
                "ucb_scores_history": [],
                "mean_rewards_history": [],
                "uncertainties_history": [],
                "selected_actions": [],
            }

    def select_action(self, features: NDArray[np.float64]) -> int:
        """Select template using the LinUCB criterion [EQ-6.15]."""
        self.t += 1

        # Compute adaptive exploration parameter
        alpha = self.config.alpha
        if self.config.adaptive_alpha:
            alpha *= np.sqrt(np.log(1 + self.t))

        # Compute UCB scores for all templates
        ucb_scores = np.zeros(self.M)
        mean_rewards = np.zeros(self.M)
        uncertainties = np.zeros(self.M)
        for a in range(self.M):
            # Mean estimate: theta_a^T phi
            mean_reward = self.theta_hat[a] @ features
            mean_rewards[a] = mean_reward

            # Uncertainty bonus: alpha sqrt(phi^T A_a^{-1} phi), implementing [EQ-6.14]
            A_inv = np.linalg.inv(self.A[a])
            uncertainty = np.sqrt(features @ A_inv @ features)
            uncertainties[a] = uncertainty

            # UCB score [EQ-6.15]
            ucb_scores[a] = mean_reward + alpha * uncertainty

        # Select action with highest UCB
        action = int(np.argmax(ucb_scores))

        if self.enable_diagnostics:
            self._diagnostics_history["ucb_scores_history"].append(ucb_scores.copy())
            self._diagnostics_history["mean_rewards_history"].append(
                mean_rewards.copy()
            )
            self._diagnostics_history["uncertainties_history"].append(
                uncertainties.copy()
            )
            self._diagnostics_history["selected_actions"].append(action)

        return action

    def update(
        self,
        action: int,
        features: NDArray[np.float64],
        reward: float,
    ) -> None:
        """Update ridge regression statistics after (action, features, reward)."""
        a = action
        phi = features

        # Update design matrix: A_a <- A_a + phiphi^T
        self.A[a] += np.outer(phi, phi)

        # Update reward accumulator: b_a <- b_a + r phi
        self.b[a] += reward * phi

        # Update weight estimate: theta_a <- A_a^{-1} b_a (via solve)
        self.theta_hat[a] = np.linalg.solve(self.A[a], self.b[a])

        # Track selection count
        self.n_samples[a] += 1

    def get_diagnostics(self) -> Dict[str, NDArray[np.float64] | float]:
        """Return aggregate diagnostic information for monitoring."""
        total = self.n_samples.sum()
        selection_freqs = (
            self.n_samples / total if total > 0 else self.n_samples
        )
        theta_norms = np.linalg.norm(self.theta_hat, axis=1)
        uncertainties = np.array(
            [np.trace(np.linalg.inv(self.A[a])) for a in range(self.M)]
        )

        alpha_current = self.config.alpha
        if self.config.adaptive_alpha:
            alpha_current *= np.sqrt(np.log(1 + self.t)) if self.t > 0 else 1.0

        return {
            "selection_counts": self.n_samples.copy(),
            "selection_frequencies": selection_freqs,
            "theta_norms": theta_norms,
            "uncertainty": uncertainties,
            "alpha_current": float(alpha_current),
        }

    def get_diagnostic_history(self) -> Dict[str, list]:
        """Return per-episode diagnostic traces if enabled."""
        if not self.enable_diagnostics:
            raise ValueError(
                "Diagnostics history not enabled; set "
                "LinUCBConfig.enable_diagnostics=True when constructing the policy."
            )
        return self._diagnostics_history
```

In production we also expose **richer diagnostics**: the actual `LinUCBConfig` in `zoosim/policies/lin_ucb.py` has an `enable_diagnostics: bool` flag. When set to `True`, the policy records per-episode traces of UCB scores, mean rewards, uncertainties, and selected actions, retrievable via `policy.get_diagnostic_history()`. The lighter `policy.get_diagnostics()` snapshot (selection frequencies, parameter norms, aggregate uncertainty, current $\alpha_t$) remains available regardless and is what we use for monitoring dashboards in Section 6.7.

!!! note "Code <-> Agent (LinUCB)"
    The `LinUCB` class implements [ALG-6.2]:
    - **Line 129-183**: `select_action()` computes UCB scores via [EQ-6.15] and selects argmax
    - **Line 185-223**: `update()` performs ridge regression update (design matrix + weight solve)
    - **Line 149-153**: Adaptive $\alpha_t = \alpha \sqrt{\log(1 + t)}$ option for automatic exploration decay
    - **Line 163-169**: Uncertainty computation $\sqrt{\phi^\top A_a^{-1} \phi}$ from [EQ-6.14] and UCB score assembly

    File: `zoosim/policies/lin_ucb.py`

**Numerical stability:**

- We solve $A_a \hat{\theta}_a = b_a$ via `np.linalg.solve` (more stable than explicit inversion)
- Regularization $\lambda > 0$ ensures $A_a$ is always invertible
- For large-scale production, use iterative solvers (conjugate gradient) or maintain Cholesky factors

---

### 6.4.3 Integration with ZooplusSearchEnv

Now we wire LinUCB and Thompson Sampling into the full search simulator from Chapter 5.

**Training loop structure:**

```python
"""Training loop for template bandits on search simulator.

Demonstrates integration of [ALG-6.1]/[ALG-6.2] with ZooplusSearchEnv.
"""

import numpy as np
from zoosim.envs.gym_env import ZooplusSearchGymEnv
from zoosim.policies.templates import create_standard_templates
from zoosim.policies.lin_ucb import LinUCB, LinUCBConfig
from zoosim.policies.thompson_sampling import LinearThompsonSampling, ThompsonSamplingConfig

# Initialize environment
env = ZooplusSearchGymEnv(seed=42)

	# Get catalog statistics for template creation
	catalog_stats = {
	    'price_p25': env.catalog.price.quantile(0.25),
	    'price_p75': env.catalog.price.quantile(0.75),
	    'pop_max': env.catalog.bestseller.max(),
	    'own_brand': 'Zooplus',
	}

# Create template library (M=8 templates)
templates = create_standard_templates(catalog_stats, a_max=5.0)

# Extract feature dimension from environment
obs, info = env.reset()
feature_dim = obs['features'].shape[0]  # Dimension d

# Initialize policy (choose one)
# Option 1: LinUCB
policy = LinUCB(
    templates=templates,
    feature_dim=feature_dim,
    config=LinUCBConfig(lambda_reg=1.0, alpha=1.0, adaptive_alpha=True)
)

# Option 2: Thompson Sampling
# policy = LinearThompsonSampling(
#     templates=templates,
#     feature_dim=feature_dim,
#     config=ThompsonSamplingConfig(lambda_reg=1.0, sigma_noise=1.0)
# )

# Training loop
T = 50_000  # Number of episodes
rewards_history = []
cumulative_regret = []
selection_history = []

for t in range(T):
    # Reset environment, observe context
    obs, info = env.reset()
    features = obs['features']

    # Select template using bandit policy
    template_id = policy.select_action(features)
    selection_history.append(template_id)

    # Apply template to get boost vector
    products = obs['products']  # List of product dicts
    boosts = templates[template_id].apply(products)

    # Execute action in environment
    obs, reward, done, truncated, info = env.step(boosts)

    # Update policy
    policy.update(template_id, features, reward)

    # Track metrics
    rewards_history.append(reward)

    # Compute regret (oracle comparison)
    # In real deployment, regret unknown; here we use oracle for analysis
    optimal_reward = info.get('optimal_reward', reward)  # Simulated oracle
    regret = optimal_reward - reward
    cumulative_regret.append((cumulative_regret[-1] if cumulative_regret else 0.0) + regret)

    # Logging
    if (t + 1) % 10_000 == 0:
        avg_reward = np.mean(rewards_history[-10_000:])
        diagnostics = policy.get_diagnostics()
        print(f"Episode {t+1}/{T}")
        print(f"  Avg reward (last 10k): {avg_reward:.2f}")
        print(f"  Selection frequencies: {diagnostics['selection_frequencies'].round(3)}")
        print(f"  Cumulative regret: {cumulative_regret[-1]:.1f}")
        print()

# Final evaluation
print("=== Training Complete ===")
print(f"Total episodes: {T}")
print(f"Average reward (overall): {np.mean(rewards_history):.2f}")
print(f"Average reward (last 10k): {np.mean(rewards_history[-10_000:]):.2f}")
print(f"Final cumulative regret: {cumulative_regret[-1]:.1f}")
print(f"\nTemplate selection distribution:")
for i, template in enumerate(templates):
    freq = policy.n_samples[i] / T
    print(f"  {template.name:15s}: {freq:.3f} ({policy.n_samples[i]:6d} times)")
```

**Expected output (LinUCB, 50k episodes):**

```
Episode 10000/50000
  Avg reward (last 10k): 112.34
  Selection frequencies: [0.023 0.187 0.245 0.092 0.134 0.078 0.156 0.085]
  Cumulative regret: 1823.4

Episode 20000/50000
  Avg reward (last 10k): 118.67
  Selection frequencies: [0.015 0.203 0.276 0.088 0.125 0.064 0.178 0.051]
  Cumulative regret: 2941.2

Episode 30000/50000
  Avg reward (last 10k): 121.23
  Selection frequencies: [0.011 0.215 0.289 0.081 0.118 0.053 0.191 0.042]
  Cumulative regret: 3789.8

Episode 40000/50000
  Avg reward (last 10k): 122.14
  Selection frequencies: [0.009 0.218 0.297 0.076 0.113 0.047 0.198 0.042]
  Cumulative regret: 4412.1

Episode 50000/50000
  Avg reward (last 10k): 122.58
  Selection frequencies: [0.008 0.221 0.302 0.073 0.109 0.044 0.202 0.041]
  Cumulative regret: 4897.3

=== Training Complete ===
Total episodes: 50000
Average reward (overall): 118.45
Average reward (last 10k): 122.58
Final cumulative regret: 4897.3

Template selection distribution:
  Neutral        : 0.008 (   412 times)
  Positive CM2   : 0.221 ( 11023 times)
  Private Label  : 0.302 ( 15089 times)
  Popular        : 0.073 (  3641 times)
  Premium        : 0.109 (  5472 times)
  Budget         : 0.044 (  2187 times)
  Discount       : 0.202 ( 10112 times)
  Strategic      : 0.041 (  2064 times)
```

**Interpretation:**

1. **Convergence**: Average reward increases from ~112 to ~123 ($\approx 10\%$ improvement)
2. **Exploration decay**: Neutral template selection drops from 2.3% to 0.8% as confidence grows
3. **Winner templates**: Private Label (30%), Positive CM2 (22%), Discount (20%) dominate
4. **Regret growth**: Cumulative regret ~5000 over 50k episodes -> average per-episode regret ~0.1 (excellent!)

**Key insight:** In this simulator configuration, profitability (CM2) and private-label affinity are strong reward levers; LinUCB discovers this by concentrating probability mass on the corresponding templates.

---

## 6.5 First Experiment---When Bandits Lose

We now turn from the theoretical development to the first experiment. Under the assumptions of [THM-6.1] and [THM-6.2], one might expect LinUCB and Thompson Sampling to match or outperform a strong static template. The first run shows that this expectation can be false under an impoverished feature map.

We deploy the contextual bandits and compare them against the best static template.

### 6.5.1 Experimental Setup: The Most Obvious Features

Before we run the experiment, we need to make a crucial design choice: **what context features do we give the bandits?**

Remember, our linear model assumes
$$
\mu(x, a) = \theta_a^\top \phi(x)
$$
and all of the regret guarantees in [THM-6.1] and [THM-6.2] are conditional on this model being a reasonable approximation of reality.

The feature map $\phi : \mathcal{X} \to \mathbb{R}^d$ is our responsibility. The bandit will learn the best weights $\theta_a$, but we must decide *what* to encode in $\phi(x)$.

What's the most natural choice? Two obvious sources of context:

- **User segments.** Our simulator has four user types: premium buyers who purchase expensive items, pl_lovers who prefer own-brand products, litter_heavy users who buy in bulk, and price_hunters who seek discounts. These segments have genuinely different preferences---a premium user and a price hunter should receive different rankings.
- **Query types.** Users express different intent: specific product searches (e.g. `"royal canin kitten food"`), general browsing (e.g. `"cat supplies"`), and deal-seeking (e.g. `"discounts on cat food"`).

So our first instinct is simple and reasonable:
$$
\phi_{\text{simple}}(x)
  = [\text{segment}_{\text{onehot}}, \text{query\_type}_{\text{onehot}}]
$$

This gives $d = 4 + 3 = 7$ dimensions: a one-hot encoding for user segment (four binary indicators, exactly one is 1), plus a one-hot encoding for query type (three binary indicators, exactly one is 1).

From the bandit's perspective this feels expressive enough. With $\phi_{\text{simple}}$ it can, in principle, learn patterns like:

- "Premium users with specific queries respond well to the Premium template."
- "pl_lover users with browsing queries respond well to Private Label."
- "Price hunters with deal-seeking queries respond well to Budget or Discount templates."

The linear model $\theta_a^\top \phi(x)$ can represent these patterns: each coordinate of $\theta_a$ is simply "how much this segment or query type likes template $a$".

!!! note "Pedagogical Design: Feature Engineering as Iterative Process"
    We deliberately omit product-level information (prices, CM2, discounts, bestseller/popularity signals) and user preference signals (price sensitivity, private-label affinity). This is intentional: feature engineering is iterative. We start with a minimal representation, measure performance, diagnose the bottleneck, and then add the missing signals.

    This first experiment also induces model misspecification to instantiate [REM-6.1.1] in a controlled way: the theorems remain correct, but the representation is too weak for the linear model to be useful.

Segment and query type are also the first features stakeholders typically request. If these features suffice, we obtain an interpretable baseline; if they do not, the failure is informative.

**Experimental protocol.**

We use our standard simulator configuration (10 000 products, realistic distributions) and run three policies using `scripts/ch06/template_bandits_demo.py`:

1. **Static template sweep.** Evaluate each of the 8 templates for 2 000 episodes, and record average Reward/GMV/CM2.
2. **LinUCB.** Train for 20 000 episodes with $\phi_{\text{simple}}$, ridge regularization $\lambda = 1.0$, UCB coefficient $\alpha = 1.0$.
3. **Thompson Sampling.** Train for 20 000 episodes with $\phi_{\text{simple}}$, same regularization and noise scale as in Section 6.4.

Run:

```bash
uv run python scripts/ch06/template_bandits_demo.py \
    --n-static 2000 \
    --n-bandit 20000 \
    --features simple \
    --world-seed 20250322 \
    --bandit-base-seed 20250349
```

We treat the above $(\text{world-seed}, \text{bandit-base-seed}) = (20250322, 20250349)$ as a canonical run for the chapter narrative; Lab 6.5 checks multi-seed robustness of the qualitative conclusions.

Why 20 000 episodes for the bandits but only 2 000 per static template? Static templates are deterministic---once we have a few thousand episodes, we can estimate their means quite precisely. Bandits, however, must **explore**. The regret bounds suggest that with $T = 20\,000$ and $M = 8$ templates, we should pay roughly
$$
O\bigl(\sqrt{M T}\bigr) \approx O\bigl(\sqrt{8 \cdot 20\,000}\bigr) \approx 400
$$
episodes of regret and then enjoy near-optimal behaviour for the remaining 19 600 episodes. On paper that is more than enough to beat any fixed template.

**Our hypothesis:** contextual bandits with segment + query-type features should at least match, and probably exceed, the best static template's GMV.

### 6.5.2 The Moment of Truth: When Bandits Lose

The script runs. Progress indicators tick forward. LinUCB explores aggressively at first (all 8 templates get non-trivial mass), then gradually commits to favourites. Thompson Sampling behaves similarly but with stochastic selection trajectories---its posteriors never collapse to a single arm because variance remains.

After a couple of minutes, the summary prints:

```
Static templates (per-episode averages):
ID  Template             Reward         GMV         CM2
 0  Neutral                5.75        5.28        0.50
 1  Positive CM2           5.44        5.07        0.63
 2  Private Label          7.35        6.73        0.61
 3  Popular                4.79        4.53        0.52
 4  Premium                7.56        7.11        0.74  <- Best static
 5  Budget                 3.44        3.04        0.26
 6  Discount               5.04        4.62        0.45
 7  Strategic              4.83        3.99       -0.13

Best static template: ID=4 (Premium) with avg reward=7.56, GMV=7.11

LinUCB (20000 episodes, simple features):
  Global avg:  Reward=5.62, GMV=5.12, CM2=0.51

Thompson Sampling (20000 episodes, simple features):
  Global avg:  Reward=6.69, GMV=6.18, CM2=0.61
```

Read those lines carefully:

- Best static template (Premium): **7.11 GMV**
- LinUCB with $\phi_{\text{simple}}$: **5.12 GMV** ($\approx -28\%$)
- Thompson Sampling with $\phi_{\text{simple}}$: **6.18 GMV** ($\approx -13\%$)

The bandits lose by double-digit percentages relative to a one-line static rule.

Rerunning with different seeds preserves the qualitative pattern (LinUCB roughly -28%, Thompson Sampling roughly -13% in this setup). The algorithms with clean regret bounds underperform a static Premium template that favours products matching the premium segment's preferences.

### 6.5.3 Cognitive Dissonance and Per-Segment Heterogeneity

If these numbers feel uneasy, that is intended.

On one side we have the theorems from Sections 6.2 and 6.3 telling us:

- "LinUCB and Thompson Sampling achieve $O(d\sqrt{M T \log T})$ regret."
- "Empirical regret curves in synthetic experiments decay nicely (Figure 6.4)."

On the other side we have the simulator reporting:

- "The contextual bandit underperforms the best static template on GMV."

Both statements can be true. The tension between them is the pedagogical engine of this chapter.

The same run includes a per-segment table. For instance:

- Price hunters lose ~50 % GMV when forced into Premium-like boosts.
- PL-lover users lose ~30 % GMV when Private Label is disabled.
- Premium users are already near their Pareto frontier.

The per-segment table makes the paradox sharper: **global GMV is dominated by premium buyers**, but the biggest opportunity lies in underserved segments that the simple features cannot separate properly.

In Section 6.6 we resist the temptation to blame bugs or hyperparameters and instead put the theorems themselves on the table: we read the fine print and identify the violated assumptions.

!!! note "Code <-> Experiment (Simple Features)"
    The simple-feature experiment is implemented in `scripts/ch06/template_bandits_demo.py` (feature construction + evaluation tables) and saved as a JSON artifact by `scripts/ch06/ch06_compute_arc.py`.

    - Feature map: `context_features_simple` (segment + query type + bias).
    - Artifact: `docs/book/ch06/data/template_bandits_simple_summary.json`.

    Reproduce the full Chapter-6 compute arc (simple to rich oracle to rich estimated) and regenerate all three JSON summaries:
    ```bash
    uv run python scripts/ch06/ch06_compute_arc.py \
      --n-static 2000 \
      --n-bandit 20000 \
      --base-seed 20250322 \
      --bandit-seed 20250349 \
      --out-dir docs/book/ch06/data \
      --prior-weight 50 \
      --lin-alpha 0.2 \
      --ts-sigma 0.5
    ```

---

## 6.6 Diagnosis---Why Theory Failed Practice

When an RL algorithm underperforms a simple baseline, there are three possibilities:

1. **Implementation error.** Bug in the code, wrong hyperparameters, or insufficient data.
2. **The theorem is wrong.** The regret bound does not actually hold because the proof has a flaw.
3. **Assumptions are violated.** The theorem is correct, but its hypotheses do not hold in the present setting.

We can rule out (1): the posterior updates match the closed-form equations in Section 6.2 and Section 6.3, tests in `tests/ch06/` pass, and 20 000 episodes is plenty for an 8-arm bandit. We can rule out (2): [THM-6.2] is a widely used result (Abbasi-Yadkori et al.), with a proof that has been checked many times over.

What remains is (3): we violated the assumptions.

### 6.6.1 Revisiting the Theorem's Fine Print

[THM-6.2] states that LinUCB achieves $O(d\sqrt{M T \log T})$ regret under four assumptions:

**(A1) Linearity.** True mean reward is linear in features:
$$
\mu(x, a) = \theta_a^{*\top} \phi(x)
$$
for some unknown $\theta_a^* \in \mathbb{R}^d$ and all $x, a$.

**(A2) Bounded features.** $\|\phi(x)\|_2 \le L$ for all contexts $x$.

**(A3) Bounded parameters.** $\|\theta_a^*\|_2 \le S$ for all arms $a$.

**(A4) Sub-Gaussian noise.** Observed reward is $r_t = \theta_{a_t}^{*\top} \phi_t + \eta_t$ where $\eta_t$ is $\sigma$-sub-Gaussian.

For our simple-feature experiment:

- (A2) holds trivially: $\phi_{\text{simple}}$ is one-hot, so $\|\phi\|_2 = 1$.
- (A3) is a scale assumption: take $S := \max_a \|\theta_a^*\|_2$; it affects constants but not the $\sqrt{T}$ rate.
- (A4) is plausible in a fixed simulator run: conditioning on the sampled catalog and finite `top_k`, per-episode rewards are bounded, hence the noise is sub-Gaussian for some $\sigma$.

That leaves **(A1) linearity**.

In prose, (A1) says: "There exists a linear function of the chosen features that predicts expected reward for every context-action pair." This is a much stronger statement than it looks. It does not say "reward is roughly monotone in some features" or "a linear model works on average". It says that reward lives exactly on a hyperplane in feature space.

### 6.6.2 What Linearity Really Means for $\phi_{\text{simple}}$

With $\phi_{\text{simple}} = [\text{segment}, \text{query\_type}]$, linearity says:
> For each template $a$, there exist numbers $\theta_{a,\text{segment}}$ and $\theta_{a,\text{query}}$ such that the expected GMV is the **sum** of a "segment effect" and a "query-type effect".

Concretely, suppose template 2 (Private Label) has
$$
\theta_2
  = [\theta_{2,\text{premium}},
     \theta_{2,\text{pl\_lover}},
     \theta_{2,\text{litter\_heavy}},
     \theta_{2,\text{price\_hunter}},
     \theta_{2,\text{specific}},
     \theta_{2,\text{browsing}},
     \theta_{2,\text{deal\_seeking}}]^\top.
$$

For a pl_lover user with a browsing query we always have
$$
\phi_{\text{simple}} =
  [0, 1, 0, 0,\ 0, 1, 0]^\top
$$
and so
$$
\mu(x, a = 2)
  = \theta_{2,\text{pl\_lover}} + \theta_{2,\text{browsing}}.
$$

The model assumes that:

- The effect of being a PL-lover is *additive* and independent of which products happen to be available.
- The effect of the query being "browsing" is also additive and independent.
- There is no interaction beyond the sum of these two numbers.

This is where reality diverges.

### 6.6.3 Concrete Counterexample: Two Episodes, Same Features, Different Worlds

Consider two episodes, both labelled as:

- **User:** pl_lover
- **Query type:** browsing

So both have the **same** $\phi_{\text{simple}}$.

**Episode A (PL-friendly shelf).**

- Base ranker's top-$k$ results contain mostly own-brand products (say 80 % PL).
- Prices cluster around EUR15 with healthy margins.
- When we apply Private Label (template 2), the boost pushes even more PL products into the top slots. The user sees a wall of own-brand products they like at acceptable prices and buys two items.

**Episode B (PL-hostile shelf).**

- Base ranker's top-$k$ results contain almost no own-brand products (say 10 % PL).
- Prices cluster around EUR40 with thinner margins.
- Applying Private Label now drags a handful of mediocre PL products up into top positions, replacing highly relevant national-brand products. The user is underwhelmed and leaves without buying.

**Visual summary:**

| Episode | User Segment | Query Type | $\phi_{\text{simple}}$ | Top-K PL Fraction | Private Label Outcome | GMV |
|---------|--------------|------------|------------------------|-------------------|-------------------|-----|
| **A** | pl_lover | browsing | [0,1,0,0,0,1,0] | 80% | Boosts many relevant PL products | **8.2** |
| **B** | pl_lover | browsing | [0,1,0,0,0,1,0] | 10% | Boosts few mediocre PL products | **2.1** |

**Caption:** Episodes A and B are indistinguishable to the bandit (identical $\phi_{\text{simple}}$) but yield **4x different GMV**. The missing information is the PL fraction in the base ranker's top-K---a critical context the simple features do not capture.

**This is model misspecification:** The linear model $\mu(x, a) = \theta_a^\top \phi(x)$ assigns the same expected reward to both episodes, but reality disagrees violently.

From the simulator's point of view, Episodes A and B are completely different: catalog composition, price and margin distributions, and match between user preferences and available products all change. From the bandit's point of view, **they are indistinguishable**---both correspond to the same one-hot vector.

Linearity in $\phi_{\text{simple}}$ therefore fails in the most brutal way: **the same feature vector leads to vastly different expected rewards** depending on hidden variables the bandit cannot see.

### 6.6.4 Feature Poverty and Model Misspecification

This is the essence of **feature poverty**:

- The simulator knows a rich state: user price sensitivity and PL preference, catalog price/margin/discount distributions, base-ranker relevance scores, etc.
- The bandit only sees a 7-dim feature vector encoding segment and query type.

The result is a **misspecified model**:

- The true reward $\mu(x,a)$ depends on rich interactions between user preferences and product attributes.
- The linear model is forced to explain these interactions using only segment and query labels.

In this regime, regret guarantees still hold in a narrow sense: LinUCB and TS quickly find the *best linear policy on $\phi_{\text{simple}}$*. But the best linear policy in such a poor feature space may simply be "pick the least bad static template", which is exactly what we observe.

The take-away from Section 6.6 is not "LinUCB is bad" or "Thompson Sampling fails." It is:

> Regret bounds are guarantees **conditional on the feature representation.**

With $\phi_{\text{simple}}$ we gave the algorithms a bad hypothesis class. The failure is on us, not on the theorems.

In Section 6.7 we fix the right thing: we redesign the features.

---

## 6.7 Retry with Rich Features

We have diagnosed the problem: **feature poverty**. Our 7-dimensional one-hot encoding of segment and query type does not capture the information needed to learn a good policy. The bandit cannot see what products are in the result set, cannot see user preferences beyond crude segment labels, cannot see how well the base ranker matched the query.

The fix is conceptually simple: **give the bandit better features.**

The hard part is choosing features that:

1. **Capture reward drivers** --- the information the simulator uses to compute GMV, CM2, and engagement.
2. **Remain action-independent** --- they cannot depend on which template we are *about* to choose.
3. **Stay fixed-dimensional** --- linear models need $\phi(x) \in \mathbb{R}^d$ with constant $d$.
4. **Avoid leakage** --- no peeking at future clicks or using ground-truth labels that would not be available in production.

### 6.7.1 Aggregates over the Base Ranker's Top-K

The key design idea is to compute features from the **base ranker's top-$K$ results**, *before* applying any template.

The base ranker from Chapter 5 already scores products by relevance. Given a query and catalog, it produces a ranking. We can take the top $K$ products from this ranking (we use $K=20$ in the demo) and compute aggregates:

- Average and standard deviation of price.
- Average CM2 (contribution margin, in currency units).
- Average discount.
- Fraction of own-brand (PL) products.
- Fraction of products in strategic categories.
- Average bestseller score (our popularity proxy).
- Average relevance score from the base ranker.

These statistics summarize **what the shelf looks like** before boosting. Crucially, they are **action-independent**: the same regardless of which template the bandit will choose.

### 6.7.2 Oracle vs. Estimated: The Production Reality Gap

We diagnosed feature poverty. Now we fix it. But in fixing it, we face a choice that reveals something deeper about algorithm selection.

Our simulator knows each user's true latent preferences---their exact price sensitivity ($\theta_{\text{price}}$) and private-label affinity ($\theta_{\text{pl}}$). In production, we do not have this luxury. Real systems estimate preferences from noisy behavioral signals: clicks, dwell time, and purchase history.

This distinction matters materially for algorithm selection. We run two experiments with rich features:

1. **Oracle latents**: We give the bandit the true user preferences (an idealized benchmark).

2. **Estimated latents**: We give the bandit noisy estimates of preferences (a production-style setting).

The comparison isolates the role of feature noise:
- With oracle features, both LinUCB and Thompson Sampling achieve roughly +32% GMV over the best static template (for our reference seed, LinUCB is marginally higher).
- With estimated features, Thompson Sampling remains near +31% GMV while LinUCB drops to roughly +6%.

This is the chapter's deepest lesson: **algorithm selection depends on feature quality**.

### 6.7.3 Building $\phi_{\text{rich}}$: From 7 to 17 Dimensions

We start from the simple features:
$$
\phi_{\text{simple}}(x)
  = [\text{segment one-hot} (4), \text{query-type one-hot} (3)]
  \in \{0,1\}^7.
$$

We then add:

1. **User latent preferences (2 dims).**

   The simulator represents each user with continuous parameters
   $\theta_{\text{price}} \in [-1,1]$ (price sensitivity) and
   $\theta_{\text{pl}} \in [-1,1]$ (preference for own-brand).
   In the "oracle rich" experiment we feed these true values into the feature map.

2. **Base-top-$K$ aggregates (8 dims).**

   Over the top-$K$ products under the **base** ranker we compute:

   - `avg_price`, `std_price`
   - `avg_cm2`, `avg_discount`
   - `frac_pl`, `frac_strategic`
   - `avg_bestseller`, `avg_relevance`

Putting everything together we obtain a 17-dimensional feature vector:
$$
\phi_{\text{rich}}(x) \in \mathbb{R}^{17}.
$$

In `scripts/ch06/template_bandits_demo.py` this is implemented as
`context_features_rich`. The function concatenates segment and query one-hots,
user preferences $(\theta_{\text{price}}, \theta_{\text{pl}})$, and the eight
aggregates, then applies a simple z-normalization using fixed means and
standard deviations baked into the script.

!!! note "Code <-> Features (Rich Mode)"
    Rich features are computed in `scripts/ch06/template_bandits_demo.py`:

    - `context_features_rich`: oracle user latents + base-top-$K$ aggregates
    - `context_features_rich_estimated`: same structure with *estimated* latents
    - `feature_mode` CLI flag: `--features {simple,rich,rich_est}`

    The policy classes in `zoosim/policies/{lin_ucb,thompson_sampling}.py`
    simply consume the resulting $\phi(x)$; all the modelling decisions about
    *what* to expose live in the feature functions.

### 6.7.4 Experiment A: Rich Features with Oracle Latents

We now run exactly the same experiment as in Section 6.5, but with $\phi_{\text{rich}}$ containing the **true user latent preferences**:

```bash
uv run python scripts/ch06/template_bandits_demo.py \
    --n-static 2000 \
    --n-bandit 20000 \
    --features rich \
    --world-seed 20250322 \
    --bandit-base-seed 20250349 \
    --hparam-mode rich_est \
    --prior-weight 50 \
    --lin-alpha 0.2 \
    --ts-sigma 0.5
```

The simulator, templates, and basic hyperparameters are unchanged. Only the features differ---now enriched with oracle user latents and base-top-$K$ aggregates.

The output now shows a reversal relative to Section 6.5:

```
Static templates (per-episode averages):
ID  Template             Reward         GMV         CM2
 0  Neutral                5.75        5.28        0.50
 1  Positive CM2           5.44        5.07        0.63
 2  Private Label          7.35        6.73        0.61
 3  Popular                4.79        4.53        0.52
 4  Premium                7.56        7.11        0.74  <- Best static
 5  Budget                 3.44        3.04        0.26
 6  Discount               5.04        4.62        0.45
 7  Strategic              4.83        3.99       -0.13

Best static template: ID=4 (Premium) with avg reward=7.56, GMV=7.11

LinUCB (20000 episodes, rich oracle features):
  Global avg:  Reward=10.19, GMV=9.42, CM2=0.97

Thompson Sampling (20000 episodes, rich oracle features):
  Global avg:  Reward=10.15, GMV=9.39, CM2=0.97
```

Read these numbers carefully---the **algorithm ranking has reversed**:

| Algorithm | GMV | vs. Static Best |
|-----------|-----|-----------------|
| Static (Premium) | 7.11 | baseline |
| **LinUCB** | **9.42** | **+32.5%** |
| Thompson Sampling | 9.39 | +32.1% |

With oracle user latents, **both algorithms perform excellently**---and nearly identically! The clean features allow both LinUCB's UCB bonus and Thompson Sampling's posterior sampling to converge efficiently to the optimal policy. LinUCB edges out TS by a razor-thin margin (+0.4 percentage points), but the practical difference is negligible.

This makes theoretical sense: LinUCB's regret bound [THM-6.2] is proved under a realizability assumption that the conditional mean reward is linear in the features. Oracle latents expose major drivers of user response and therefore drastically reduce model misspecification. The environment remains nonlinear (slate interactions and click/purchase mechanics), so realizability is not exact; rather, the approximation error becomes small enough that LinUCB behaves close to the well-specified regime in this experiment.

### 6.7.5 Experiment B: Rich Features with Estimated Latents

The crucial twist is that in production we do not have oracle user latents. Real systems estimate user preferences from observed behavior---clicks, purchases, and browsing patterns. These estimates are noisy, delayed, and sometimes wrong.

We run the same experiment with estimated latents instead of oracle:

```bash
uv run python scripts/ch06/template_bandits_demo.py \
    --n-static 2000 \
    --n-bandit 20000 \
    --features rich_est \
    --world-seed 20250322 \
    --bandit-base-seed 20250349 \
    --hparam-mode rich_est \
    --prior-weight 50 \
    --lin-alpha 0.2 \
    --ts-sigma 0.5
```

The output reveals the production reality:

```
LinUCB (20000 episodes, rich estimated features):
  Global avg:  Reward=8.20, GMV=7.52, CM2=0.76

Thompson Sampling (20000 episodes, rich estimated features):
  Global avg:  Reward=10.08, GMV=9.31, CM2=0.97
```

Now the algorithm ranking **diverges dramatically**:

| Algorithm | GMV | vs. Static Best |
|-----------|-----|-----------------|
| Static (Premium) | 7.11 | baseline |
| LinUCB | 7.52 | +5.8% |
| **Thompson Sampling** | **9.31** | **+31.0%** |

With estimated (noisy) features, Thompson Sampling wins decisively by 25 percentage points.

### 6.7.6 The Algorithm Selection Principle

These two experiments reveal the chapter's deepest lesson:

!!! tip "Algorithm Selection Depends on Feature Quality"
    $$
    \text{Clean/Oracle features} \rightarrow \text{LinUCB (precise exploitation)}
    $$
    $$
    \text{Noisy/Estimated features} \rightarrow \text{Thompson Sampling (robust exploration)}
    $$

**Why does this happen?**

- **LinUCB** constructs
  $$
  \text{UCB}_a(x) = \hat{\theta}_a^\top \phi(x) +
    \alpha \sqrt{\phi(x)^\top A_a^{-1} \phi(x)}
  $$
  and becomes nearly deterministic as the uncertainty term shrinks. With clean oracle features, this precision is a virtue---LinUCB converges quickly to the optimal policy. But with noisy features, LinUCB can **lock into a suboptimal template** based on spurious correlations in the estimated latents.

- **Thompson Sampling** samples
  $\theta_a \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)$ each round. Even after 20,000 episodes, the posterior covariance $\Sigma_a$ retains some mass---noise and misspecification prevent total collapse. TS therefore **never fully stops exploring**, hedging against feature noise.

**Production implication:** Since production systems invariably have noisy estimated features (not oracle access to true user preferences), Thompson Sampling is a robust default. LinUCB is appealing when feature quality is high and deterministic behavior is valuable (e.g., debugging and A/B analysis).

This mirrors empirical findings in the broader RL literature: posterior-sampling and ensemble-based methods often outperform hard UCB-style bonuses in complex environments [@russo:tutorial_ts:2018, @osband:bootstrapped_dqn:2016].

### 6.7.7 Per-Segment Breakdown: Who Actually Benefits?

Global GMV averages hide important heterogeneity. The script reports per-segment metrics such as:

```
Per-segment GMV (static best vs bandits, rich features):
Segment          Static GMV  LinUCB GMV      TS GMV    LinUCB Delta%     TS Delta%
premium               31.60       31.30       31.95       -0.9%      +1.1%
pl_lover               5.34       11.56       11.85      +116.4%    +121.9%
litter_heavy           4.94        6.60        6.28       +33.6%     +27.2%
price_hunter           0.00        0.00        0.00        +0.0%      +0.0%
```

Both bandits more than double GMV for the pl_lover segment and improve litter_heavy, while keeping premium GMV essentially unchanged in this oracle-feature run.

From a business perspective, this is exactly the sort of trade-off we want to understand:

- TS learns a **balanced** policy: keep premium shoppers happy *and* fix underserved cohorts.
- LinUCB finds a **PL-centric** policy: great for PL-lovers, less so for premium.

The same per-segment machinery also works with the `--show-volume` flag, which logs order counts alongside GMV/CM2. In the labs we replicate plots showing how bandits can triple order volume for some segments even when global GMV barely moves.

---

## 6.8 Summary & What's Next

We now summarize the technical artifacts, the empirical compute arc, and the lessons that guide the remainder of the book. This chapter deliberately includes a failure mode and its diagnosis: we build a correct implementation, observe a negative result under an impoverished representation, and then fix the representation.

### 6.8.1 What We Built

On the technical side:

- **Discrete template action space** (Section 6.1): 8 interpretable boost strategies encoding business logic (Positive CM2, Private Label, Premium, Budget, Discount, etc.).
- **Thompson Sampling and LinUCB theory** (Sections 6.2--6.3): posterior sampling and UCB for linear contextual bandits, with sublinear regret guarantees under explicit assumptions.
- **Production-quality implementations** (Section 6.4): type-hinted NumPy code in `zoosim/policies/thompson_sampling.py` and `zoosim/policies/lin_ucb.py`, wired to the simulator via `scripts/ch06/template_bandits_demo.py`.

On the empirical side (the three-stage compute arc):

- **Stage 1: Simple-feature experiment** (Section 6.5): with $\phi_{\text{simple}}$ (segment + query type, $d=8$), both bandits fail. LinUCB lands at 5.12 GMV (-28%), TS at 6.18 GMV (-13%).
- **Stage 2: Diagnosis** (Section 6.6): we locate the culprit in violated assumptions---feature poverty and linear model misspecification---not in bugs or lack of data.
- **Stage 3a: Rich features + oracle latents** (Section 6.7.4): with $\phi_{\text{rich}}$ containing true user latents ($d=18$), **both algorithms excel**---LinUCB at 9.42 GMV (+32.5%), TS at 9.39 GMV (+32.1%). Near-perfect tie.
- **Stage 3b: Rich features + estimated latents** (Section 6.7.5): same rich features but with estimated (noisy) latents, **TS wins decisively** at 9.31 GMV (+31.0%), LinUCB at 7.52 GMV (+5.8%).

### 6.8.2 Five Lessons

**Lesson 1 --- Regret bounds are conditional guarantees.**

The guarantee in [THM-6.2] is of the form:

> *If* $\mu(x,a)$ is linear in the chosen features and the other assumptions hold, *then* LinUCB finds a near-optimal linear policy efficiently.

It does not say that LinUCB discovers the globally optimal policy for the environment. With $\phi_{\text{simple}}$ the best linear policy is simply bad; the theorem holds, but the outcome is still disappointing. Whenever we use theoretical guarantees in practice, we trace each assumption back to a concrete property of the system.

**Lesson 2 --- Feature engineering sets the performance ceiling.**

We changed nothing about the environment, templates, or algorithms between Sections 6.5 and 6.7. Only the features changed. Yet the GMV numbers swung from "-13% vs. static" to "+31% vs. static".

We cannot learn what the features do not expose. In contextual bandits (and in deep RL, despite the flexibility of neural networks) representation design is policy design.

**Lesson 3 --- Simple baselines encode valuable domain knowledge.**

The Premium template is a one-line heuristic that captures a deep business insight: boosting premium products maximizes revenue per purchase. It wins by default in the simple-feature regime and remains competitive even with rich features.

Hand-crafted baselines like Premium define a **safe lower bound** and a **warm start** for learning. Bandits should be evaluated relative to them, not in isolation.

**Lesson 4 --- Failure is a design signal, not an embarrassment.**

The -28%/-13% GMV numbers in Section 6.5 are not a sign that bandits are useless. They are a sign that the modelling choices are wrong. Because we looked at the failure honestly, we discovered precisely which assumptions broke and how to correct them.

In production RL, we experience many such failures. The playbook from this chapter is:

1. Start from a strong baseline and articulate expectations.
2. When the algorithm underperforms, enumerate the theorem's assumptions.
3. Diagnose feature poverty vs. model misspecification vs. data issues.
4. Fix the representation first; reach for more complex algorithms only if needed.

**Lesson 5 --- Algorithm selection depends on feature quality.**

The contrast between Section 6.7.4 (oracle latents) and Section 6.7.5 (estimated latents) reveals the chapter's deepest insight: **the same features can favor different algorithms depending on noise level**.

- With *clean, oracle features*, both algorithms excel equally (~+32% each)---the "scalpel" and the "Swiss Army knife" are equally sharp when data is perfect.
- With *noisy, estimated features*, Thompson Sampling's robust exploration wins decisively (+31% vs. LinUCB's +6%).

Production systems invariably have noisy features---estimated from clicks, inferred from behavior, aggregated from proxies. We default to Thompson Sampling in production and use LinUCB when feature quality is high and deterministic behavior is valuable (e.g., direct measurements or carefully validated latent estimates).

### 6.8.3 Where to Go Next

**For hands-on practice:**

`docs/book/ch06/exercises_labs.md` contains exercises and labs that cover:

- **Lab 6.1**: Reproducing the simple-feature failure (Stage 1)
- **Lab 6.2a**: Rich features with oracle latents---Both excel (Stage 3a)
- **Lab 6.2b**: Rich features with estimated latents---TS wins (Stage 3b)
- **Lab 6.2c**: Synthesis---understanding the algorithm selection principle
- **Labs 6.3-6.5**: Hyperparameter sensitivity, exploration dynamics, multi-seed robustness
- **Exercises 6.1-6.14**: Theoretical proofs, implementation challenges, and ablation studies

**For practitioners scaling experiments:**

When running many seeds, feature variants, or large episode counts (100k+), the CPU implementation (Section 6.4) becomes slow (~30 seconds per run). See the optional **Advanced Lab 6.A: From CPU Loops to GPU Batches** (`ch06_advanced_gpu_lab.md`).

**Prerequisites:** Completion of main Chapter 6 narrative + Labs 6.1-6.3, CUDA-capable GPU

**Time budget:** 2-3 hours (spread over multiple sessions)

**Topics covered:** Batch-parallel simulation on GPU, correctness verification via parity checks, when GPU acceleration matters, and how to migrate CPU code safely.

**The GPU lab teaches production skills:** Vectorization, device management, seed alignment, and parity testing---techniques we use when scaling any RL experiment.

**If we do not have a GPU or are not planning large-scale experiments, we skip this lab.** The CPU implementation is sufficient for understanding the algorithms.

### 6.8.4 Extensions & Practice

For practitioners planning production deployment or interested in advanced topics, see **`appendices.md`** for:

- **Appendix 6.A: Neural Linear Bandits** --- Representation learning with neural networks, when to use vs. avoid, PyTorch implementation
- **Appendix 6.B: Theory-Practice Gap Analysis** --- What theory guarantees vs. what we implement, why it works anyway, failure modes, recent work (2020-2025), open problems
- **Appendix 6.C: Modern Context & Connections** --- Industry deployments (Netflix, Spotify, Microsoft Bing), contextual bandits vs. Deep RL, bandits vs. Offline RL
- **Appendix 6.D: Production Checklist** --- Configuration alignment, guardrails, reproducibility, monitoring, testing

These appendices deepen and generalize the core narrative but are **optional**. You can return to them after completing Chapter 7.

**Chapter 7 preview:** We take the core insight---"features and templates constrain what we can learn"---and apply it to continuous actions via $Q(x,a)$ regression, where templates become vectors in a high-dimensional action space.

## Exercises & Labs

See `docs/book/ch06/exercises_labs.md` for:

- **Exercise 6.1**: Prove [PROP-5.1] properties of semantic relevance (warmup)
- **Exercise 6.2**: Implement epsilon-greedy baseline, compare regret to LinUCB
- **Exercise 6.3**: Derive ridge regression solution $\hat{\theta} = (A^\top A + \lambda I)^{-1} A^\top b$
- **Exercise 6.4**: Verify LinUCB and TS have identical posterior mean (mathematical proof)
- **Exercise 6.5**: Implement Cholesky-based sampling for Thompson Sampling
- **Exercise 6.6**: Add new template "Category Diversity" that boosts underrepresented categories
- **Exercise 6.7**: Implement hierarchical templates (meta-template selects category, sub-template selects boost)
- **Exercise 6.8**: Conduct ablation study: Remove features one-by-one, measure regret impact
- **Exercise 6.9**: Extend templates to be query-conditional (different templates per query type)
- **Exercise 6.10**: Implement UCB with adaptive $\alpha_t = c\sqrt{\log(1+t)}$, tune $c$
- **Exercise 6.11**: Add polynomial features $[\phi(x), \phi(x)^2]$, compare linear vs. quadratic
- **Exercise 6.12**: Implement Neural Linear bandit, train on 20k episodes, evaluate
- **Exercise 6.13**: Extend training to 100k episodes, verify +10% GMV vs. best static
- **Exercise 6.14**: Add time-of-day feature, show bandits learn diurnal patterns

**Lab 6.1: Hyperparameter Sensitivity**

Grid search over $(\lambda, \alpha) \in \{0.1, 1.0, 10.0\} \times \{0.5, 1.0, 2.0\}$. Plot heatmap of final reward.

**Lab 6.2: Visualization**

Plot:
1. Template selection heatmap (template vs. episode, color = selection frequency)
2. Uncertainty evolution (trace($\Sigma_a$) vs. episode for each template)
3. Regret decomposition (per-template contribution to cumulative regret)

**Lab 6.3: Multi-Seed Evaluation**

Run LinUCB with 10 different seeds, report mean $\pm$ standard deviation of final GMV. Verify robustness.

---

## Summary

**What we built:**

1. **Discrete template action space** (8 interpretable boost strategies)
2. **Thompson Sampling** (Bayesian posterior sampling, automatic exploration)
3. **LinUCB** (frequentist UCB, deterministic, tunable exploration)
4. **Production implementation** (PyTorch/NumPy, numerical stability, diagnostics)
5. **Full integration** with `zoosim` search simulator
6. **Experimental validation** (learning curves, exploration dynamics, baselines)

**Key results:**

- Bandits can beat the best static template by a few percent GMV (context-dependent).
- Regret grows sublinearly under the model assumptions (Sections 6.2--6.3).
- Exploration decays automatically in Thompson Sampling (no manual schedule needed).
- Policies remain interpretable (template selection and scores are inspectable).

**What's next:**

- **Chapter 7**: Continuous actions via $Q(x, a)$ regression (move beyond discrete templates)
- **Chapter 10**: Hard constraints (CM2 floor, exposure, rank stability) via Lagrangian methods
- **Chapter 11**: Multi-session MDPs (retention, long-term value optimization)
- **Chapter 13**: Offline RL (learn from logged data without online interaction)

**The textbook journey:**

We've now built a **production-ready RL system** that:
- Starts with strong priors (base ranker + templates)
- Learns from interaction (bandit algorithms)
- Balances exploration and exploitation (provable regret bounds)
- Remains interpretable (template selection visible to business)

From here, we scale to more complex action spaces (continuous boosts, slate optimization) and harder constraints (multi-objective optimization, safety guarantees). The foundation is solid, and we now continue.

---

**Reproducibility checklist (Chapter 6).**

- Core implementations: `zoosim/policies/{templates,thompson_sampling,lin_ucb}.py`
- Experiments: `scripts/ch06/template_bandits_demo.py`, `scripts/ch06/ch06_compute_arc.py`
- Tests: `tests/ch06/` (run with `uv run pytest tests/ch06 -q`)
- Canonical artifacts: `docs/book/ch06/data/template_bandits_{simple,rich_oracle,rich_estimated}_summary.json`
- End-to-end verification script: `scripts/ch06/run_full_verification.sh` (writes to `docs/book/ch06/data/verification_<timestamp>/`)


# Chapter 6 --- Exercises & Labs

This file contains exercises and labs for **Chapter 6: Discrete Template Bandits**.

**Time budget:** 90-120 minutes total
**Difficulty calibration:** Graduate-level RL, assumes familiarity with probability, linear algebra, and Python

---

## Theory Exercises (30 min total)

### Exercise 6.1: Properties of Cosine Similarity (10 min)

**Problem:**

Consider semantic relevance $s_{\text{sem}}(\mathbf{q}, \mathbf{e}) = \frac{\mathbf{q} \cdot \mathbf{e}}{\|\mathbf{q}\|_2 \|\mathbf{e}\|_2}$ from [DEF-5.2] (Chapter 5 reference, used in template features).

Prove the following properties:

(a) $s_{\text{sem}}(\mathbf{q}, \mathbf{e}) \in [-1, 1]$ for all nonzero $\mathbf{q}, \mathbf{e} \in \mathbb{R}^d$

(b) $s_{\text{sem}}(\alpha \mathbf{q}, \beta \mathbf{e}) = \text{sign}(\alpha \beta) \cdot s_{\text{sem}}(\mathbf{q}, \mathbf{e})$ for all $\alpha, \beta \neq 0$

(c) If $\mathbf{e}_1, \mathbf{e}_2$ are orthogonal ($\mathbf{e}_1 \perp \mathbf{e}_2$), then $s_{\text{sem}}(\mathbf{q}, \mathbf{e}_1 + \mathbf{e}_2) \neq s_{\text{sem}}(\mathbf{q}, \mathbf{e}_1) + s_{\text{sem}}(\mathbf{q}, \mathbf{e}_2)$ in general

*Hint: Use Cauchy-Schwarz inequality for (a). For (c), construct a counterexample.*

**Solution:**

See `docs/book/ch06/ch06_lab_solutions.md` (Exercise 6.1) for a complete proof and numerical verification.

---

### Exercise 6.2: Ridge Regression Closed Form (15 min)

**Problem:**

In [ALG-6.2] (LinUCB), we update weights via:
$$
\hat{\theta}_a = A_a^{-1} b_a
$$
where $A_a = \lambda I + \sum_{t: a_t = a} \phi_t \phi_t^\top$ and $b_a = \sum_{t: a_t = a} r_t \phi_t$.

(a) Prove this is equivalent to the ridge regression solution:
$$
\hat{\theta}_a = \arg\min_\theta \left\{ \sum_{t: a_t = a} (r_t - \theta^\top \phi_t)^2 + \lambda \|\theta\|^2 \right\}
$$

(b) Show that as $\lambda \to 0$, the solution converges to ordinary least squares (OLS):
$$
\hat{\theta}_a^{\text{OLS}} = \left(\sum \phi_t \phi_t^\top\right)^{-1} \sum r_t \phi_t
$$

(c) Explain why $\lambda > 0$ is critical for numerical stability when $\sum \phi_t \phi_t^\top$ is singular or ill-conditioned.

*Hint: For (a), take the gradient of the objective, set to zero. For (c), discuss condition number.*

**Solution:**

*Proof of (a):*

The objective is:
$$
J(\theta) = \sum_{i=1}^n (r_i - \theta^\top \phi_i)^2 + \lambda \|\theta\|^2
$$

Expanding:
$$
J(\theta) = \sum_i (r_i^2 - 2r_i \theta^\top \phi_i + \theta^\top \phi_i \phi_i^\top \theta) + \lambda \theta^\top \theta
$$

Taking the gradient with respect to $\theta$:
$$
\nabla_\theta J = \sum_i (-2 r_i \phi_i + 2 \phi_i \phi_i^\top \theta) + 2\lambda \theta
$$

Setting $\nabla_\theta J = 0$:
$$
\sum_i \phi_i \phi_i^\top \theta + \lambda \theta = \sum_i r_i \phi_i
$$

Rearranging:
$$
\left(\sum_i \phi_i \phi_i^\top + \lambda I\right) \theta = \sum_i r_i \phi_i
$$

Thus:
$$
\hat{\theta} = \left(\sum \phi_i \phi_i^\top + \lambda I\right)^{-1} \sum r_i \phi_i = A^{-1} b \quad \checkmark
$$

*Proof of (b):*

As $\lambda \to 0$:
$$
\hat{\theta}_a = \left(\sum \phi_t \phi_t^\top + \lambda I\right)^{-1} \sum r_t \phi_t \to \left(\sum \phi_t \phi_t^\top\right)^{-1} \sum r_t \phi_t = \hat{\theta}_a^{\text{OLS}}
$$

assuming $\sum \phi_t \phi_t^\top$ is invertible.

*Answer to (c):*

When $\sum \phi_t \phi_t^\top$ is rank-deficient (e.g., fewer samples than features, or collinear features), OLS solution is undefined or numerically unstable (large condition number). Regularization $\lambda I$ adds $\lambda$ to all eigenvalues, bounding the condition number:
$$
\kappa(A) = \frac{\lambda_{\max}(\sum \phi \phi^\top) + \lambda}{\lambda_{\min}(\sum \phi \phi^\top) + \lambda} \leq \frac{\lambda_{\max}}{\lambda}
$$

This prevents numerical blow-up during matrix inversion.

---

### Exercise 6.3: Thompson Sampling vs. LinUCB Posterior (5 min)

**Problem:**

Show that Thompson Sampling ([ALG-6.1]) and LinUCB ([ALG-6.2]) maintain **identical posterior mean** $\hat{\theta}_a$ after observing the same data.

Specifically, verify that:
- Thompson Sampling, written in sufficient-statistics (ridge) form [EQ-6.8], has posterior mean $\hat{\theta}_a = A_a^{-1} b_a$
- LinUCB maintains the same $(A_a, b_a)$ and sets $\hat{\theta}_a = A_a^{-1} b_a$

In particular, when $\sigma^2=1$ we have $\Sigma_a = A_a^{-1}$, so the Bayesian posterior mean coincides with the ridge regression estimate.

**Solution:**

**Claim:** Under $\Sigma_a^{-1} = A_a$ and $\sigma^2 = 1$, the posterior mean
$\hat{\theta}_a$ is identical for Thompson Sampling ([ALG-6.1]) and LinUCB ([ALG-6.2]).

**Thompson Sampling.**
After $n$ observations $\{(\phi_i, r_i)\}_{i=1}^n$ for a fixed action $a$,
the Bayesian linear regression update ([EQ-6.8]) gives
$$
\Sigma_a^{-1} = \lambda I + \sum_{i=1}^n \phi_i \phi_i^\top,
$$
and, with zero-mean prior,
$$
\hat{\theta}_a
  = \Sigma_a \left(\sum_{i=1}^n r_i \phi_i\right).
$$

**LinUCB.**
With the same data, [ALG-6.2] maintains
$$
A_a = \lambda I + \sum_{i=1}^n \phi_i \phi_i^\top,
\qquad
b_a = \sum_{i=1}^n r_i \phi_i,
$$
and sets
$$
\hat{\theta}_a = A_a^{-1} b_a.
$$

**Equivalence.**
Identifying $\Sigma_a^{-1} = A_a$ shows
$$
\hat{\theta}_a^{\text{TS}}
  = \Sigma_a \left(\sum_{i=1}^n r_i \phi_i\right)
  = A_a^{-1} b_a
  = \hat{\theta}_a^{\text{LinUCB}}.
$$

Thus both algorithms maintain the **same ridge regression estimate** for each action; the only difference lies in **action selection**:
- Thompson Sampling samples $\tilde{\theta}_a \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)$ and selects $\arg\max_a \tilde{\theta}_a^\top \phi$
- LinUCB uses the deterministic UCB rule $\hat{\theta}_a^\top \phi + \alpha \sqrt{\phi^\top \Sigma_a \phi}$ and selects $\arg\max_a$

---

## Implementation Exercises (40 min total)

### Exercise 6.4: $\varepsilon$-Greedy Baseline (15 min)

**Problem:**

Implement an $\varepsilon$-greedy policy for template selection:

1. With probability $\epsilon$: Select template uniformly at random
2. With probability $1 - \epsilon$: Select template with highest estimated mean reward

Use the same ridge regression updates as LinUCB, but replace UCB exploration with $\varepsilon$-greedy.

**Specification:**

```python
class EpsilonGreedy:
    """Epsilon-greedy policy for contextual bandits.

    Args:
        templates: List of M boost templates
        feature_dim: Feature dimension d
        epsilon: Exploration rate in [0, 1]
        lambda_reg: Ridge regression regularization
        seed: Random seed
    """
    def __init__(self, templates, feature_dim, epsilon=0.1, lambda_reg=1.0, seed=42):
        # Initialize similar to LinUCB
        pass

    def select_action(self, features):
        """Select template using epsilon-greedy.

        With probability epsilon: random action
        With probability 1-epsilon: argmax_a theta_hat_a^T phi
        """
        # TODO: Implement
        pass

    def update(self, action, features, reward):
        """Ridge regression update (same as LinUCB)."""
        # TODO: Implement
        pass
```

Run on simulator for 50k episodes with $\epsilon \in \{0.05, 0.1, 0.2\}$. Compare cumulative regret to LinUCB.

**Expected result:** $\varepsilon$-greedy has **linear regret** $O(\epsilon T)$ (constant exploration never stops), while LinUCB has **sublinear regret** $O(\sqrt{T})$.

**Solution:**

See `docs/book/ch06/ch06_lab_solutions.md` (Exercise 6.4) and the runnable implementation in `scripts/ch06/lab_solutions/`.

---

### Exercise 6.5: Cholesky-Based Thompson Sampling (20 min)

**Problem:**

The current TS implementation ([CODE-6.X]) samples from $\mathcal{N}(\hat{\theta}_a, \Sigma_a)$ by:
```python
Sigma_a = np.linalg.inv(Sigma_inv[a])
theta_tilde = np.random.multivariate_normal(theta_hat[a], Sigma_a)
```

This requires matrix inversion **every episode** (expensive for large $d$).

**Optimized approach:** Maintain Cholesky factor $L_a$ where $\Sigma_a^{-1} = L_a L_a^\top$. Sample via:
$$
\tilde{\theta}_a = \hat{\theta}_a + L_a^{-T} z, \quad z \sim \mathcal{N}(0, I)
$$

Implement this optimization:

1. Precompute $L_a = \text{cholesky}(\Sigma_a^{-1})$ after each update
2. Sample: Solve $L_a^\top v = z$ for $v$, then $\tilde{\theta}_a = \hat{\theta}_a + v$

Benchmark: Compare runtime for $d \in \{10, 50, 100, 500\}$ over 1000 episodes.

**Expected speedup:** $5$-$10 \times$ faster for $d \geq 100$.

**Solution:**

```python
import numpy as np
from scipy.linalg import solve_triangular

class FastThompsonSampling:
    def __init__(self, M, d, lambda_reg=1.0):
        self.M, self.d = M, d
        self.theta_hat = np.zeros((M, d))
        self.Sigma_inv = np.array([lambda_reg * np.eye(d) for _ in range(M)])
        self.cholesky_factors = [np.linalg.cholesky(self.Sigma_inv[a]) for a in range(M)]

    def select_action(self, features):
        theta_samples = []
        for a in range(self.M):
            # Sample z ~ N(0, I)
            z = np.random.randn(self.d)

            # Solve L_a^T v = z for v
            v = solve_triangular(self.cholesky_factors[a].T, z, lower=False)

            # theta_tilde = theta_hat + v
            theta_tilde = self.theta_hat[a] + v
            theta_samples.append(theta_tilde)

        expected_rewards = [theta_samples[a] @ features for a in range(self.M)]
        return int(np.argmax(expected_rewards))

    def update(self, action, features, reward):
        a = action
        phi = features.reshape(-1, 1)

        # Update precision
        self.Sigma_inv[a] += phi @ phi.T

        # Update Cholesky factor
        self.cholesky_factors[a] = np.linalg.cholesky(self.Sigma_inv[a])

        # Update mean
        Sigma_a = np.linalg.inv(self.Sigma_inv[a])
        self.theta_hat[a] = Sigma_a @ (self.Sigma_inv[a] @ self.theta_hat[a] + phi.flatten() * reward)
```

**Benchmark code:**

```python
import time

for d in [10, 50, 100, 500]:
    # Naive TS
    policy_naive = LinearThompsonSampling(templates, d, ...)
    start = time.time()
    for t in range(1000):
        features = np.random.randn(d)
        action = policy_naive.select_action(features)
        policy_naive.update(action, features, np.random.randn())
    time_naive = time.time() - start

    # Cholesky TS
    policy_fast = FastThompsonSampling(M=8, d=d, lambda_reg=1.0)
    start = time.time()
    for t in range(1000):
        features = np.random.randn(d)
        action = policy_fast.select_action(features)
        policy_fast.update(action, features, np.random.randn())
    time_fast = time.time() - start

    print(f"d={d:3d}: Naive {time_naive:.3f}s, Cholesky {time_fast:.3f}s, Speedup {time_naive/time_fast:.1f}x")

# Expected output:
# d= 10: Naive 0.123s, Cholesky 0.098s, Speedup 1.3x
# d= 50: Naive 0.456s, Cholesky 0.089s, Speedup 5.1x
# d=100: Naive 1.234s, Cholesky 0.145s, Speedup 8.5x
# d=500: Naive 28.45s, Cholesky 2.341s, Speedup 12.2x
```

---

### Exercise 6.6: Add Category Diversity Template (5 min)

**Problem:**

Extend the template library (Section 6.1.1) with a new template:

**Template ID 8: Category Diversity**

Boost products from **underrepresented categories** in the current result set to increase diversity.

**Algorithm:**
1. Count category frequencies in top-$k$ results
2. Boost products from categories with count < $k / C$ where $C$ is number of categories

**Implementation:**

```python
def create_diversity_template(catalog_stats, a_max=5.0):
    """Create category diversity boost template.

    Args:
        catalog_stats: Must include 'num_categories' (total categories C)
        a_max: Maximum boost

    Returns:
        template: BoostTemplate instance
    """
    C = catalog_stats['num_categories']

    def diversity_boost_fn(p, result_set):
        """Compute diversity boost for product p given current result set.

        Args:
            p: Product dictionary with 'category' key
            result_set: List of products currently in top-k

        Returns:
            boost: Float in [0, a_max]
        """
        # Count categories in result_set
        category_counts = {}
        for prod in result_set:
            cat = prod['category']
            category_counts[cat] = category_counts.get(cat, 0) + 1

        # Expected uniform count
        k = len(result_set)
        expected_count = k / C

        # Product's category count
        p_cat = p['category']
        p_count = category_counts.get(p_cat, 0)

        # Boost if underrepresented
        if p_count < expected_count:
            return a_max * (1 - p_count / expected_count)
        else:
            return 0.0

    return BoostTemplate(
        id=8,
        name="Category Diversity",
        description="Boost underrepresented categories",
        boost_fn=diversity_boost_fn
    )
```

**Task:** Integrate this template into the library, run LinUCB with M=9 templates for 50k episodes. Report:
1. Selection frequency of diversity template
2. Catalog diversity metric: $H = -\sum_c p_c \log p_c$ where $p_c$ is fraction of top-10 results from category $c$

**Expected result:** Diversity template selected in ~5-10% of episodes; diversity $H$ increases by 0.2-0.5 nats.

---

## Experimental Exercises (40 min total)

### Lab 6.1: Reproducing the Simple-Feature Failure (20 min)

**Objective:** Reproduce the Section 6.5 experiment showing that contextual bandits with simple features underperform a strong static baseline.

**Procedure:**

1. Run the demo script in simple-feature mode:
   ```bash
   uv run python scripts/ch06/template_bandits_demo.py \
       --n-static 2000 \
       --n-bandit 20000 \
       --features simple \
       --world-seed 20250322 \
       --bandit-base-seed 20250349
   ```
2. Record:
   - Best static template and its GMV (should be Premium with GMV $\approx 7.11$)
   - LinUCB GMV (target $\approx 5.12$)
   - Thompson Sampling GMV (target $\approx 6.18$)
3. Compare LinUCB/TS to the best static template in terms of GMV and CM2.
4. Inspect the per-segment table printed by the script and identify at least two segments where bandits hurt GMV relative to the static winner.

**Expected result:** LinUCB $\approx -30\%$ GMV vs. static, TS $\approx -10\%$ GMV vs. static with clear per-segment losers. This is the Section 6.5 failure.

---

### Lab 6.2a: Rich Features with Oracle Latents---Both Excel (15 min)

**Objective:** Re-run the experiment with rich features containing **true (oracle) user latents** (Section 6.7.4) and observe both algorithms performing excellently.

**Procedure:**

1. Run the demo with oracle latents:
   ```bash
   uv run python scripts/ch06/template_bandits_demo.py \
       --n-static 2000 \
       --n-bandit 20000 \
       --features rich \
       --world-seed 20250322 \
       --bandit-base-seed 20250349 \
       --hparam-mode rich_est \
       --prior-weight 50 \
       --lin-alpha 0.2 \
       --ts-sigma 0.5
   ```
2. Record:
   - Best static template and its GMV (Premium, GMV $\approx 7.11$)
   - LinUCB GMV (target $\approx 9.42$)
   - Thompson Sampling GMV (target $\approx 9.39$)
3. Compute percentage lift vs. best static template for both algorithms.

**Expected result:** LinUCB $\approx +32\%$ GMV vs. static, TS $\approx +32\%$. With clean oracle features, both algorithms perform excellently---nearly tied. This is Section 6.7.4.

---

### Lab 6.2b: Rich Features with Estimated Latents---TS Wins (15 min)

**Objective:** Re-run the experiment with rich features containing **estimated (noisy) user latents** (Section 6.7.5) and observe Thompson Sampling's dominance.

**Procedure:**

1. Run the demo with estimated latents:
   ```bash
   uv run python scripts/ch06/template_bandits_demo.py \
       --n-static 2000 \
       --n-bandit 20000 \
       --features rich_est \
       --world-seed 20250322 \
       --bandit-base-seed 20250349 \
       --hparam-mode rich_est \
       --prior-weight 50 \
       --lin-alpha 0.2 \
       --ts-sigma 0.5
   ```
2. Record:
   - LinUCB GMV (target $\approx 7.52$)
   - Thompson Sampling GMV (target $\approx 9.31$)
3. Compute percentage lift vs. best static template for both algorithms.

**Expected result:** TS $\approx +31\%$ GMV vs. static, LinUCB $\approx +6\%$. With noisy estimated features, Thompson Sampling's robust exploration wins. This is Section 6.7.5.

---

### Lab 6.2c: Synthesis---The Algorithm Selection Principle (20 min)

**Objective:** Understand why the algorithm ranking reverses between oracle and estimated features, leading to the Algorithm Selection Principle (Section 6.7.6).

**Procedure:**

1. Create a 2x2 comparison table:

   | Features | LinUCB GMV | TS GMV | Winner | Margin |
   |----------|------------|--------|--------|--------|
   | Oracle   | ~9.42      | ~9.39  | LinUCB | +0.4 pts |
   | Estimated| ~7.52      | ~9.31  | TS     | +25 pts |

2. Compute the "reversal magnitude": How many GMV points does the winner advantage change by?
3. **Key insight question:** Why does feature noise favor Thompson Sampling?
   - Hint: Consider LinUCB's UCB bonus shrinkage vs. TS's perpetual posterior variance.
4. **Production question:** In a real e-commerce system, do we have oracle latents or only estimated latents?
5. Write a 3-sentence recommendation for which algorithm to use in production.

**Expected synthesis:** Production systems have noisy estimated features, so Thompson Sampling is a robust default. LinUCB is most appropriate when feature quality is exceptionally high (direct measurements, carefully validated estimates, or A/B test signals). This is Lesson 5.
---

### Lab 6.3: Hyperparameter Sensitivity (20 min)

**Objective:** Understand how $\lambda$ (regularization) and $\alpha$ (exploration) affect LinUCB performance.

**Procedure:**

1. Grid search over $(\lambda, \alpha)$:
   - $\lambda \in \{0.01, 0.1, 1.0, 10.0\}$
   - $\alpha \in \{0.1, 0.5, 1.0, 2.0, 5.0\}$

2. For each combination:
   - Train LinUCB for 50k episodes on `zoosim`
   - Record final average reward (last 10k episodes)

3. Plot heatmap: X-axis $\lambda$, Y-axis $\alpha$, color = final reward

4. Identify optimal hyperparameters

**Expected findings:**

- $\lambda$ too small (<0.1): Overfitting, unstable weights
- $\lambda$ too large (>10): Underfitting, slow learning
- $\alpha$ too small (<0.5): Insufficient exploration, gets stuck
- $\alpha$ too large (>2): Excessive exploration, ignores rewards

**Optimal region:** $\lambda \in [0.5, 2.0]$, $\alpha \in [0.5, 1.5]$

**Deliverable:** Heatmap plot + 2-paragraph analysis of sensitivity

---

### Lab 6.4: Visualization of Exploration Dynamics (15 min)

**Objective:** Visualize how bandits explore the template space over time.

**Plots to create:**

1. **Template selection heatmap**
   - X-axis: Episode (binned into 1000-episode windows)
   - Y-axis: Template ID (0-7)
   - Color: Selection frequency in window
   - Shows: How exploration decays, which templates dominate when

2. **Uncertainty evolution**
   - X-axis: Episode
   - Y-axis: $\text{trace}(\Sigma_a)$ (total uncertainty) for each template
   - Multiple lines (one per template)
   - Shows: How uncertainty shrinks as data accumulates

3. **Regret decomposition**
   - X-axis: Episode
   - Y-axis: Cumulative regret
   - Stacked area chart: Regret contribution from each template
   - Shows: Which templates contribute most to regret (bad early selections)

**Code template:**

```python
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Selection heatmap
window_size = 1000
num_windows = T // window_size
selection_matrix = np.zeros((M, num_windows))

...
```

---

### Advanced Lab 6.A: From CPU Loops to GPU Batches (60--120 min)

This is an advanced, end-to-end lab that teaches how and why to move from the canonical (but slow) Chapter 6 implementation under `scripts/ch06/` to the GPU-accelerated path under `scripts/ch06/optimization_gpu/`. It assumes completion of at least Labs 6.1--6.3.

See the dedicated draft:

- `docs/book/ch06/ch06_advanced_gpu_lab.md`

for:

- Conceptual explanation of CPU vs GPU execution for template bandits
- A guided tour of `template_bandits_gpu.py`, `ch06_compute_arc_gpu.py`, and `run_bandit_matrix_gpu.py`
- Step-by-step tasks comparing CPU and GPU runs, exploring batch size and device choices, and extending diagnostics

This advanced lab is optional for first-time readers but strongly recommended if we plan to scale Chapter 6 experiments to many seeds, feature variants, or larger episode counts.

for w in range(num_windows):
    window_selections = selection_history[w*window_size:(w+1)*window_size]
    freqs = np.bincount(window_selections, minlength=M) / window_size
    selection_matrix[:, w] = freqs

plt.figure(figsize=(12, 6))
sns.heatmap(selection_matrix, cmap='viridis', cbar_kws={'label': 'Selection Frequency'})
plt.xlabel('Episode Window (x1000)')
plt.ylabel('Template ID')
plt.yticks(np.arange(M) + 0.5, [t.name for t in templates], rotation=0)
plt.title('Template Selection Dynamics')
plt.savefig('template_heatmap.png', dpi=150)

# 2. Uncertainty evolution
# [Implement: Plot trace(Sigma_a) vs. episode for each template]

# 3. Regret decomposition
# [Implement: Stacked area chart of per-template regret]
```

**Deliverable:** Three plots + interpretation paragraph for each

---

### Lab 6.5: Multi-Seed Robustness (5 min)

**Objective:** Verify bandit performance is robust to random seed.

**Procedure:**

1. Run LinUCB with seeds $\{42, 123, 456, 789, 1011, 2022, 3033, 4044, 5055, 6066\}$
2. For each seed, record final average reward (last 10k episodes)
3. Compute mean $\pm$ std across seeds
4. Plot: Box plot of final rewards across seeds

**Expected result:**

- Mean final reward: $\approx 122$ GMV
- Std: $\approx 2$-$3$ GMV
- All seeds within $\pm 5\%$ of mean $\rightarrow$ robust

**If variance is high (>5 GMV):**
- Check: Are templates deterministic? (should be)
- Check: Is environment seed fixed? (should be independent per trial)
- Diagnosis: High variance suggests exploration randomness dominates $\rightarrow$ increase $T$ or decrease $\alpha$

**Deliverable:** Box plot + robustness assessment

---

## Advanced Exercises (Optional, 30+ min)

### Exercise 6.7: Hierarchical Templates (20 min)

**Problem:**

The flat template library has no structure. Design a **hierarchical template system**:

**Level 1 (Meta-template):** Select business objective
- Objective A: Maximize margin
- Objective B: Maximize volume (clicks/purchases)
- Objective C: Strategic goals

**Level 2 (Sub-template):** Given objective, select tactic
- If Objective A (margin): {Positive CM2, Premium, Private Label}
- If Objective B (volume): {Popular, Discount, Budget}
- If Objective C (strategic): {Strategic, Category Diversity}

**Bandit hierarchy:**
1. Train meta-bandit over 3 objectives (context = user segment)
2. For each objective, train sub-bandit over tactics (context = query + product features)

**Implementation sketch:**

```python
class HierarchicalBandit:
    def __init__(self, meta_templates, sub_templates_dict, feature_dim):
        """
        Args:
            meta_templates: List of 3 objectives
            sub_templates_dict: Dict mapping objective_id -> list of sub-templates
            feature_dim: Context feature dimension
        """
        self.meta_bandit = LinUCB(meta_templates, feature_dim, ...)
        self.sub_bandits = {
            obj_id: LinUCB(templates, feature_dim, ...)
            for obj_id, templates in sub_templates_dict.items()
        }

    def select_action(self, features):
        """Two-stage selection."""
        # Stage 1: Select objective
        obj_id = self.meta_bandit.select_action(features)

        # Stage 2: Select tactic given objective
        tactic_id = self.sub_bandits[obj_id].select_action(features)

        return (obj_id, tactic_id)

    def update(self, action, features, reward):
        """Update both levels."""
        obj_id, tactic_id = action

        # Update sub-bandit (tactic level)
        self.sub_bandits[obj_id].update(tactic_id, features, reward)

        # Update meta-bandit (objective level)
        self.meta_bandit.update(obj_id, features, reward)
```

**Task:** Implement, run for 50k episodes, compare to flat LinUCB. Report:
1. Meta-level selection distribution (which objectives learned?)
2. Sub-level selection distribution per objective
3. Final GMV vs. flat LinUCB

**Expected result:** Hierarchical bandit achieves similar GMV with **faster convergence** (fewer parameters to learn per level) and **better interpretability** (business can understand objective $\rightarrow$ tactic mapping).

---

### Exercise 6.8: Neural Linear Bandits (40 min) **[Advanced, Optional]**

!!! warning "Prerequisites for Exercise 6.8"
    This exercise requires:
    - Neural network implementation skills (PyTorch)
    - Understanding of representation learning (Chapter 12)
    - Pretraining on logged data (Chapter 13)

    **If Chapters 12-13 are not completed:** Skip this exercise or treat it as a reading exercise (analyze the provided solution without implementing from scratch).

    **For advanced students:** This is a preview of techniques used in later deep RL chapters.

**Problem:**

Implement Neural Linear bandit as described in Appendix 6.A:

1. **Representation network:**
   - Input: Raw features (100-dim)
   - Architecture: [100 -> 64 -> 64 -> 20]
   - Activation: ReLU

2. **Pretraining:**
   - Collect 10k logged episodes using random template selection
   - Train network to predict reward: $r \approx \theta^\top f_\psi(x)$ where $\theta$ is linear head
   - Loss: MSE
   - Optimizer: Adam, lr=1e-3, 100 epochs

3. **Bandit training:**
   - Freeze representation $f_\psi$
   - Use LinUCB with features $\phi(x) = f_\psi(x)$

**Comparison:**

Run three conditions:
- **Baseline:** LinUCB with hand-crafted features (Chapter 5)
- **Neural Linear:** LinUCB with learned features $f_\psi$
- **Oracle:** LinUCB with true optimal features (if known)

**Metrics:**
- Sample efficiency: Episodes to reach 95% of final reward
- Final GMV
- Feature quality: $R^2$ of reward prediction on held-out set

**Expected result:**

| Method | Sample Efficiency | Final GMV | Feature $R^2$ |
|--------|-------------------|-----------|---------------|
| Hand-crafted | 20k episodes | 122.5 | 0.73 |
| Neural Linear | 15k episodes | 124.2 | 0.81 |
| Oracle (unfair) | 10k episodes | 126.0 | 0.92 |

**Conclusion:** Neural Linear can improve over hand-crafted features **if sufficient pretraining data exists** (10k+ episodes). Otherwise, feature engineering is safer.

---

### Exercise 6.9: Query-Conditional Templates (30 min)

**Problem:**

Current templates are **product-only** (do not depend on query). Extend to **query-conditional templates**:

**Example:** "Discount" template should:
- Boost discounted products **more** for query `"deals"`, `"sale"`
- Boost discounted products **less** for query `"premium dog food"`

**Design:**

Each template becomes:
$$
t(p, q) = w_{\text{base}} \cdot f(p) + w_{\text{query}} \cdot g(q, p)
$$

where:
- $f(p)$: Product feature (margin, discount, etc.)
- $g(q, p)$: Query-product interaction (e.g., cosine similarity of query to "discount" keywords)
- $w_{\text{base}}, w_{\text{query}}$: Learned weights (bandit parameters)

**Implementation:**

1. Extend `BoostTemplate` to accept query as input:
   ```python
   class QueryConditionalTemplate:
       def apply(self, products, query):
           return np.array([self.boost_fn(p, query) for p in products])
   ```

2. Augment features: $\phi(x) = [\phi_{\text{user}}(x), \phi_{\text{product}}(x), \phi_{\text{query}}(x)]$

3. Run LinUCB with augmented features

**Evaluation:**

Compare:
- **Product-only templates** (baseline)
- **Query-conditional templates** (proposed)

Metrics:
- GMV by query type (navigational, informational, transactional)
- Diversity of template selection per query type

**Expected result:** Query-conditional templates achieve **+3-5% GMV** on queries with strong intent signals (e.g., "cheap", "premium", "best") compared to product-only templates.

---

## Solutions

Complete solutions are provided in:

- `docs/book/ch06/ch06_lab_solutions.md` (rendered solutions with representative outputs)
- `scripts/ch06/lab_solutions/` (runnable code)

Run all solutions:
```bash
uv run python -m scripts.ch06.lab_solutions --all
```

---

## Time Allocation Summary

| Category | Time (min) | Exercises |
|----------|------------|-----------|
| **Theory** | 30 | 6.1 (10), 6.2 (15), 6.3 (5) |
| **Implementation** | 40 | 6.4 (15), 6.5 (20), 6.6 (5) |
| **Labs** | 80 | Lab 6.1 (20), Lab 6.2 (20), Lab 6.3 (20), Lab 6.4 (15), Lab 6.5 (5) |
| **Advanced (Optional)** | 90+ | 6.7 (20), 6.8 (40), 6.9 (30) |
| **Total (Core)** | 150 | |
| **Total (with Advanced)** | 240+ | |

**Recommended path:**

- **Minimal (90 min):** Theory 6.1-6.3, Impl 6.4, Labs 6.1
- **Standard (110 min):** All core exercises + Labs 6.1-6.3
- **Deep dive (200 min):** Core + Advanced 6.7-6.9

---


# Chapter 6 --- Lab Solutions

These solutions integrate contextual bandit theory with runnable implementations. Each solution weaves the main results from Chapter 6 ([ALG-6.1], [ALG-6.2], [THM-6.1], [THM-6.2]) with executable code.

For the production-simulator experiments, we provide fixed seeds and reference committed artifacts under `docs/book/ch06/data/`. Other small outputs are representative and may vary slightly with environment and RNG.

---

## Theory Exercises

### Exercise 6.1: Properties of Cosine Similarity (10 min)

**Problem:** Prove properties of semantic relevance $s_{\text{sem}}(\mathbf{q}, \mathbf{e}) = \frac{\mathbf{q} \cdot \mathbf{e}}{\|\mathbf{q}\|_2 \|\mathbf{e}\|_2}$ from [DEF-5.2].

#### Theoretical Foundation

Cosine similarity measures the angle between vectors in embedding space, invariant to magnitude. It appears throughout our template feature design.

#### Solution

**Part (a): Boundedness** $s_{\text{sem}}(\mathbf{q}, \mathbf{e}) \in [-1, 1]$

*Proof:* By Cauchy-Schwarz inequality:
$$
|\mathbf{q} \cdot \mathbf{e}| \leq \|\mathbf{q}\|_2 \|\mathbf{e}\|_2
$$

Dividing both sides by $\|\mathbf{q}\|_2 \|\mathbf{e}\|_2 > 0$:
$$
\left|\frac{\mathbf{q} \cdot \mathbf{e}}{\|\mathbf{q}\|_2 \|\mathbf{e}\|_2}\right| \leq 1
$$

Thus $s_{\text{sem}} \in [-1, 1]$. $\square$

**Part (b): Scale Invariance** $s_{\text{sem}}(\alpha \mathbf{q}, \beta \mathbf{e}) = \text{sign}(\alpha \beta) \cdot s_{\text{sem}}(\mathbf{q}, \mathbf{e})$

*Proof:*
$$
s_{\text{sem}}(\alpha \mathbf{q}, \beta \mathbf{e}) = \frac{(\alpha \mathbf{q}) \cdot (\beta \mathbf{e})}{\|\alpha \mathbf{q}\|_2 \|\beta \mathbf{e}\|_2}
= \frac{\alpha \beta (\mathbf{q} \cdot \mathbf{e})}{|\alpha| \|\mathbf{q}\|_2 \cdot |\beta| \|\mathbf{e}\|_2}
= \frac{\alpha \beta}{|\alpha \beta|} \cdot s_{\text{sem}}(\mathbf{q}, \mathbf{e})
$$

Since $\frac{\alpha \beta}{|\alpha \beta|} = \text{sign}(\alpha \beta)$, the result follows. $\square$

**Part (c): Non-additivity** For orthogonal $\mathbf{e}_1 \perp \mathbf{e}_2$, generally:
$$
s_{\text{sem}}(\mathbf{q}, \mathbf{e}_1 + \mathbf{e}_2) \neq s_{\text{sem}}(\mathbf{q}, \mathbf{e}_1) + s_{\text{sem}}(\mathbf{q}, \mathbf{e}_2)
$$

*Counterexample:* Let $\mathbf{q} = (1, 1)$, $\mathbf{e}_1 = (1, 0)$, $\mathbf{e}_2 = (0, 1)$.

Then $\mathbf{e}_1 \perp \mathbf{e}_2$ since $\mathbf{e}_1 \cdot \mathbf{e}_2 = 0$.

- LHS: $s_{\text{sem}}(\mathbf{q}, \mathbf{e}_1 + \mathbf{e}_2) = s_{\text{sem}}((1,1), (1,1)) = \frac{2}{2} = 1$
- RHS: $s_{\text{sem}}((1,1), (1,0)) + s_{\text{sem}}((1,1), (0,1)) = \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{2}} = \sqrt{2} \approx 1.41$

Since $1 \neq \sqrt{2}$, the property fails. $\square$

```python
from scripts.ch06.lab_solutions import exercise_6_1_cosine_properties

results = exercise_6_1_cosine_properties(verbose=True)
```

**Actual Output:**
```
======================================================================
Exercise 6.1: Cosine Similarity Properties
======================================================================

Part (a): Boundedness verification
  Testing 1000 random vector pairs (d=16)...
  All similarities in [-1, 1]: True
  Min observed: -0.664, Max observed: 0.673

Part (b): Scale invariance verification
  Testing alpha=2.5, beta=-3.0
  s(q, e) = 0.2224
  s(alphaq, betae) = -0.2224
  sign(alphabeta) * s(q, e) = -0.2224
  Equality holds: True (diff = 2.78e-17)

Part (c): Non-additivity counterexample
  q = [1.0, 1.0], e1 = [1.0, 0.0], e2 = [0.0, 1.0]
  e1 perp e2: True (dot product = 0.0)
  s(q, e1 + e2) = 1.0000
  s(q, e1) + s(q, e2) = 1.4142
  Non-additivity demonstrated: 1.0000 != 1.4142

[OK] All properties verified numerically.
```

#### Analysis

The bounded range $[-1, 1]$ ensures that cosine-based features do not dominate other features in the context vector. The scale invariance means we can normalize embeddings without affecting similarity computations. Non-additivity implies that aggregating embeddings (e.g., averaging product embeddings) does not preserve similarity relationships---a key consideration when designing template features.

---

### Exercise 6.2: Ridge Regression Closed Form (15 min)

**Problem:** Prove LinUCB weight update is ridge regression, show OLS limit, explain regularization.

#### Theoretical Foundation

LinUCB [ALG-6.2] maintains per-action statistics $A_a = \lambda I + \sum_t \phi_t \phi_t^\top$ and $b_a = \sum_t r_t \phi_t$, with weight estimate $\hat{\theta}_a = A_a^{-1} b_a$.

#### Solution

*Proof of (a):*

The objective is:
$$
J(\theta) = \sum_{i=1}^n (r_i - \theta^\top \phi_i)^2 + \lambda \|\theta\|^2
$$

Expanding:
$$
J(\theta) = \sum_i (r_i^2 - 2r_i \theta^\top \phi_i + \theta^\top \phi_i \phi_i^\top \theta) + \lambda \theta^\top \theta
$$

Taking the gradient with respect to $\theta$:
$$
\nabla_\theta J = \sum_i (-2 r_i \phi_i + 2 \phi_i \phi_i^\top \theta) + 2\lambda \theta
$$

Setting $\nabla_\theta J = 0$:
$$
\sum_i \phi_i \phi_i^\top \theta + \lambda \theta = \sum_i r_i \phi_i
$$

Thus:
$$
\hat{\theta} = \left(\sum \phi_i \phi_i^\top + \lambda I\right)^{-1} \sum r_i \phi_i = A^{-1} b \quad \checkmark
$$

```python
from scripts.ch06.lab_solutions import exercise_6_2_ridge_regression

results = exercise_6_2_ridge_regression(seed=42, verbose=True)
```

**Actual Output:**
```
======================================================================
Exercise 6.2: Ridge Regression Equivalence
======================================================================

Generating synthetic regression data (n=100, d=7)...

Part (a): LinUCB vs explicit ridge regression
  LinUCB weights (A^{-1}b): [0.296, -1.021, 0.731, 0.917, -1.944, -1.271, 0.117]
  Ridge regression (closed form): [0.296, -1.021, 0.731, 0.917, -1.944, -1.271, 0.117]
  Max difference: 0.00e+00
  [OK] Equivalence verified (numerical precision)

Part (b): OLS limit as lambda -> 0
  lambda = 1.0e+00: ||theta_ridge - theta_ols|| = 0.0300
  lambda = 1.0e-01: ||theta_ridge - theta_ols|| = 0.0030
  lambda = 1.0e-02: ||theta_ridge - theta_ols|| = 0.0003
  lambda = 1.0e-03: ||theta_ridge - theta_ols|| = 0.0000
  lambda = 1.0e-04: ||theta_ridge - theta_ols|| = 0.0000
  [OK] Convergence to OLS demonstrated

Part (c): Regularization and condition number
  Without regularization (lambda=0): kappa(Phi^TPhi) = 2.22e+00
  With regularization (lambda=1):    kappa(A) = 2.20e+00
  Condition number reduced by factor: 1x

  Why this matters:
  - Ill-conditioned matrices amplify numerical errors
  - In early training, Sigmaphiphi^T may be rank-deficient
  - Regularization lambdaI ensures A is always invertible
  - Bounds condition number: kappa(A) <= (lambda_max + lambda)/lambda
```

#### Analysis

The ridge regression interpretation is fundamental: LinUCB is simply online Bayesian linear regression with a Gaussian prior. The regularization parameter $\lambda$ serves dual purposes:

1. **Statistical:** Acts as prior precision, shrinking weights toward zero (prevents overfitting)
2. **Numerical:** Ensures matrix invertibility even with few samples

Note: In this well-conditioned synthetic example, the condition number reduction is minimal. In real problems with collinear features or few samples, the reduction can be dramatic (100-1000x).

---

### Exercise 6.3: Thompson Sampling vs LinUCB Posterior (5 min)

**Problem:** Show TS and LinUCB maintain identical posterior means.

#### Theoretical Foundation

Both algorithms perform Bayesian linear regression but differ in action selection:
- **LinUCB:** UCB rule $\hat{\theta}_a^\top \phi + \alpha \sqrt{\phi^\top \Sigma_a \phi}$
- **Thompson Sampling:** Sample $\tilde{\theta}_a \sim \mathcal{N}(\hat{\theta}_a, \Sigma_a)$, then $\arg\max_a \tilde{\theta}_a^\top \phi$

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_3_ts_linucb_equivalence

results = exercise_6_3_ts_linucb_equivalence(seed=42, n_episodes=500, verbose=True)
```

**Actual Output:**
```
======================================================================
Exercise 6.3: Thompson Sampling vs LinUCB Posterior Equivalence
======================================================================

Running 500 episodes with identical data streams...
Episode 100: Max |theta_TS - theta_LinUCB| = 0.00e+00
Episode 200: Max |theta_TS - theta_LinUCB| = 0.00e+00
Episode 300: Max |theta_TS - theta_LinUCB| = 0.00e+00
Episode 400: Max |theta_TS - theta_LinUCB| = 0.00e+00
Episode 500: Max |theta_TS - theta_LinUCB| = 0.00e+00

Final comparison (action 0):
  TS posterior mean: [-0.008, -0.087, -0.007...]
  LinUCB weights:    [-0.008, -0.087, -0.007...]
  Max difference: 0.00e+00 (numerical precision)

Final comparison (action 3):
  TS posterior mean: [0.193, -0.096, -0.086...]
  LinUCB weights:    [0.193, -0.096, -0.086...]
  Max difference: 0.00e+00 (numerical precision)

[OK] Posterior means are identical to numerical precision.

Key insight:
  TS and LinUCB learn the SAME model (ridge regression).
  They differ only in HOW they use uncertainty for exploration:
  - LinUCB: Deterministic UCB bonus sqrt(phi^TSigmaphi)
  - TS: Stochastic sampling from posterior
```

#### Analysis

This equivalence is important practically: we can switch between LinUCB and TS without re-training. The choice depends on deployment context:

- **LinUCB:** Deterministic, easier to debug, reproducible A/B tests
- **Thompson Sampling:** Often better empirical performance, especially with many actions (avoids over-optimism of UCB)

---

## Implementation Exercises

### Exercise 6.4: epsilon-Greedy Baseline (15 min)

**Problem:** Implement epsilon-greedy for template selection, compare to LinUCB.

#### Theoretical Foundation

epsilon-greedy is the simplest exploration strategy:
- With probability $\epsilon$: random action
- With probability $1-\epsilon$: greedy action

**Regret bound:** With constant $\epsilon$, regret is $O(\epsilon T)$ (linear), while LinUCB achieves $O(\sqrt{T})$ (sublinear). This fundamental difference emerges from epsilon-greedy's inability to focus exploration on uncertain actions.

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_4_epsilon_greedy

results = exercise_6_4_epsilon_greedy(
    n_episodes=20000,
    epsilons=[0.05, 0.1, 0.2],
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Exercise 6.4: epsilon-Greedy Baseline Implementation
======================================================================

Running experiments with n_episodes=20,000...

Training epsilon-greedy (epsilon=0.05)...
  Progress: 100% (20000/20000)
Training epsilon-greedy (epsilon=0.1)...
  Progress: 100% (20000/20000)
Training epsilon-greedy (epsilon=0.2)...
  Progress: 100% (20000/20000)
Training LinUCB (alpha=1.0)...
  Progress: 100% (20000/20000)

Results (average reward over last 5000 episodes):

  Policy             | Avg Reward | Cumulative Regret
  -------------------|------------|------------------
  epsilon-greedy (epsilon=0.05)  |       2.73 |             6,799
  epsilon-greedy (epsilon=0.1)   |       2.56 |             9,270
  epsilon-greedy (epsilon=0.2)   |       2.26 |            14,917
  LinUCB (alpha=1.0)     |       2.89 |             4,133

Regret Analysis:

  At T=20,000:
  - epsilon-greedy (epsilon=0.05): Regret $\approx 6$,799 $\approx 0.34$ x T (linear)
  - epsilon-greedy (epsilon=0.1): Regret $\approx 9$,270 $\approx 0.46$ x T (linear)
  - epsilon-greedy (epsilon=0.2): Regret $\approx 14$,917 $\approx 0.75$ x T (linear)
  - LinUCB: Regret $\approx 4$,133 $\approx 29$ x sqrtT (sublinear)

  Theoretical prediction:
  - epsilon-greedy: O(epsilonT) because exploration never stops
  - LinUCB: O(dsqrtT log T) because uncertainty naturally decreases

[OK] Demonstrated linear vs sublinear regret scaling.
```

#### Analysis

The epsilon-greedy regret scales linearly with $T$ because it wastes samples exploring uniformly even after the optimal action is identified. LinUCB's UCB-based exploration naturally diminishes as uncertainty decreases, achieving sublinear regret.

**Practical implication:** epsilon-greedy is acceptable for short horizons or when $\epsilon$ is decayed, but for long-running production systems, UCB or Thompson Sampling is preferred.

---

### Exercise 6.5: Cholesky-Based Thompson Sampling (20 min)

**Problem:** Optimize TS sampling using Cholesky factorization.

#### Theoretical Foundation

Naive TS requires computing $\Sigma_a = A_a^{-1}$ every episode (cost: $O(d^3)$). The Cholesky approach:

1. Maintain $L_a$ where $A_a = L_a L_a^\top$ (Cholesky factor)
2. Sample: $\tilde{\theta}_a = \hat{\theta}_a + L_a^{-\top} z$ where $z \sim \mathcal{N}(0, I)$

This reduces per-episode cost by avoiding full matrix inversion.

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_5_cholesky_ts

results = exercise_6_5_cholesky_ts(
    dims=[10, 50, 100, 500],
    n_episodes=1000,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Exercise 6.5: Cholesky-Based Thompson Sampling
======================================================================

Benchmarking naive vs Cholesky TS for varying feature dimensions...

Feature Dim | Naive Time | Cholesky Time |  Speedup
------------|------------|---------------|---------
     d=10   |     0.074s |        0.038s |     1.9x
     d=50   |     0.830s |        0.062s |    13.3x
     d=100  |     2.343s |        0.108s |    21.8x
     d=500  |    48.610s |        0.920s |    52.8x

Correctness verification (d=50):
  Same posterior mean: True (max diff = 2.22e-16)
  Sample covariance matches Sigma: True (Frobenius diff = 0.0032)

Why it works:
  If A = LL^T (Cholesky), then Sigma = A^{-1} = L^-^TL^{-1}.
  To sample theta ~ N(mu, Sigma):
    z ~ N(0, I)
    theta = mu + L^-^Tz  [since Cov(L^-^Tz) = L^-^TL^{-1} = Sigma]

  Cost comparison:
    Naive: O(d^3) for matrix inverse per sample
    Cholesky: O(d^2) for triangular solve per sample
              + O(d^2) Cholesky update (amortized)

[OK] Cholesky optimization provides 5-11x speedup for d >= 50.
```

#### Analysis

The Cholesky optimization is essential for production Thompson Sampling with rich features. At $d = 500$, naive implementation takes 48.6 seconds per 1000 episodes---unacceptable for real-time serving. The Cholesky approach reduces this to under 1 second, achieving a **52.8x speedup**.

**Advanced optimization:** For truly large $d$, use rank-1 Cholesky updates (`scipy.linalg.cho_solve` + incremental updates) to achieve $O(d^2)$ per update instead of $O(d^3)$ for full refactorization.

---

### Exercise 6.6: Category Diversity Template (5 min)

**Problem:** Implement a diversity-boosting template for underrepresented categories.

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_6_diversity_template

results = exercise_6_6_diversity_template(
    n_episodes=20000,
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Exercise 6.6: Category Diversity Template
======================================================================

Creating category diversity template (ID=8)...

Running LinUCB with M=9 templates (8 standard + 1 diversity)...
  Progress: 100% (20000/20000)

Template Selection Frequencies (20,000 episodes):

  Template ID | Name               | Selection % | Avg Reward
  ------------|--------------------|-------------|-----------
            0 | No Boost           |        1.1% |       1.18
            1 | Positive CM2       |        1.1% |       1.17
            2 | Popular            |       19.4% |       1.29
            3 | Discount           |        0.6% |       1.05
            4 | Premium            |       48.2% |       1.31
            5 | Private Label      |       28.3% |       1.30
            6 | Budget             |        0.3% |       0.89
            7 | Strategic          |        0.8% |       1.05
            8 | Category Diversity |        0.2% |       0.74

Diversity Metrics (top-10 results):

  Metric                     |   Random Baseline | Learned Policy
  ---------------------------|-------------------|---------------
  Entropy H (nats)           |              1.23 |           1.23

  DeltaH $\approx 0$ nats (no significant change)

WARNING: Diversity template selected only 0.2% of the time.
  The bandit learned it provides lower expected reward in this environment.
  This is correct behavior---not all templates are useful in all contexts.
```

#### Analysis

This result is pedagogically valuable: **the bandit correctly learns that the diversity template is not useful in this environment**.

Key observations:
- **Selection frequency: 0.2%** --- Nearly the lowest of all templates
- **Average reward: 0.74** --- Below the best templates (Premium: 1.31, Popular: 1.29)
- **No entropy improvement** --- The low selection rate means diversity has negligible impact

This demonstrates an important lesson: **not all templates are useful in all contexts**. The diversity template may shine in scenarios with:

- Many distinct categories (this simulation has only 4)
- Users who explicitly value variety (not modeled here)
- Category-imbalanced rankings where diversity provides novelty value

The bandit correctly learns to suppress low-value templates. This behavior is not a bug---the algorithm is maximizing expected reward by avoiding templates that do not help.

---

### Exercise 6.6b: When Diversity Actually Helps (10 min)

**Problem:** Design a scenario where diversity templates provide significant value.

#### Theoretical Foundation

Exercise 6.6 showed diversity failing. But when **does** diversity help? The key insight:

> **Diversity is valuable when the base ranker's bias misses user preferences.**

This happens when:
1. The base ranker has **popularity bias** (category A dominates top-K)
2. Users have **diverse latent preferences** (60% want non-A categories)
3. Users who do not see their preferred category **rarely convert**

This models the "long tail" effect in e-commerce: most revenue comes from niche preferences, not just popular items.

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_6b_diversity_when_helpful

results = exercise_6_6b_diversity_when_helpful(
    n_episodes=20000,
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Exercise 6.6b: When Diversity Actually Helps
======================================================================

Scenario: Biased base ranker + diverse user preferences
  - Base ranker: 80% of top-K from category A (popularity bias)
  - Users: 40% prefer A, 20% prefer B, 20% prefer C, 20% prefer D
  - Reward: Higher when user sees their preferred category

Running 3-way comparison...
  1. Biased baseline (no diversity)
  2. Always-diversity (force diversity every episode)
  3. Bandit-learned (learns when to use diversity)
  Progress: 100% (20000/20000)

======================================================================
Results (average over last 5,000 episodes)
======================================================================

  Policy               |   Avg Reward | Entropy (nats) |   Delta Reward
  ---------------------|--------------|----------------|-----------
  Biased Baseline      |        0.354 |           0.00 |          ---
  Always Diversity     |        0.689 |           0.94 |     +0.335 (+94.4%)
  Bandit (learned)     |        0.690 |           0.94 |     +0.335 (+94.5%)

  Bandit diversity selection rate: 99.2%

======================================================================
Analysis: Why Diversity Helps Here
======================================================================

  Category distribution in biased top-10:
    Category A: 10/10 (100%)
    Category B: 0/10 (0%)
    Category C: 0/10 (0%)
    Category D: 0/10 (0%)

  Category distribution in diverse top-10:
    Category A: 7/10 (70%)
    Category B: 1/10 (10%)
    Category C: 1/10 (10%)
    Category D: 1/10 (10%)

  User preference distribution:
    Prefer category A: 40%
    Prefer category B: 20%
    Prefer category C: 20%
    Prefer category D: 20%

  The mismatch:
    - Biased ranker: 100% category A in top-10
    - But only 40% of users prefer category A!
    - 60% of users want B/C/D but rarely see them

  Diversity fixes this:
    - Entropy: 0.00 -> 0.94 (massive increase)
    - Reward:  0.354 -> 0.689 (+94.4%)

[OK] Diversity improves reward by +94.4% in this biased-ranker scenario!
  The bandit learns to select diversity 99% of the time.
```

#### Analysis

This is a dramatic result: **+94.4% reward improvement** from diversity. The mechanism:

| User Type | % of Traffic | Biased Ranker | Diverse Ranker |
|-----------|--------------|---------------|----------------|
| Prefers A | 40% | Sees A, converts [OK] | Sees A, converts [OK] |
| Prefers B | 20% | Sees NO B, does not convert [X] | Sees B, converts [OK] |
| Prefers C | 20% | Sees NO C, does not convert [X] | Sees C, converts [OK] |
| Prefers D | 20% | Sees NO D, does not convert [X] | Sees D, converts [OK] |

The biased ranker **completely ignores** 60% of user preferences. Diversity fixes this by ensuring each category appears at least once in top-K.

**Key lessons:**

1. **Exercise 6.6 vs 6.6b:** Diversity value depends on the mismatch between ranker bias and user preferences
2. **Bandit learns correctly:** 99.2% diversity selection rate shows the algorithm discovered diversity's value
3. **When to use diversity:** When the ranker has systematic bias that does not match the user preference distribution

**Diagnostic question:** "Is the base ranker's category distribution aligned with user preferences?" If not, diversity templates may provide significant value.

---

## Experimental Labs

### Lab 6.1: Simple-Feature Failure (20 min)

**Objective:** Reproduce the Section 6.5 experiment showing that contextual bandits with simple features underperform a strong static baseline.

#### Theoretical Foundation

Simple features (segment one-hot + query type one-hot) omit the key latent preferences that determine which templates are actually valuable. This lab makes that failure mode concrete.

#### Running the Lab

!!! note "Production Simulator Required"
    Lab 6.1 uses the full `zoosim` simulator via `scripts/ch06/template_bandits_demo.py`. Run:
    ```bash
    uv run python scripts/ch06/template_bandits_demo.py \
        --n-static 2000 \
        --n-bandit 20000 \
        --features simple \
        --world-seed 20250322 \
        --bandit-base-seed 20250349
    ```

#### Expected Results (canonical artifact)

The committed summary `docs/book/ch06/data/template_bandits_simple_summary.json` reports:

| Policy | GMV | CM2 | DeltaGMV vs static |
|--------|-----|-----|----------------|
| Static (Premium) | 7.11 | 0.74 | +0.00% |
| LinUCB | 5.12 | 0.46 | -27.95% |
| Thompson Sampling | 6.18 | 0.63 | -13.07% |

This is the feature-poverty failure mode: regret guarantees are conditional on the feature map, and $\phi_{\text{simple}}$ is too impoverished to support the desired policy.

---

### Lab 6.2a: Rich Features with Oracle Latents (15 min)

**Objective:** Re-run the experiment with rich features containing oracle user latents (Section 6.7.4) and observe both algorithms recovering and nearly tying.

#### Running the Lab

!!! note "Production Simulator Required"
    ```bash
    uv run python scripts/ch06/template_bandits_demo.py \
        --n-static 2000 \
        --n-bandit 20000 \
        --features rich \
        --world-seed 20250322 \
        --bandit-base-seed 20250349 \
        --hparam-mode rich_est \
        --prior-weight 50 \
        --lin-alpha 0.2 \
        --ts-sigma 0.5
    ```

#### Expected Results (canonical artifact)

From `docs/book/ch06/data/template_bandits_rich_oracle_summary.json`:

| Policy | GMV | CM2 | DeltaGMV vs static |
|--------|-----|-----|----------------|
| Static (Premium) | 7.11 | 0.74 | +0.00% |
| LinUCB | 9.42 | 0.94 | +32.47% |
| Thompson Sampling | 9.39 | 0.94 | +32.08% |

---

### Lab 6.2b: Rich Features with Estimated Latents (15 min)

**Objective:** Re-run the experiment with rich features containing estimated (noisy) user latents (Section 6.7.5) and observe Thompson Sampling's robustness.

#### Running the Lab

!!! note "Production Simulator Required"
    ```bash
    uv run python scripts/ch06/template_bandits_demo.py \
        --n-static 2000 \
        --n-bandit 20000 \
        --features rich_est \
        --world-seed 20250322 \
        --bandit-base-seed 20250349 \
        --hparam-mode rich_est \
        --prior-weight 50 \
        --lin-alpha 0.2 \
        --ts-sigma 0.5
    ```

#### Expected Results (canonical artifact)

From `docs/book/ch06/data/template_bandits_rich_estimated_summary.json`:

| Policy | GMV | CM2 | DeltaGMV vs static |
|--------|-----|-----|----------------|
| Static (Premium) | 7.11 | 0.74 | +0.00% |
| LinUCB | 7.52 | 0.72 | +5.79% |
| Thompson Sampling | 9.31 | 0.93 | +30.95% |

---

### Lab 6.3: Hyperparameter Sensitivity (20 min)

**Objective:** Understand how $\lambda$ (regularization) and $\alpha$ (exploration) affect LinUCB.

#### Solution

```python
from scripts.ch06.lab_solutions import lab_6_3_hyperparameter_sensitivity

results = lab_6_3_hyperparameter_sensitivity(
    n_episodes=10000,
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Lab 6.3: Hyperparameter Sensitivity Analysis
======================================================================

Grid: lambda  in  [0.1, 1.0, 10.0] x alpha  in  [0.5, 1.0, 2.0]
Episodes per config: 10,000

Results (average reward, last 2,500 episodes):

         | alpha=0.5  | alpha=1.0  | alpha=2.0
-----------------------------------
lambda=0.1   |  0.51 |  0.52 |  0.51
lambda=1.0   |  0.51 |  0.52 |  0.51
lambda=10.0  |  0.49 |  0.52 |  0.52

Best: lambda=1.0, alpha=1.0 -> 0.52

Insights:
  - Higher lambda provides stronger regularization (prevents overfitting)
  - Higher alpha increases exploration (helps with uncertain arms)
  - Optimal tradeoff depends on problem structure and horizon
```

#### Analysis

The hyperparameter sensitivity results show:

1. **Robust defaults:** $\lambda = 1.0$, $\alpha = 1.0$ performs well across the grid
2. **Flat landscape:** Performance varies only slightly (0.49-0.52), suggesting the algorithm is not highly sensitive to hyperparameters in this simplified setting
3. **Interaction effects:** High $\lambda$ (underfitting) can be partially compensated by high $\alpha$ (more exploration)

**Practical recommendation:** Start with $\lambda = 1.0$, $\alpha = 1.0$. Only tune if performance is clearly suboptimal.

---

### Lab 6.4: Exploration Dynamics Visualization (15 min)

**Objective:** Visualize how bandits explore template space over time.

#### Solution

```python
from scripts.ch06.lab_solutions import lab_6_4_exploration_dynamics

results = lab_6_4_exploration_dynamics(
    n_episodes=5000,
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Lab 6.4: Exploration Dynamics Visualization
======================================================================

Episode 100:
  Avg reward (last 500): 0.607
  Total uncertainty: 3.472
  Selection freq: [0.27 0.26 0.27 0.2 ]

Episode 500:
  Avg reward (last 500): 0.870
  Total uncertainty: 1.978
  Selection freq: [0.246 0.264 0.262 0.228]

Episode 1,000:
  Avg reward (last 500): 0.982
  Total uncertainty: 1.683
  Selection freq: [0.27  0.266 0.238 0.226]

Episode 2,000:
  Avg reward (last 500): 0.988
  Total uncertainty: 1.455
  Selection freq: [0.246 0.23  0.258 0.266]

Episode 5,000:
  Avg reward (last 500): 0.992
  Total uncertainty: 1.254
  Selection freq: [0.3   0.22  0.276 0.204]

======================================================================
Summary
======================================================================

Uncertainty reduction: 15.50 -> 1.25
Reduction ratio: 12.4x

Final selection distribution (last 1000):
  Arm 0: 27.4%
  Arm 1: 23.2%
  Arm 2: 26.9%
  Arm 3: 22.5%
```

#### Analysis

The exploration dynamics reveal the classic bandit learning phases:

1. **Early exploration (0-100):** High uncertainty (3.47), uniform-ish selection, low reward (0.607)
2. **Learning (100-2000):** Uncertainty decreases (3.47 -> 1.45), reward improves (0.607 -> 0.988)
3. **Exploitation (2000-5000):** Low uncertainty (1.25), stable selection, near-optimal reward (0.992)

The **12.4x uncertainty reduction** demonstrates how Thompson Sampling naturally transitions from exploration to exploitation as it learns the reward structure.

---

### Lab 6.5: Multi-Seed Robustness (5 min)

**Objective:** Verify bandit performance is robust across random seeds.

#### Solution

```python
from scripts.ch06.lab_solutions import lab_6_5_multi_seed_robustness

results = lab_6_5_multi_seed_robustness(
    n_seeds=5,
    n_episodes=5000,
    base_seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Lab 6.5: Multi-Seed Robustness Analysis
======================================================================

Configuration:
  Seeds: 5
  Episodes per seed: 5,000
  Seed 42: 0.478
  Seed 1042: 0.685
  Seed 2042: 0.446
  Seed 3042: 0.457
  Seed 4042: 0.436

======================================================================
Statistics
======================================================================
  Mean: 0.500
  Std:  0.093
  CV:   18.7%
  Range: [0.436, 0.685]

Conclusion:
  High variance (CV=18.7%) suggests sensitivity to initialization.
```

#### Analysis

The multi-seed analysis reveals **higher variance than expected** (CV = 18.7%):

- **Seed 1042** achieves 0.685 (37% above mean)
- **Seed 4042** achieves 0.436 (13% below mean)

This variance comes from:
1. **Initialization effects:** Early random explorations can lead to different learning trajectories
2. **True parameter variability:** Each seed generates different true reward parameters
3. **Exploration randomness:** Stochastic action selection creates variance

**Practical implication:** For production deployment, either:
- Run longer (more episodes to average out variance)
- Use multiple seeds and report confidence intervals
- Warm-start from prior knowledge to reduce initialization variance

---

## Advanced Exercises (Optional)

### Exercise 6.7: Hierarchical Templates (20 min)

**Problem:** Design a two-level bandit hierarchy: meta-bandit over objectives, sub-bandits over tactics.

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_7_hierarchical_templates

results = exercise_6_7_hierarchical_templates(
    n_episodes=50000,
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Exercise 6.7: Hierarchical Templates
======================================================================

	Hierarchical structure:
	  Level 1 (Meta): 3 objectives
	    - Objective A: Maximize margin (templates: Positive CM2, Premium, Private Label)
	    - Objective B: Maximize volume (templates: Popular, Discount, Budget)
	    - Objective C: Strategic goals (templates: Strategic, Category Diversity)

  Level 2 (Sub): 2-3 templates per objective

Training hierarchical bandit for 50,000 episodes...
  Progress: 100% (50000/50000)

Meta-Level Selection Distribution:

  Objective    | Selection %
  -------------|------------
  Margin (A)   |       23.6%
  Volume (B)   |       25.2%
  Strategic(C) |       51.2%

Sub-Level Selection (within objectives):

	  Margin (A):
	    Positive CM2        |        4.6%
	    Premium             |        0.5%
	    Private Label       |       94.9%

  Volume (B):
    Popular      |       88.5%
    Discount     |        0.9%
    Budget       |       10.6%

	  Strategic(C):
	    Strategic           |       46.6%
	    Category Diversity  |       53.4%

Comparison to Flat LinUCB:

  Policy           | Convergence (ep)
  -----------------|-----------------
  Flat LinUCB      |          ~12,000
  Hierarchical     |           ~8,000

Convergence speedup: 33% faster (8k vs 12k episodes)

[OK] Hierarchical bandits converge faster with similar final performance.
```

#### Analysis

Hierarchical templates offer two advantages:

1. **Faster convergence (33%):** Fewer parameters per level means faster learning
2. **Better interpretability:** Business can understand "we shifted toward strategic optimization" rather than opaque template IDs

The learned hierarchy reveals interesting patterns:
- **Private Label dominates within Margin** (94.9%): Clear winner for margin optimization
- **Popular dominates within Volume** (88.5%): Popularity drives clicks/purchases
- **Category Diversity edges out Strategic** (53.4% vs 46.6%): Close competition in strategic tier

---

### Exercise 6.9: Query-Conditional Templates (30 min)

**Problem:** Extend templates to depend on query content, not just products.

#### Solution

```python
from scripts.ch06.lab_solutions import exercise_6_9_query_conditional_templates

results = exercise_6_9_query_conditional_templates(
    n_episodes=30000,
    seed=42,
    verbose=True
)
```

**Actual Output:**
```
======================================================================
Exercise 6.9: Query-Conditional Templates
======================================================================

Query-conditional template design:
  t(p, q) = w_base * f(p) + w_query * g(q, p)

  where g(q, p) captures query-product interaction

Training comparison (30,000 episodes):

  Policy                   | Final Reward
  -------------------------|-------------
  Product-only templates   |         0.81
  Query-conditional        |         0.98

  DeltaReward: +0.16 (+20.1%)

Insight:
  Query-conditional templates learn to AMPLIFY templates when
  query content suggests they'll be effective, and SUPPRESS
  templates when query content suggests they'll hurt.

  This is learned automatically from reward feedback---
  no manual query->template rules needed.

[OK] Query-conditional templates achieve +20.1% improvement.
```

#### Analysis

Query-conditional templates demonstrate the value of incorporating additional context. The **+20.1% improvement** comes from better matching templates to user intent signals in the query.

**Key insight:** When query says "deals," boosting discounted products is obviously good. When query says "premium," boosting discounts actively hurts. Query-conditional templates learn this automatically from reward signals---no manual rules needed.

---

## Summary

These lab solutions demonstrate the core lessons of Chapter 6:

1. **Features matter more than algorithms:** The gap between simple and rich features (where it exists) dwarfs algorithm differences.

2. **Regularization matters:** The choice of regularization mode (`blend` vs `quantized`) can flip which algorithm wins.

3. **Bandits learn segment policies automatically:** Given good features, LinUCB/TS discover which templates work for which users without manual rules.

4. **Exploration has a cost:** Early regret is the price of learning; uncertainty should diminish as information accumulates.

5. **Hyperparameters have sensible defaults:** $\lambda=1.0$, $\alpha=1.0$ work across a wide range of conditions.

6. **Variance is real:** Multi-seed evaluation can exhibit meaningful dispersion; results should be reported with confidence intervals.

7. **Production requires engineering:** Numerical stability, robust seed handling, and interpretable diagnostics matter as much as theoretical elegance.

8. **Diversity depends on context:** The value of diversity templates depends on whether the base ranker's category mix is misaligned with user preferences.

---

## Running the Code

All solutions are in `scripts/ch06/lab_solutions/`:

```bash
# Run all exercises and labs
uv run python -m scripts.ch06.lab_solutions --all

# Run specific exercise
uv run python -m scripts.ch06.lab_solutions --exercise 6.1
uv run python -m scripts.ch06.lab_solutions --exercise 6.5
uv run python -m scripts.ch06.lab_solutions --exercise 6.6b  # When diversity helps
uv run python -m scripts.ch06.lab_solutions --exercise lab6.3

# Run production simulator experiments (requires zoosim)
uv run python scripts/ch06/template_bandits_demo.py \
    --n-static 2000 \
    --n-bandit 20000 \
    --features simple \
    --world-seed 20250322 \
    --bandit-base-seed 20250349

uv run python scripts/ch06/template_bandits_demo.py \
    --n-static 2000 \
    --n-bandit 20000 \
    --features rich \
    --world-seed 20250322 \
    --bandit-base-seed 20250349 \
    --hparam-mode rich_est \
    --prior-weight 50 \
    --lin-alpha 0.2 \
    --ts-sigma 0.5

uv run python scripts/ch06/template_bandits_demo.py \
    --n-static 2000 \
    --n-bandit 20000 \
    --features rich_est \
    --world-seed 20250322 \
    --bandit-base-seed 20250349 \
    --hparam-mode rich_est \
    --prior-weight 50 \
    --lin-alpha 0.2 \
    --ts-sigma 0.5
```

---


# Chapter 6 --- Advanced Lab: From CPU Loops to GPU Batches

Time budget: 2--3 hours (spread over a few sessions if needed)  
Prerequisites: Comfortable with NumPy and basic Python; no prior PyTorch or GPU experience assumed.

---

## 1. Why This Lab Exists

In the core Chapter 6 narrative, we encountered the **canonical implementation** of template bandits:

- Static templates vs LinUCB vs Thompson Sampling
- Simple vs rich context features
- Honest failure with impoverished features, recovery with richer signals

All of that was implemented in a **clean but sequential** NumPy style:

- `scripts/ch06/template_bandits_demo.py`
- `scripts/ch06/run_bandit_matrix.py`

These scripts are perfect for:

- Reading end-to-end logic
- Instrumenting with print statements
- Connecting equations to code line-by-line

They are **not** perfect when we want to:

- Run many seeds or large grids of scenarios
- Go from 20k episodes to 200k or 2M
- Iterate quickly on feature variants or hyperparameters

At some point, the canonical implementation becomes a **compute bottleneck**: the code is mathematically right, but iteration speed collapses.

This lab provides a guided path from reading and running the Chapter 6 CPU code to using a GPU-accelerated version, understanding what changed, and knowing when it is safe to trust it.

We will walk from minimal GPU background to:

- Understanding the mental model of GPU batches
- Reading the main GPU implementation
- Running GPU-accelerated experiments
- Applying best practices to avoid subtle bugs

We do not need to be "PyTorch people" to finish this lab. We can think of the GPU as a slightly stricter NumPy: arrays live on a device, and we try hard not to move them around unnecessarily.

---

## 2. The Starting Point --- Canonical CPU Implementation

Before touching GPUs, we ensure we are comfortable with the **canonical CPU path**. We will treat it as ground truth.

The key scripts live under:

- `scripts/ch06/template_bandits_demo.py`
- `scripts/ch06/run_bandit_matrix.py`

The core experiment is implemented by:

- `run_template_bandits_experiment` in  
  `scripts/ch06/template_bandits_demo.py`

It:

1. Generates a simulator world from `SimulatorConfig`
2. Evaluates each template as a static policy
3. Runs LinUCB and Thompson Sampling over `n_bandit` episodes
4. Logs aggregate metrics (Reward, GMV, CM2, Orders) and per-segment results

The batch runner:

- `scripts/ch06/run_bandit_matrix.py`

wraps this core experiment into a small **scenario grid**:

1. Simple features vs rich features vs rich_est
2. Optional rich-regularization modes
3. A few world and bandit seeds

It then:

- Runs scenarios sequentially or in parallel threads
- Captures stdout into a JSON artifact
- Writes results under `docs/book/ch06/data/`

### 2.1. Warm-Up: Run the CPU Batch Once

From the repository root, we run:

```bash
uv run python scripts/ch06/run_bandit_matrix.py \
  --n-static 1000 \
  --n-bandit 20000 \
  --max-workers 4
```

Output (abridged, representative):

```text
Planned scenarios:
  [1/5] simple_baseline      features=simple    world_seed=20250701  bandit_seed=20250801  prior= 0 lin_alpha=1.00 ts_sigma=1.00 reg=none
  ...

[1/5] Running scenario 'simple_baseline'...
...
[OK] Completed 'simple_baseline'
...

Saved batch results to docs/book/ch06/data/bandit_matrix_20250701T120000Z.json
```

It is not necessary to inspect every line yet. The takeaway is:

- The CPU pipeline is working.
- We know how to launch the canonical experiment.
- We have a **reference JSON artifact** that later GPU runs must agree with (within noise).

---

## 3. Why the CPU Version Becomes Painful

The CPU implementation is written the way we teach algorithms:

- Clear loops
- Direct calls to `sample_user`, `sample_query`, `compute_reward`
- Easy to print intermediate values

But numerically, it behaves like this:

- Each episode is a **separate Python loop iteration**
- Each episode walks through:
  - Sample user
  - Sample query
  - Compute base scores
  - Apply each template
  - Simulate user response
  - Update bandit

Imagine a single cashier scanning items one by one, printing receipts, and answering questions. Every action requires the cashier's attention.

Now suppose we want to:

- Increase episode count from 20k to 200k
- Sweep over many seeds and feature modes
- Compare multiple hyperparameter settings

The cashier analogy turns into a bottleneck:

- Even if each episode is cheap, **Python loop overhead + per-episode simulator calls** add up.
- CPU vectorization helps in places (NumPy inside each episode), but the outer loop remains serial.

What we want instead is an **assembly line**:

- The GPU carries out the same simple operation (e.g., dot products, sampling, elementwise activations) on many items in parallel.
- We pay some setup cost (batching, moving data to GPU), but then thousands of episodes are processed in one sweep.

This is exactly what the GPU implementation under:

- `scripts/ch06/optimization_gpu/`

is designed to do.

---

## 4. GPU Mental Model: From Loops to Batches

If we are new to GPUs, the mental model can feel intimidating. We strip it down to something concrete.

### 4.1. CPU vs GPU in One Sentence

- **CPU**: a few powerful cores; great at complex control flow; mediocre at doing the same tiny operation millions of times.
- **GPU**: thousands of lightweight cores; terrible at complex branching; excellent at doing the same tiny operation on large arrays.

In Chapter 6, almost all of our heavy work is:

- Matrix multiplications, dot products, softmax and top-k, random draws, simple elementwise arithmetic

These map perfectly to the GPU.

### 4.2. Arrays with a Passport: Tensors and Devices

The only new concepts we need from PyTorch are:

- `torch.Tensor`: like a NumPy array, but can live on CPU or GPU.
- `device`: where the tensor lives (`"cpu"`, `"cuda"`, `"mps"`, ...).

In `scripts/ch06/optimization_gpu/template_bandits_gpu.py`, this is wrapped by:

- `_as_device(device)`  
  (`scripts/ch06/optimization_gpu/template_bandits_gpu.py:132-164`)

which chooses:

- CUDA if available
- Otherwise Apple MPS if available
- Otherwise CPU

We can think of it as:

```python
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
```

but with a few extra safety checks.

Once a tensor is on a device, we want to:

- Keep all subsequent operations on the same device
- Avoid moving data back-and-forth (`.cpu().numpy()`) inside tight loops

The GPU code in this repo is written so that:

- Inputs are moved to the GPU once per batch.
- All simulation steps stay in GPU memory.
- Only aggregated results are brought back to NumPy at the edges.

### 4.3. From For-Loops to Batches

Conceptually:

- CPU version:
  - Loop over `episode_idx` from 1 to `n_episodes`
  - Within each loop, sample a single user, single query, single ranking outcome
- GPU version:
  - Choose a batch size `B` (e.g. 1024)
  - Sample `B` users and `B` queries **at once**
  - Compute relevance and template boosts for all `B` episodes simultaneously
  - Simulate user behavior for all `B` episodes on the GPU

The bandit algorithms themselves (LinUCB and Thompson Sampling) remain **sequential in episode index**:

- We still update the posterior after each selected action.
- We still reveal only the chosen action's reward to the policy.

The GPU's job is not to change the algorithm; it is to **amortize simulator work** across many episodes.

---

## 5. A Guided Tour of the GPU Implementation

The GPU implementation lives primarily in:

- `scripts/ch06/optimization_gpu/template_bandits_gpu.py`

and is orchestrated by:

- `scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py`
- `scripts/ch06/optimization_gpu/run_bandit_matrix_gpu.py`

We will follow the same structure as the CPU code:

1. Build the world and templates
2. Precompute GPU-friendly representations
3. Simulate static templates
4. Run bandit policies interactively

### 5.1. Building the GPU World

The CPU version asks the simulator for:

- Catalog, users, queries, behavior, reward config

The GPU version does the same, but then **packs everything into tensors** in a `GPUWorld` dataclass:

- See `_prepare_world` in  
  `scripts/ch06/optimization_gpu/template_bandits_gpu.py:260-319`

Key pieces:

- `product_prices`, `product_cm2`, `product_discount`, `product_is_pl`
- `product_embeddings`, `normalized_embeddings`
- `template_boosts` (boost values per template x product)
- `lexical_matrix` and `pos_bias` for relevance
- `segment_params` (price and PL distributions per segment)

We can think of `GPUWorld` as:

- "All the catalog and behavior data, but now as dense tensors on a device."

Analogy:

- The CPU version keeps shelf labels, prices, and product attributes in many small Python objects.
- The GPU version moves them onto a big **warehouse whiteboard** that all workers can see in one glance.

Once `GPUWorld` is constructed, all costly operations (matrix multiplications, top-k, sampling) happen inside these tensors.

!!! note "Code <-> Simulator (GPU world construction)"
    - Files:
      - `scripts/ch06/template_bandits_demo.py:300-420` (CPU static templates and bandit runs)
      - `scripts/ch06/optimization_gpu/template_bandits_gpu.py:260-319` (GPU `GPUWorld` construction)

### 5.2. Sampling Users and Queries in Batches

The sampler helpers in the GPU implementation correspond closely to the CPU's per-episode sampling:

- `_sample_segments` chooses user segments according to `cfg.users.segment_mix`
- `_sample_theta` samples continuous user preferences for price and PL, plus a categorical preference vector
- `_sample_theta_emb` samples user embedding noise for semantic relevance

These live in:

- `scripts/ch06/optimization_gpu/template_bandits_gpu.py:320-409`

The important difference:

- All of them work on **whole batches** (`batch_size` episodes) at a time.
- They return tensors of shape `(batch_size, ...)` instead of scalars.

We can view `batch_size=1024` as:

- Drawing 1024 users
- Drawing 1024 queries
- Generating 1024 episodes worth of latent preferences

in one go, using vectorized math and GPU random generators.

### 5.3. Computing Relevance on the GPU

The base relevance scores (before templates) are computed by:

- `_compute_base_scores` in  
  `scripts/ch06/optimization_gpu/template_bandits_gpu.py:620-639`

This mirrors the Chapter 5 relevance model:

- Cosine similarity between query and product embeddings
- Lexical overlap via a precomputed matrix
- Gaussian noise

All operations are **tensor operations** on the chosen device:

- `normalized_queries @ world.normalized_embeddings.T`
- `world.lexical_matrix[query_intent_idx]`
- `torch.randn(...)` with a `torch.Generator`

The output is a tensor:

- Shape: `(batch_size, num_products)`
- Semantics: base score for each episode x product pair

### 5.4. Simulating Sessions in Parallel

The heart of the GPU acceleration is:

- `_simulate_sessions` in  
  `scripts/ch06/optimization_gpu/template_bandits_gpu.py:640-736`

This function:

1. Adds template boosts to base scores for every template and episode at once.
2. Applies `top_k` to get rankings for each template and episode.
3. Steps through positions `pos = 0, ..., top_k-1` in a vectorized loop:
   - Computes utilities from price sensitivity, PL preferences, category affinity, and semantic match.
   - Samples clicks and purchases via logistic models and behavioral noise.
   - Updates satisfaction and purchase counts.
4. Aggregates reward components (GMV, CM2, Orders, clicks) into a `TemplateBatchMetrics` tensor bundle.

Conceptually, we are running:

- "all templates x all episodes"

simultaneously, but with careful use of tensor shapes so the GPU can execute it efficiently.

### 5.5. Safety Guard and Reward Parity

A crucial design principle in this project is:

- "If the theorem does not compile, it is not ready."  
  Here: "If the GPU rewards do not match the CPU rewards, we do not trust the speedup."

The GPU implementation therefore includes an explicit **safety guard**:

- `_validate_reward_sample` in  
  `scripts/ch06/optimization_gpu/template_bandits_gpu.py:792-821`

On each batch, it:

1. Picks a random episode + template from the tensor metrics.
2. Reconstructs a concrete `(ranking, clicks, buys)` trace.
3. Calls the canonical `reward.compute_reward` from Chapter 5.
4. Asserts that the scalar reward matches the tensor reward (within tight tolerance).
5. Ensures the Chapter 5 click-weight guard remains enforced.

If there is any mismatch, the GPU code raises an error instead of silently drifting.

!!! note "Code <-> Env (reward safety guard)"
    - Files:
      - `scripts/ch06/optimization_gpu/template_bandits_gpu.py:792-821` (GPU reward validation)
      - `scripts/ch06/optimization_gpu/PARITY_FINDINGS.md:1-40` (parity log, reward safety guard)

This is the GPU analogue of a **unit test baked into the simulation loop**.

### 5.6. Interactive Bandit Policies on GPU Batches

The last piece is to connect the batched simulator to the bandit policies:

- `_run_policy_interactive` in  
  `scripts/ch06/optimization_gpu/template_bandits_gpu.py:1120-1184`

The control flow is:

1. While `produced < n_episodes`:
   - Simulate a batch of episodes and extract features + per-template metrics.
   - For each episode in the batch (loop in Python):
     - Convert features to NumPy for the policy (LinUCB or TS).
     - Select an action (template index).
     - Record the chosen reward and update the policy.
2. Aggregate per-episode and per-segment results.
3. Collect diagnostics (template selection counts and frequencies).

Notice the hybrid:

- **Outer logic**: still sequential per episode (to respect bandit semantics).
- **Inner simulator**: fully batched on the GPU.

This preserves the correctness of the learning algorithm while amortizing the heavy work.

!!! note "Code <-> Agent (GPU bandits)"
    - Files:
      - `scripts/ch06/template_bandits_demo.py:620-1120` (CPU bandit loop)
      - `scripts/ch06/optimization_gpu/template_bandits_gpu.py:1120-1184` (GPU interactive loop)

---

## 6. Running the GPU Compute Arc

Now that we have a conceptual map, we run the GPU-accelerated version of the core Chapter 6 compute arc:

- Simple features (failure)
- Rich features (recovery)

The orchestrator is:

- `scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py`

### 6.1. Basic Run on CPU (No GPU Required)

Even without a GPU, we can run the GPU implementation in **CPU mode**. This is the safest first step.

Run:

```bash
uv run python scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py \
  --n-static 2000 \
  --n-bandit 20000 \
  --device cpu \
  --batch-size 1024
```

Output (abridged, representative):

```text
CHAPTER 6 --- GPU COMPUTE ARC: SIMPLE -> RICH FEATURES
Static episodes: 2,000
Bandit episodes: 20,000
Base seed:      20250601
Batch size:     1024
Device:         cpu

Experiment 1: Simple features (failure mode) --- features=simple, ...
RESULTS (Simple Features):
  Static (best):  GMV = 123.45
  LinUCB:         GMV =  88.90  (-28.0%)
  TS:             GMV =  95.12  (-23.0%)

Experiment 2: Rich features (oracle) --- features=rich, ...
RESULTS (Rich Features):
  Static (best):  GMV = 123.45  (shared world)
  LinUCB:         GMV = 126.89  (+2.8%)
  TS:             GMV = 157.00  (+27.2%)

IMPROVEMENT (Rich vs Simple):
  LinUCB:  +38.0 GMV  (+42.8%)
  TS:      +61.9 GMV  (+65.0%)
```

We should recognize:

- The failure with `features=simple` (bandits underperform static templates).
- The recovery with `features=rich` (Chapter 6's +3% / +27% GMV story).

Behind the scenes, all the heavy lifting is already happening on batched tensors via PyTorch. We are simply running it on CPU.

### 6.2. Switching to GPU

With a GPU available, the lab is now a one-flag change:

```bash
uv run python scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py \
  --n-static 2000 \
  --n-bandit 20000 \
  --device cuda \
  --batch-size 1024
```

or simply:

```bash
uv run python scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py \
  --n-static 2000 \
  --n-bandit 20000 \
  --device auto \
  --batch-size 1024
```

Internally:

- `device="auto"` selects `"cuda"` if `torch.cuda.is_available()` is `True`, otherwise `"mps"` or `"cpu"`.
- All tensors are created directly on that device.

What we check:

- The **GMV numbers** match the CPU `device=cpu` run within minor stochastic variation.
- The wall-clock runtime for:
  - `n_bandit = 20_000` is similar or slightly faster.
  - Larger `n_bandit` (e.g. 100k, 200k) becomes noticeably faster on GPU.

### 6.3. Generated Artifacts and Plots

The script saves:

- JSON summaries:
  - `docs/book/ch06/data/template_bandits_simple_gpu_summary.json`
  - `docs/book/ch06/data/template_bandits_rich_gpu_summary.json`
- Figures:
  - Segment GMV comparison
  - Template selection frequencies for simple and rich features

The last part of the script dynamically imports:

- `scripts/ch06/plot_results.py`

to reuse the CPU plotting logic.

!!! note "Code <-> Env (compute arc CLI)"
    - Files:
      - `scripts/ch06/ch06_compute_arc.py:1-120` (CPU compute arc)
      - `scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py:1-220` (GPU compute arc)

---

## 7. GPU Batch Runner: Full Scenario Grid

The compute arc focuses on **two experiments** (simple vs rich) on a single world. When we want to sweep multiple scenarios, use:

- `scripts/ch06/optimization_gpu/run_bandit_matrix_gpu.py`

This is the GPU counterpart of:

- `scripts/ch06/run_bandit_matrix.py`

### 7.1. Running the GPU Batch Runner

Example:

```bash
uv run python scripts/ch06/optimization_gpu/run_bandit_matrix_gpu.py \
  --n-static 1000 \
  --n-bandit 20000 \
  --batch-size 1024 \
  --device auto \
  --max-workers 1 \
  --show-volume
```

Output (abridged, representative):

```text
CHAPTER 6 --- GPU BATCH RUNNER

[1/5] Running scenario 'simple_baseline' on auto...
Scenario Summary --- simple_baseline
----------------------------------
Static templates (per-episode averages):
  ...

Summary (per-episode averages):
  Policy              Reward       GMV       CM2    Orders   DeltaGMV vs static
  Static-...          123.45     123.45     80.00    0.80        +0.00%
  LinUCB              100.01      88.90     70.00    0.70       -28.00%
  ThompsonSampling    110.11      95.12     72.00    0.72       -23.00%

...

Saved batch results to docs/book/ch06/data/bandit_matrix_gpu_20250701T121000Z.json
```

Differences from the CPU batch runner:

- Additional flags:
  - `--batch-size` controls the simulation batch size on GPU.
  - `--device` chooses `"cpu"`, `"cuda"`, `"mps"`, or `"auto"`.
  - `--show-volume` adds Orders columns to tables.
- Default `--max-workers` is 1 (conservative), because:
  - Running multiple GPU-heavy jobs in parallel can cause memory pressure.
  - If we do increase it, we keep an eye on GPU memory usage.

### 7.2. When Does GPU Actually Help?

For small workloads (e.g. 2k episodes), a GPU may not be faster:

- There is overhead for:
  - Creating GPU tensors
  - Transferring any input data
  - Initializing CUDA context

The GPU shines when:

- `n_bandit` is large (e.g. 50k--500k episodes)
- `batch_size` is tuned to fill the GPU reasonably well

Analogy:

- Calling the GPU for 100 episodes is like renting a 100-seat bus to transport 3 people. The bus moves fast, but boarding takes time and we are not using its capacity.
- Calling the GPU for 100k episodes with `batch_size=4096` is like running a shuttle service at full capacity. We amortize the boarding cost and benefit from the bus's speed.

---

## 8. Best Practices for Safe CPU to GPU Migration

This lab is not just about "making it faster." It is about making sure the GPU version is:

- Correct (matches the canonical implementation)
- Reproducible (same seeds, same answers)
- Understandable (debuggable when needed)

We emphasize the following core practices.

### 8.1. Always Start on CPU

Even when the goal is GPU acceleration:

1. Run the GPU code with `--device cpu`.
2. Compare outputs against the canonical CPU scripts:
   - `template_bandits_demo.py`
   - `run_bandit_matrix.py`
3. Only once we see parity in metrics do we switch `--device` to `cuda` or `auto`.

This avoids a common failure mode:

- Changing both the **algorithm** and the **hardware** at once makes divergence diagnosis ambiguous.

### 8.2. Keep Bandit Logic on the CPU (for Now)

Notice that:

- The GPU implementation still runs LinUCB and Thompson Sampling in Python/NumPy.
- Only the heavy simulation pieces live on the GPU.

This is intentional:

- The bandit algorithms are not the bottleneck; the simulator is.
- Keeping policies on CPU makes them easier to inspect, instrument, and test.

In more advanced projects, we might move some policy computation to the GPU (e.g. neural networks). For this chapter, we keep that complexity out of the picture.

### 8.3. Avoid Excessive `.cpu()` / `.numpy()` Conversions

As a rule of thumb:

- Inside the simulator:
  - Stay in `torch.Tensor` on the chosen device.
- At the edges:
  - Convert to NumPy only when passing data into bandit logic or returning final summaries.

Excessive back-and-forth conversion has two risks:

1. Performance: each conversion can involve synchronization and memory movement.
2. Bugs: it is easy to accidentally detach from the correct batch, mix up shapes, or forget to move a tensor back.

The existing GPU code is written to minimize these conversions; use it as a template for future extensions.

### 8.4. Seed Management: Align NumPy and Torch

Look at how seeds are handled in:

- CPU implementation:
  - `scripts/ch06/template_bandits_demo.py:560-620`
- GPU implementation:
  - `scripts/ch06/optimization_gpu/template_bandits_gpu.py:1240-1274`

Both use the same pattern:

- Static templates: `base_seed`
- LinUCB: `base_seed + 1`
- Thompson Sampling: `base_seed + 2`

On GPU, this is done with:

- `torch.Generator(device=device_obj).manual_seed(base_seed + k)`
- `np.random.default_rng(base_seed + k)`

This alignment is critical:

- It allows us to run CPU and GPU versions with the **same base seeds** and expect comparable trajectories.
- Differences then reflect only the small numerical differences (e.g. float32 vs float64), not entirely different random histories.

### 8.5. Trust Parity Logs and Guardrails

The file:

- `scripts/ch06/optimization_gpu/PARITY_FINDINGS.md`

documents historical parity mismatches and their fixes:

- Reward safety guard alignment with Chapter 5
- Seed alignment between CPU and GPU

Treat this as:

- A living changelog connecting theory to implementation.
- A reminder that performance optimizations must not silently change semantics.

If we extend the GPU implementation:

- Add new findings there.
- Run small CPU vs GPU comparisons to ensure no regression.

---

## 9. Hands-On Tasks

This section turns the narrative into concrete steps. Each task builds confidence in using and extending the GPU implementation without requiring PyTorch expertise.

### Task 9.1 --- CPU vs GPU Time Comparison

Goal:

- Measure how runtime scales with `n_bandit` on CPU vs GPU.

Steps:

1. Pick a fixed `n_static` (e.g. 2000).
2. Sweep `n_bandit` over `{20_000, 50_000, 100_000}`.
3. For each setting, run:

   ```bash
   # CPU
   time uv run python scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py \
     --n-static 2000 \
     --n-bandit N \
     --device cpu \
     --batch-size 1024

   # GPU (if available)
   time uv run python scripts/ch06/optimization_gpu/ch06_compute_arc_gpu.py \
     --n-static 2000 \
     --n-bandit N \
     --device cuda \
     --batch-size 1024
   ```

4. Record:
   - Wall-clock time
   - GMV results (to verify parity)

Output (example summary table):

```text
N       CPU time   GPU time   LinUCB GMV (CPU/GPU)   TS GMV (CPU/GPU)
20k     0m45s      0m40s      126.9 / 127.0          157.0 / 156.9
50k     2m00s      1m05s      127.1 / 127.2          156.8 / 156.8
100k    4m10s      1m50s      127.0 / 127.1          157.1 / 157.1
```

Exact numbers will differ, but we expect:

- Near-identical GMV values across CPU and GPU.
- Increasing relative speedup of GPU as `N` grows.

### Task 9.2 --- Batch Size Trade-Off

Goal:

- Understand how `batch_size` affects performance and memory usage.

Steps:

1. Fix `n_static=2000`, `n_bandit=50_000`, `device=cuda` (or `cpu` if no GPU).
2. Try:
   - `batch-size = 256`
   - `batch-size = 1024`
   - `batch-size = 4096`
3. Measure runtime and watch memory usage (on GPU, `nvidia-smi` can help).

Interpretation:

- Small batch sizes underutilize the GPU (more Python overhead, less parallelism).
- Very large batch sizes may:
  - Run out of memory
  - Increase variance in per-batch timing

We look for a **sweet spot** where:

- Runtime is near-minimal
- Memory usage is stable

### Task 9.3 --- Feature Mode Sanity Check

Goal:

- Verify that the "simple vs rich" narrative still holds under GPU acceleration.

Steps:

1. Run `ch06_compute_arc_gpu.py` with:
   - `--features` is controlled internally by the script (simple vs rich).
2. Compare:
   - Simple features GMV gap vs static.
   - Rich features GMV improvement vs static.
3. Compare CPU (`--device cpu`) vs GPU (`--device cuda`) runs.

Expected qualitative result:

- Simple features:
  - LinUCB and TS **underperform** static templates.
- Rich features:
  - TS achieves **+20--30% GMV** vs static in this simulator configuration.

If this narrative flips, treat it as a red flag and investigate:

- Configuration mismatches
- Seed misalignment
- Code changes that alter the reward definition

### Task 9.4 --- Extend a Diagnostic

Goal:

- Add a small, safe diagnostic to the GPU implementation and verify that it behaves as expected.

Ideas (pick one):

- Log the average fraction of PL products in the top-k under each template.
- Track the fraction of episodes where the bandit chooses the static best template.
- Count how often each user segment sees an improvement vs static.

Implementation sketch:

1. Identify where the relevant tensors live (`TemplateBatchMetrics`, segment indices, etc.).
2. Compute a simple summary statistic per batch.
3. Aggregate into a Python list or running average.
4. Print it at the end of the run (or add it to the JSON artifact).

The intention is not to redesign the algorithm, but to:

- Practice reading and modifying `template_bandits_gpu.py`.
- Build confidence in extending a GPU implementation safely.

---

## 10. Reflection: When to Reach for the GPU

After this lab, we should be able to answer:

1. **Why** did we move from the canonical CPU implementation to a GPU-accelerated one?
   - Not because CPUs are "bad," but because experiments need to scale.
2. **What** changed in the GPU version?
   - Outer loops remain semantically the same.
   - Inner simulator steps are batched and moved to tensors on a device.
3. **How** do we know we did not break the math?
   - Reward parity checks.
   - Seed alignment.
   - Small-N CPU vs GPU comparisons.

The deeper lesson is not "use GPUs." It is:

- **Separate correctness from performance.**  
  First, write a canonical implementation that we can reason about line-by-line.  
  Then, and only then, introduce acceleration---while constantly checking that we have not changed the underlying problem.

When we later move to:

- Chapter 7 (continuous actions with neural networks)
- Chapter 12 (differentiable ranking)
- Chapter 13 (offline RL with deep value functions)

this habit will protect us from many subtle, expensive bugs.

We now have a first serious experience with:

- Porting a nontrivial RL experiment from CPU loops to GPU batches.
- Maintaining theoretical guarantees and simulator semantics across implementations.

That is the level of care expected of a practicing RL engineer.

---


