---
title: "Reinforcement Learning for Search from Scratch - Chapter 0 (with Labs & Solutions)"
author: "Vlad Prytula"
---

<!-- AUTOGENERATED: ch00 bundle from docs/book/ch00 (incl. labs + available solutions) -->

# Chapter 0 --- Motivation: A First RL Experiment

*Vlad Prytula*

## 0.0 Who Should Read This?

This chapter is an optional warm-up: it is deliberately light on mathematics and heavy on code. We build a tiny search world, train a small agent to learn context-adaptive boost weights, and observe the core RL loop in action. Chapters 1--3 provide the rigorous foundations that explain *why* the experiment works and *when* it fails.

Two reading paths work well:

- Practitioner track: begin here; the goal is a working end-to-end system in ~30 minutes, then return to theory as needed.
- Foundations track: skim this chapter for the concrete thread, then begin Chapter 1's rigorous development; we return here whenever an example is useful.

Ethos: every theorem in this book compiles. Mathematics and code are in constant dialogue.

---

## 0.1 The Friday Deadline

We consider the following scenario. We have just joined the search team at zooplus, Europe's leading pet supplies retailer. Our first task seems straightforward: improve the ranking for "cat food" searches.

The current system uses Elasticsearch's BM25 relevance plus some manual boost multipliers---a `category_match` bonus, a `discount_boost` for promotions, a `margin_boost` for profitable products. Our manager hands us last week's A/B test results and says:

> "Revenue is flat, but profit dropped 8%. Can we fix the boosts by Friday?"

We dig into the data. The test increased `discount_boost` from 1.5 to 2.5, hoping to drive sales. It worked---clicks went up 12%. But the wrong people clicked. Price-sensitive shoppers loved the discounted bulk bags. Premium customers, who usually buy veterinary-grade specialty foods, saw cheap products ranked first and bounced. Click-through rate (CTR) rose, but conversion rate (CVR) plummeted for high-value segments.

The problem is clear: one set of boost weights cannot serve all users. Price hunters need discount_boost = 2.5. Premium shoppers need discount_boost = 0.3. Bulk buyers fall somewhere in between.

We need **context-adaptive weights** that adjust to user type. But testing all combinations manually would take months of A/B experiments.

This is where reinforcement learning enters the story.

---

## 0.2 The Core Insight: Boosts as Actions

We reframe the problem in RL language. If any terminology is unfamiliar, we treat it as a working placeholder: Chapters 1--3 make each object precise and state the assumptions under which it is well-defined.

**Context** (what we observe): User segment, query type, session history
**Action** (what we choose): Boost weight template $\mathbf{w} = [w_{\text{discount}}, w_{\text{quality}}, w_{\text{margin}}, \ldots]$
**Outcome** (what happens): User clicks, purchases, abandons
**Reward** (what we optimize): GMV + profitability + engagement (we'll make this precise in a moment)

Traditional search tuning treats boosts as **fixed parameters** to optimize offline. RL treats them as **actions to learn online**, adapting to each context.

The Friday deadline problem becomes: *Can an algorithm learn which boost template to use for each user type, using only observed outcomes (clicks, purchases, revenue)?*

The answer is yes; we now build it.

---

## 0.3 A Tiny World: Toy Simulator and Reward

We start with a high-signal toy environment. Three user types, ten products, a small action space. The goal is intuition and a quick end-to-end run. Chapter 4 builds the realistic simulator (`zoosim`).

### 0.3.1 User Types

Real search systems have complex user segmentation (behavioral embeddings from clickstreams, transformer-based intent models, predicted LTV, real-time session signals). Our toy has three archetypes:

```python
from typing import NamedTuple

class UserType(NamedTuple):
    """User preferences over product attributes.

    Fields:
        discount: Sensitivity to discounts (0 = indifferent, 1 = only buys discounts)
        quality: Sensitivity to brand quality (0 = indifferent, 1 = only buys premium)
    """
    discount: float
    quality: float

USER_TYPES = {
    "price_hunter": UserType(discount=0.9, quality=0.1),  # Budget-conscious
    "premium":      UserType(discount=0.1, quality=0.9),  # Quality-focused
    "bulk_buyer":   UserType(discount=0.5, quality=0.5),  # Balanced
}
```

These map to real patterns:
- **Price hunters**: ALDI shoppers, coupon clippers, bulk buyers
- **Premium**: Brand-loyal, willing to pay for specialty/veterinary products
- **Bulk buyers**: Multi-pet households, mix of price and quality

### 0.3.2 Products (Sketch)

Ten products with simple features:

```python
from dataclasses import dataclass

@dataclass
class Product:
    id: int
    base_relevance: float  # BM25-like score for query "cat food"
    margin: float          # Profit margin (0.1 = 10%)
    quality: float         # Brand quality score (0-1)
    discount: float        # Discount flag (0 or 1)
    price: float           # EUR per item
```

Example: Product 3 is a premium veterinary diet (high quality, high margin, no discount, high price). Product 7 is a bulk discount bag (low quality, low margin, discounted, low price per kg).

We'll use deterministic generation with a fixed seed so results are reproducible.

### 0.3.3 Actions: Boost Weight Templates

The full action space is continuous: $\mathbf{a} = [w_{\text{discount}}, w_{\text{quality}}, w_{\text{margin}}] \in [-2, 2]^3$.

For this chapter, we **discretize** to a $5 \times 5$ grid (25 templates) to keep learning tabular and fast:

```python
import numpy as np

# Discretize [-1, 1] x [-1, 1] into a 5x5 grid
discount_values = np.linspace(-1, 1, 5)  # [-1.0, -0.5, 0.0, 0.5, 1.0]
quality_values = np.linspace(-1, 1, 5)

ACTIONS = [
    (w_disc, w_qual)
    for w_disc in discount_values
    for w_qual in quality_values
]  # 25 total actions
```

Each action is a **template**: a pair `(w_discount, w_quality)` that modifies the base relevance scores.

Why do we discretize? Tabular Q-learning needs a finite action space. Chapter 7 handles continuous actions via regression and optimization. Here we use the simplest algorithm that works end-to-end.

### 0.3.4 Toy Reward Function

Real search systems balance multiple objectives (see Chapter 1, #EQ-1.2 for the full formulation). Our toy uses a simplified scalar:

$$
R_{\text{toy}} = 0.6 \cdot \text{GMV} + 0.3 \cdot \text{CM2} + 0.1 \cdot \text{CLICKS}
$$

Components:

- **GMV** (Gross Merchandise Value): Total EUR purchased (simulated based on user preferences + product attributes + boost-induced ranking)
- **CM2** (Contribution Margin 2): Profitability after variable costs
- **CLICKS**: Engagement signal (prevents pure GMV exploitation; see Chapter 1, Section 1.2.1 for why this matters)

Notes:

- No explicit STRAT (strategic exposure) term in the toy
- Chapter 1 presents the general, numbered formulation that this toy instantiates
- The weights (0.6, 0.3, 0.1) are business parameters, not learned

!!! info "Pedagogical Simplification"
    The full $R_{\text{toy}}$ formula requires simulating user interactions (clicks, purchases, cart dynamics).
    For Chapter 0's Q-learning demonstration, we use a **closed-form surrogate** (Section 0.4.3)
    that captures the essential preference-alignment structure without simulator complexity.
    The true GMV/CM2/click-based reward appears in Chapter 4+ with the full `zoosim` environment.

Key property: $R_{\text{toy}}$ is stochastic. The same user type and boost weights can yield different outcomes due to user behavior noise (clicks are probabilistic, cart abandonment is random). This forces the agent to learn robust policies.

---

## 0.4 A First RL Agent: Tabular Q-Learning

We now arrive at the core idea: learn which boost template to use for each user type via $\varepsilon$-greedy tabular learning.

### 0.4.1 Problem Recap

- **Contexts** $\mathcal{X}$: Three user types `{price_hunter, premium, bulk_buyer}`
- **Actions** $\mathcal{A}$: 25 boost templates ($5 \times 5$ grid)
- **Reward** $R$: Stochastic $R_{\text{toy}}$ from Section 0.3.4
- **Goal**: Find a policy $\pi: \mathcal{X} \to \mathcal{A}$ that maximizes expected reward

This is a **contextual bandit** (Chapter 1 makes this formal). Each episode:

1. Sample user type $x \sim \rho$ (uniform over 3 types)
2. Choose action $a = \pi(x)$ (boost template)
3. Simulate user behavior under ranking induced by $a$
4. Observe reward $r \sim R(x, a)$
5. Update policy $\pi$

No sequential state transitions (yet). Single-step decision. Pure exploration-exploitation.

### 0.4.2 Algorithm: $\varepsilon$-Greedy Q-Learning

We'll maintain a **Q-table**: $Q(x, a) \approx \mathbb{E}[R \mid x, a]$ (expected reward for using boost template $a$ in context $x$).

Policy:
- With probability $\varepsilon$: explore (random action)
- With probability $1 - \varepsilon$: exploit ($a^* = \arg\max_a Q(x, a)$)

Update rule (after observing $r$):
$$
Q(x, a) \leftarrow (1 - \alpha) Q(x, a) + \alpha \cdot r
$$

This is **incremental mean estimation** (stochastic approximation), not Q-learning in the MDP sense. With constant learning rate $\alpha$, this converges to a weighted average of recent rewards. With decaying $\alpha_t \propto 1/t$, it converges to $\mathbb{E}[R \mid x, a]$ by the Robbins-Monro theorem [@robbins:stochastic_approx:1951].

We call this "Q-learning" informally because we are learning a Q-table, but the standard Q-learning algorithm for MDPs includes a $\gamma \max_{a'} Q(s', a')$ term for bootstrapping future values. In bandits ($\gamma = 0$), this term vanishes, reducing to the update above. Chapter 3's Bellman contraction analysis applies to the general MDP case; for bandits, standard stochastic approximation suffices.

### 0.4.3 Minimal Implementation

Here's the complete agent in ~50 lines.

**Pedagogical reward model.** Rather than simulate full user interactions (GMV, CM2, clicks),
we use a closed-form reward that encodes user preferences directly:

- Price hunters prefer high discount weight ($w_{\text{disc}}$)
- Premium users prefer high quality weight ($w_{\text{qual}}$)
- Bulk buyers prefer balanced, moderate weights

This surrogate enables rapid Q-learning iterations while preserving the
essential optimization structure. The output rewards are **preference-alignment scores**
(not EUR), with values typically in $[-2, 3]$.

**Discretization note.** We index the $5 \times 5$ grid of weight pairs for compactness:
action $(i, j)$ maps to weights via $w = -1 + 0.5 \cdot \text{index}$.
Thus $(0,0) \mapsto (-1, -1)$ and $(4,4) \mapsto (1, 1)$.

```python
import numpy as np
from typing import List, Tuple

# Setup
rng = np.random.default_rng(42)  # Reproducibility
X = ["price_hunter", "premium", "bulk_buyer"]  # Contexts
A = [(i, j) for i in range(5) for j in range(5)]  # 25 boost templates (indexed)

# Initialize Q-table: Q[context][action] = 0.0
Q = {x: {a: 0.0 for a in A} for x in X}


def choose_action(x: str, eps: float = 0.1) -> Tuple[int, int]:
    """Epsilon-greedy action selection.

    Args:
        x: User context (type)
        eps: Exploration probability

    Returns:
        Boost template (w_discount_idx, w_quality_idx)
    """
    if rng.random() < eps:
        return A[rng.integers(len(A))]  # Explore
    return max(A, key=lambda a: Q[x][a])  # Exploit


def reward(x: str, a: Tuple[int, int]) -> float:
    """Simulate reward for context x and action a.

    Toy model: preference alignment + noise.
    In reality, this would run the full simulator (rank products,
    simulate clicks/purchases, compute GMV+CM2+CLICKS).

    Args:
        x: User type
        a: Boost template indices (i, j) in [0, 4] x [0, 4]

    Returns:
        Scalar reward ~ R_toy from Section 0.3.4
    """
    i, j = a  # i = discount index, j = quality index

    # Map indices to [-1, 1] weights
    # i=0 -> w_discount=-1.0, i=4 -> w_discount=1.0
    w_discount = -1.0 + 0.5 * i
    w_quality = -1.0 + 0.5 * j

    # Simulate reward based on user preferences
    if x == "price_hunter":
        # Prefer high discount boost (i=4), low quality boost (j=0)
        base = 2.0 * w_discount - 0.5 * w_quality
    elif x == "premium":
        # Prefer high quality boost (j=4), low discount boost (i=0)
        base = 2.0 * w_quality - 0.5 * w_discount
    else:  # bulk_buyer
        # Balanced preferences: penalize extreme boosts, prefer moderate values
        base = 1.0 - abs(w_discount) - abs(w_quality)

    # Add stochastic noise (user behavior variability)
    noise = rng.normal(0.0, 0.5)

    return float(base + noise)


def train(T: int = 3000, eps: float = 0.1, lr: float = 0.1) -> List[float]:
    """Train Q-learning agent for T episodes.

    Args:
        T: Number of training episodes
        eps: Exploration probability (epsilon-greedy)
        lr: Learning rate ($\alpha$ in update rule)

    Returns:
        List of rewards per episode (for plotting learning curves)
    """
    history = []

    for t in range(T):
        # Sample context (user type) uniformly
        x = X[rng.integers(len(X))]

        # Choose action (boost template) via epsilon-greedy
        a = choose_action(x, eps)

        # Simulate outcome and observe reward
        r = reward(x, a)

        # Q-learning update: Q(x,a) <- (1-alpha)Q(x,a) + alpha*r
        Q[x][a] = (1 - lr) * Q[x][a] + lr * r

        history.append(r)

    return history


# Train agent
hist = train(T=3000, eps=0.1, lr=0.1)

# Evaluate learned policy
print(f"Final average reward (last 100 episodes): {np.mean(hist[-100:]):.3f}")
print("\nLearned policy:")
for x in X:
    a_star = max(A, key=lambda a: Q[x][a])
    print(f"  {x:15s} -> action {a_star} (Q = {Q[x][a_star]:.3f})")
```

With a fixed seed, we obtain representative output of the form (preference-alignment scores, not EUR):

```
Final average reward (last 100 episodes): 1.640  # preference-alignment scale
Learned policy:
  price_hunter    -> action (4, 1) (Q = 1.948)
  premium         -> action (1, 4) (Q = 2.289)
  bulk_buyer      -> action (2, 2) (Q = 0.942)
```

What just happened?

1. The agent explored 25 boost templates $\times$ 3 user types = 75 state-action pairs
2. After 3000 episodes, it learned:
   - **Price hunters**: Use `(4, 1)` = high discount boost (+1.0), low quality boost (-0.5)
   - **Premium shoppers**: Use `(1, 4)` = low discount boost (-0.5), high quality boost (+1.0)
   - **Bulk buyers**: Use `(2, 2)` = balanced boosts (0.0, 0.0) --- exactly optimal.
3. This matches our intuition from Section 0.3.1!

Stochastic convergence. Across random seeds, the learned actions might vary slightly (e.g., `(4, 0)` vs `(4, 1)` for price hunters), but the pattern holds: discount-heavy for price hunters, quality-heavy for premium shoppers, balanced for bulk buyers.

### 0.4.4 Learning Curves and Baselines

We visualize learning progress and compare to baselines.

```python
import matplotlib.pyplot as plt

def plot_learning_curves(history: List[float], window: int = 50):
    """Plot smoothed learning curve with baselines."""
    # Compute rolling average
    smoothed = np.convolve(history, np.ones(window)/window, mode='valid')

    fig, ax = plt.subplots(figsize=(10, 6))

    # Learning curve
    ax.plot(smoothed, label='Q-learning (smoothed)', linewidth=2)

    # Baselines
    random_baseline = np.mean([reward(x, A[rng.integers(len(A))])
                               for _ in range(1000)
                               for x in X])
    ax.axhline(random_baseline, color='red', linestyle='--',
               label=f'Random policy ({random_baseline:.2f})')

    # Static best (tuned for average user)
    static_best = np.mean([reward(x, (2, 2)) for _ in range(300) for x in X])
    ax.axhline(static_best, color='orange', linestyle='--',
               label=f'Static best ({static_best:.2f})')

    # Oracle (knows user type, chooses optimally)
    # Optimal actions: price_hunter->(4,0), premium->(0,4), bulk_buyer->(2,2)
    oracle_rewards = {
        "price_hunter": np.mean([reward("price_hunter", (4, 0)) for _ in range(50)]),
        "premium": np.mean([reward("premium", (0, 4)) for _ in range(50)]),
        "bulk_buyer": np.mean([reward("bulk_buyer", (2, 2)) for _ in range(50)]),
    }
    oracle = np.mean(list(oracle_rewards.values()))
    ax.axhline(oracle, color='green', linestyle='--',
               label=f'Oracle ({oracle:.2f})')

    ax.set_xlabel('Episode')
    ax.set_ylabel('Reward (smoothed)')
    ax.set_title('Learning Curve: Contextual Bandit for Boost Optimization')
    ax.legend()
    ax.grid(alpha=0.3)

    plt.tight_layout()
    return fig

# Generate and save plot
fig = plot_learning_curves(hist)
fig.savefig('docs/book/ch00/learning_curves.png', dpi=150)
print("Saved learning curve to docs/book/ch00/learning_curves.png")
```

Expected output:

![Learning Curves](docs/book/ch00/learning_curves.png)

- **Random policy** (red dashed): ~0.0 average reward (baseline---random actions average out)
- **Static best** (orange dashed): ~0.3 (one-size-fits-all `(2,2)` helps bulk buyers but hurts price hunters and premium)
- **Q-learning** (blue solid): Starts near 0, converges to ~1.6 by episode 1500
- **Oracle** (green dashed): ~2.0 (theoretical maximum with perfect knowledge of optimal actions per user)

Key insight: Q-learning reaches about 82% of oracle performance by learning from experience alone. No manual tuning and no A/B tests are required. In this run, the bulk buyer segment recovers the optimal action `(2, 2)`.

---

## 0.5 Reading the Experiment: What We Learned

### Convergence Pattern

The learning curve has three phases:

1. **Pure exploration** (episodes 0--500): High variance, $\varepsilon$-greedy tries random actions, Q-values are noisy
2. **Exploitation begins** (episodes 500--1500): Agent identifies good actions per context, reward climbs steadily
3. **Convergence** (episodes 1500--3000): Q-values stabilize, reward plateaus at ~82% of oracle

This is **regret minimization** in action. Chapter 1 formalizes this; Chapter 6 analyzes convergence rates.

### Per-Segment Performance

If we track rewards separately by user type:

```python
# Track per-segment performance
segment_rewards = {x: [] for x in X}

for _ in range(100):  # 100 test episodes
    for x in X:
        a = max(A, key=lambda a: Q[x][a])  # Greedy policy (no exploration)
        r = reward(x, a)
        segment_rewards[x].append(r)

for x in X:
    print(f"{x:15s}: mean reward = {np.mean(segment_rewards[x]):.3f}")
```

**Output:**
```
price_hunter   : mean reward = 2.309
premium        : mean reward = 2.163
bulk_buyer     : mean reward = 0.917
```

**Analysis:**

- **Price hunters** get the highest rewards (~2.3)---the agent found a near-optimal action `(4, 1)` with high discount boost
- **Premium shoppers** get high rewards (~2.2)---high quality boost `(1, 4)` closely matches their preferences
- **Bulk buyers** get lower rewards (~0.9) because their **balanced preferences** have inherently lower optimal reward (base=1.0 at `(2,2)`) compared to polarized users (base=2.5). But the agent finds the **exact optimal**!
- All three segments dramatically beat the static baseline (~0.3 average) through personalization

This is **personalization** at work: different users get different rankings, each optimized for their revealed preferences.

### What We (Hand-Wavily) Assumed

This toy experiment "just worked," but we made implicit assumptions:

1. **Rewards are well-defined expectations** over stochastic outcomes (Chapter 2 makes this measure-theoretically rigorous)
2. **Exploration is safe** (in production, bad rankings lose users; Chapter 9 introduces off-policy evaluation for safer testing)
3. **The logging policy and new policy have sufficient overlap** to compare fairly (importance weights finite; Chapter 9)
4. **$\varepsilon$-greedy tabular Q converges** (for bandits, this follows from stochastic approximation theory; Chapter 3's Bellman contraction analysis applies to the full MDP case with $\gamma > 0$)
5. **Actions are discrete and state space is tiny** (Chapter 7 handles continuous actions; Chapter 4 builds realistic state)

None of these are free. The rest of the book makes them precise and shows when they hold (or how to proceed when they don't).

### Theory-Practice Gap: $\varepsilon$-Greedy Exploration

Our toy used $\varepsilon$-greedy exploration with constant $\varepsilon = 0.1$. This deserves scrutiny.

What theory says: In a stochastic $K$-armed bandit, a constant exploration rate $\varepsilon$ forces perpetual uniform exploration and yields linear regret. If $\varepsilon_t \to 0$ with a suitable schedule, $\varepsilon$-greedy can achieve sublinear regret, but its exploration remains uniform over non-greedy arms. By contrast, UCB-type algorithms direct exploration through confidence bounds and achieve logarithmic (gap-dependent) regret and worst-case $\tilde{O}(\sqrt{KT})$ regret [@auer:ucb:2002; @lattimore:bandit_algorithms:2020].

What practice shows: $\varepsilon$-greedy with constant $\varepsilon \in [0.05, 0.2]$ is often competitive because:

1. **Trivial to implement**: No confidence bounds, no posterior sampling, just a random number generator
2. **Handles non-stationarity gracefully**: Continues exploring even after "convergence" (useful when user preferences drift)
3. **The regret difference matters only at scale**: For the short horizons in this chapter, the gap between $\varepsilon$-greedy and UCB is typically negligible

When $\varepsilon$-greedy fails: High-dimensional action spaces where uniform exploration wastes samples. For our 25-action toy problem, it is adequate. For Chapter 7's continuous actions ($10^{100}$ effective arms), we need structured exploration (UCB, Thompson Sampling).

Modern context: Google's 2010 display ads paper [@li:contextual_bandit_approach:2010] used $\varepsilon$-greedy successfully at scale. In many contemporary bandit systems, Thompson Sampling is a strong default due to its uncertainty-driven exploration and empirical performance [@russo:tutorial_ts:2018; @lattimore:bandit_algorithms:2020].

**Why UCB and Thompson Sampling?** (Preview for Chapter 6)

$\varepsilon$-greedy explores **uniformly**---it wastes samples on arms it already knows are bad. UCB explores **optimistically**---it tries arms whose rewards *might* be high given uncertainty:

- **UCB:** Choose $a_t = \arg\max_a [Q(x,a) + \beta \sigma(x,a)]$ where $\sigma$ is a confidence width. Explores arms with high uncertainty, not randomly.
- **Thompson Sampling:** Maintain posterior $P(Q^* \mid \text{data})$, sample $\tilde{Q} \sim P$, act greedily on sample. Naturally balances exploration (high posterior variance $\rightarrow$ diverse samples) with exploitation.

Both achieve $\tilde{O}(d\sqrt{T})$ regret for $d$-dimensional linear bandits---matching the lower bound up to logarithms [@chu:contextual_bandits:2011; @lattimore:bandit_algorithms:2020]. In this structured setting, naive uniform exploration can be provably suboptimal, and the gap widens in high dimensions.

---

## 0.6 Limitations: Why We Need the Rest of the Book

Our toy is **pedagogical**, not production-ready. Here's what breaks at scale:

### 1. Discrete Action Space

We used 25 templates. Real search has continuous boosts: $\mathbf{w} \in [-5, 5]^{10}$ (ten features, unbounded). Discretizing to a grid would require $100^{10} = 10^{20}$ actions---intractable.

**Solution:** Chapter 7 introduces **continuous action bandits** via $Q(x, a)$ regression and cross-entropy method (CEM) optimization.

### 2. Tabular State Representation

We had 3 user types. Real search has thousands of user segments (RFM bins, geographic regions, device types, time-of-day). Plus query features (length, specificity, category). A realistic context space is **high-dimensional and continuous**.

**Solution:** Chapter 6 (neural linear bandits), Chapter 7 (deep Q-networks with continuous state/action).

### 3. No Constraints

Our agent optimized $R_{\text{toy}}$ without guardrails. Real systems must enforce:
- Profitability floors (CM2 $\geq$ threshold)
- Exposure targets (strategic products get visibility)
- Rank stability (limit reordering volatility)

**Solution:** Chapter 10 introduces production **guardrails** (CM2 floors, $\Delta$Rank@k stability). The CMDP/Lagrangian formalism is previewed in Chapter 3 (Remark 3.5.3) and developed rigorously in Appendix C (convex duality), then applied operationally in Chapter 10.

### 4. Simplified Position Bias

We didn't model how clicks depend on rank. Real users exhibit **position bias** (top-3 slots get 80% of clicks) and **abandonment** (quit after 5 results if nothing relevant).

**Solution:** Chapter 2 develops PBM/DBN click models; Chapter 5 implements them in `zoosim`.

### 5. Online Exploration Risk

We trained by interacting with users directly (episodes = real searches). In production, bad rankings **cost real money** and **lose real users**. We need safer evaluation.

**Solution:** Chapter 9 introduces **off-policy evaluation (OPE)**: estimate new policy performance using logged data from old policy, without deploying.

### 6. Single-Episode Horizon

We treated each search as independent. Real users return across sessions. Today's ranking affects tomorrow's retention.

**Solution:** Chapter 11 extends to **multi-episode MDPs** with inter-session dynamics (retention, satisfaction state).

---

## 0.7 Map to the Book

Here's how our toy connects to the rigorous treatment ahead:

| **Toy Concept**          | **Formal Treatment**                          | **Chapter** |
|--------------------------|-----------------------------------------------|-------------|
| User types               | Context space $\mathcal{X}$, distribution $\rho$ | 1           |
| Boost templates          | Action space $\mathcal{A}$, policy $\pi$         | 1, 6, 7     |
| $R_{\text{toy}}$         | Reward function $R: \mathcal{X} \times \mathcal{A} \times \Omega \to \mathbb{R}$, constraints | 1           |
| $\varepsilon$-greedy Q-learning | Bellman operator, contraction mappings        | 3           |
| Stochastic outcomes      | Probability spaces, click models (PBM/DBN)    | 2           |
| Learning curves          | Regret bounds, sample complexity              | 6           |
| Static best vs oracle    | Importance sampling, off-policy evaluation    | 9           |
| Guardrails (missing)     | CMDP preview (Remark 3.5.3), duality (Appendix C), production guardrails | 3, App. C, 10 |
| Engagement proxy         | Multi-episode MDP, retention modeling         | 11          |

Chapters 1--3 provide foundations: contextual bandits, measure theory, Bellman operators.
Chapters 4--8 build the simulator and core algorithms.
Chapters 9--11 handle evaluation, robustness, and production deployment.
Chapters 12--15 cover frontier methods (slate ranking, offline RL, multi-objective optimization).

---

## 0.8 How to Use This Book

### For Practitioners

We recommend working through Chapter 0 in full: we run the code, modify the reward function (Exercise 0.1), and compare exploration strategies (Exercise 0.2).

We then skim Chapters 1--3 on a first read. We focus on:
- The reward formulation (#EQ-1.2 in Chapter 1)
- Why engagement matters (Section 1.2.1)
- The Bellman contraction intuition (Chapter 3, skip proof details initially)

We then dive into Chapters 4--11 (simulator, algorithms, evaluation). This provides the implementation roadmap.

We return to theory as needed. When something fails (e.g., divergence in a Q-network), we revisit Chapter 3's convergence analysis.

### For Researchers / Mathematically Inclined

We skim Chapter 0 to see the concrete thread.

We start at Chapter 1. We work through definitions, theorems, and proofs, and we verify that the code validates the mathematics.

We do the exercises: a mix of proofs (30%), implementations (40%), experiments (20%), and conceptual questions (10%).

We use Chapter 0 as a touchstone. When abstractions feel heavy, we return to the toy: "How does this theorem explain why the tabular method stabilized in Section 0.4?"

### For Everyone

Ethos: mathematics and code are inseparable. Every theorem compiles. Every algorithm is proven rigorous, then implemented in production-quality code. Theory and practice in constant dialogue.

If a proof appears without code or code appears without theory, something is missing.

---

## Exercises (Chapter 0)

**Exercise 0.1** (Reward Sensitivity) [15 minutes]

Modify `reward()` to use different weights in $R_{\text{toy}}$:
- (a) Pure GMV: $(1.0, 0.0, 0.0)$ (no profitability or engagement terms)
- (b) Profit-focused: $(0.4, 0.5, 0.1)$ (prioritize CM2 over GMV)
- (c) Engagement-heavy: $(0.5, 0.2, 0.3)$ (high click weight)

For each, train Q-learning and report:
- Final average reward
- Learned actions per user type
- Does the policy change? Why?

**Hint:** Case (c) risks "clickbait" strategies (see Chapter 1, Section 1.2.1). Monitor conversion quality.

---

**Exercise 0.2** (Action Geometry) [30 minutes]

Compare two exploration strategies:

**Strategy A (current):** $\varepsilon$-greedy with uniform random action sampling

**Strategy B (neighborhood):** $\varepsilon$-greedy with **local perturbation**: when exploring, sample action near current best $a^* = \arg\max_a Q(x, a)$:
```python
def explore_local(x, sigma=1.0):
    a_star = max(A, key=lambda a: Q[x][a])
    i_star, j_star = a_star
    i_new = np.clip(i_star + rng.integers(-1, 2), 0, 4)
    j_new = np.clip(j_star + rng.integers(-1, 2), 0, 4)
    return (i_new, j_new)
```

Implement both, train for 1000 episodes, and plot learning curves. Which converges faster? Why?

**Reflection:** This is **structured exploration**. Chapter 6 introduces UCB and Thompson Sampling, which balance exploration and exploitation more principled than $\varepsilon$-greedy.

---

**Exercise 0.3** (Regret Shape) [45 minutes, extended]

Define **cumulative regret** as the gap between oracle and agent:

$$
\text{Regret}(T) = \sum_{t=1}^{T} (R^*_t - R_t)
$$

where $R^*_t$ is the oracle reward (best action for context $x_t$) and $R_t$ is the agent's reward.

(a) Implement regret tracking:
```python
def compute_regret(history, contexts, oracle_Q):
    regret = []
    cumulative = 0.0
    for t, (x, r) in enumerate(zip(contexts, history)):
        r_star = oracle_Q[x]
        cumulative += (r_star - r)
        regret.append(cumulative)
    return regret
```

(b) Plot cumulative regret vs episode count. Is it sublinear (i.e., does $\text{Regret}(T) / T \to 0$)?

(c) Fit a curve: $\text{Regret}(T) \approx C \sqrt{T}$. Does this match theory? (Chapter 6 derives $O(\sqrt{T})$ regret for UCB.)

---

**Exercise 0.4** (Advanced: Constraints) [60 minutes, extended]

Add a simple CM2 floor constraint: reject actions that violate profitability.

**Setup:** Modify `reward()` to return `(r, cm2)`. Define a floor $\tau$ in the same units as CM2 (currency per episode); start with $\tau = 2.0$.

We note that production guardrails are often expressed as **margin-rate floors**, e.g. $\text{CM2} \ge 0.15 \cdot \text{GMV}$ (Chapter 10). In this toy exercise we use an **absolute CM2 floor** to keep the implementation minimal.

**Constrained Q-learning:**
```python
def choose_action_constrained(x, eps, tau_cm2):
    # Filter feasible actions
    feasible = [a for a in A if expected_cm2(x, a) >= tau_cm2]
    if not feasible:
        return A[rng.integers(len(A))]  # Fallback to unconstrained

    if rng.random() < eps:
        return feasible[rng.integers(len(feasible))]
    return max(feasible, key=lambda a: Q[x][a])
```

(a) Implement `expected_cm2(x, a)` (running average like Q).

(b) Train with $\tau = 2.0$. How does performance change vs unconstrained?

(c) Plot the Pareto frontier: GMV vs CM2 as $\tau$ varies over $\{0, 2, 4, 6, 8, 10, 12\}$ (or a dense grid in $[0, 12]$).

**Connection:** This is a **Constrained MDP (CMDP)**. Chapter 3 previews how CMDPs connect to Bellman operators (Remark 3.5.3), Appendix C develops the duality framework, and Chapter 10 implements production guardrails for multi-constraint optimization.

---

**Exercise 0.5** (Bandit-Bellman Bridge) [20 minutes, conceptual]

Our toy is a **contextual bandit**: single-step decisions, no sequential states.

The **Bellman equation** (Chapter 3) for an MDP is:

$$
V^*(s) = \max_a \left\{ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^*(s') \right\}
$$

where $\gamma \in [0, 1)$ is a discount factor.

**Question:** Show that our Q-learning update is the $\gamma = 0$ special case of Bellman.

**Hint:**
- Set $\gamma = 0$ in Bellman equation
- Note that with no future states, $V^*(s) = \max_a R(s, a)$
- Our Q-table is $Q(x, a) \approx \mathbb{E}[R \mid x, a]$, so $V^*(x) = \max_a Q(x, a)$
- This is **one-step value iteration**

**Reflection:** Contextual bandits are MDPs with horizon 1. Multi-episode search (Chapter 11) requires the full Bellman machinery.

---

## 0.9 Code Artifacts

All code from this chapter is available in the repository:

!!! note "Code-Artifact Mapping"
    - **Run script:** `scripts/ch00/toy_problem_solution.py:1` (use `--chapter0` to reproduce this chapter's output)
    - **Sanity tests:** `tests/ch00/test_toy_example.py:1` (deterministic regression for Chapter 0 output)
    - **Learning curve plot:** `docs/book/ch00/learning_curves.png:1` (generated artifact)

    To reproduce:
    ```bash
    uv run python scripts/ch00/toy_problem_solution.py --chapter0
    uv run pytest -q tests/ch00
    ```

    Expected output:
    ```
    Final average reward (last 100 episodes): 1.640
    Learned policy:
      price_hunter    -> action (4, 1) (Q = 1.948)
      premium         -> action (1, 4) (Q = 2.289)
      bulk_buyer      -> action (2, 2) (Q = 0.942)

    Saved learning curve to docs/book/ch00/learning_curves.png
    ```

---

## 0.10 What's Next?

We have now trained a first RL agent for search ranking. It learned context-adaptive boost weights from scratch, achieving near-oracle performance without manual tuning.

But we cheated. We used a tiny discrete action space, three user types, and online exploration without safety guarantees. Real systems need:

1. **Rigorous foundations** (Chapters 1--3): Formalize contextual bandits, measure-theoretic probability, Bellman operators
2. **Realistic simulation** (Chapters 4--5): Scalable catalog generation, position bias models, rich user dynamics
3. **Continuous actions** (Chapter 7): Regression-based Q-learning, CEM optimization, trust regions
4. **Constraints and guardrails** (Chapter 10): CM2 floors, $\Delta$Rank@k stability, safe fallback policies
5. **Safe evaluation** (Chapter 9): Off-policy evaluation (IPS, DR, FQE) for production deployment
6. **Multi-episode dynamics** (Chapter 11): Retention modeling, long-term value, engagement as state

**The journey from toy to production is the journey of this book.**

**In Chapter 1**, we formalize everything we hand-waved here: What exactly is a contextual bandit? Why is the reward function #EQ-1.2 mathematically sound? How do constraints become a CMDP? Why does engagement matter, and when should it be implicit vs explicit?

Let us make it rigorous.

---

*End of Chapter 0*


# Chapter 0 --- Exercises \& Labs (Application Mode)

We keep every experiment executable. These warm-ups extend the Chapter 0 toy environment and require us to compare learning curves against the analytical expectations stated in the draft.

## Lab 0.1 --- Tabular Boost Search (Toy World)

Goal: reproduce the $\geq 90\%$ of oracle guarantee using the public `scripts/ch00/toy_problem_solution.py`.

```python
from scripts.ch00.toy_problem_solution import (
    TabularQLearning,
    discretize_action_space,
    run_learning_experiment,
)

actions = discretize_action_space(n_bins=5, a_min=-1.0, a_max=1.0)
agent = TabularQLearning(
    actions,
    epsilon_init=0.9,
    epsilon_decay=0.995,
    epsilon_min=0.05,
    learning_rate=0.15,
)

results = run_learning_experiment(
    agent,
    n_train=800,
    eval_interval=40,
    n_eval=120,
    seed=314,
)
print(f"Final mean reward: {results['final_mean']:.2f} (target >= 0.90 * oracle)")
print(f"Per-user reward: {results['final_per_user']}")
```

Output (representative):
```
Final mean reward: 16.13 (target >= 0.90 * oracle)
Per-user reward: {'price_hunter': 14.62, 'premium': 22.65, 'bulk_buyer': 11.02}
```

**What to analyze**
1. Compare the printed percentages against the oracle baseline emitted by `scripts/ch00/toy_problem_solution.py`.
2. Highlight which segments remain under-optimized and tie that back to action-grid resolution (Section 0.3).
3. Export the figure `toy_problem_learning_curves.png` produced by the script and annotate regime changes (exploration vs exploitation) in the lab notes.

## Exercise 0.2 --- Stress-Testing Reward Weights

This exercise validates the sensitivity discussion in Section 0.3.2. Modify the toy reward to overweight engagement and measure how Q-learning reacts:

```python
from scripts.ch00.toy_problem_solution import (
    USER_TYPES,
    compute_reward,
    rank_products,
    simulate_user_interaction,
)
import numpy as np

alpha, beta, delta = 0.6, 0.3, 0.3  # delta intentionally oversized
rng = np.random.default_rng(7)
user = USER_TYPES["price_hunter"]
ranking = rank_products(0.8, 0.1)
interaction = simulate_user_interaction(user, ranking, seed=7)
reward = compute_reward(interaction, alpha=alpha, beta=beta, gamma=0.0, delta=delta)
print(f"Reward with delta={delta:.1f}: {reward:.2f}")
```

Output:
```
Reward with delta=0.3: 0.30
```

**Discussion prompts**
- Explain why the oversized $\delta$ inflates reward despite lower GMV, linking directly to the `delta/alpha` $\leq 0.10$ guideline in Chapter 1.
- Propose how the same guardrail can be encoded once we migrate to the full simulator (`zoosim/dynamics/reward.py` assertions already enforce it).

---

## Exercise 0.1 --- Reward Sensitivity Analysis

**Goal:** Compare learned policies under different reward configurations.

Three configurations to test:
- **(a) Pure GMV:** $(\alpha, \beta, \delta) = (1.0, 0.0, 0.0)$
- **(b) Profit-focused:** $(\alpha, \beta, \delta) = (0.4, 0.5, 0.1)$
- **(c) Engagement-heavy:** $(\alpha, \beta, \delta) = (0.5, 0.2, 0.3)$

```python
from scripts.ch00.lab_solutions import exercise_0_1_reward_sensitivity

results = exercise_0_1_reward_sensitivity(seed=42)
```

**What to analyze:**
1. Does the learned policy change across configurations? For which user types?
2. Which configuration shows the highest risk of "clickbait" behavior (high engagement, questionable quality)?
3. Connect the findings to the guardrail discussion in Chapter 1.

**Time estimate:** 20 minutes

---

## Exercise 0.2 --- Action Geometry and the Cold Start Problem

**Learning objective:** Understand how exploration strategy effectiveness depends on policy quality.

This is the pedagogical highlight of Chapter 0. We form a hypothesis, test it, discover it is wrong, diagnose why, and validate when the original intuition does hold.

### Setup

We compare two $\varepsilon$-greedy exploration strategies on the toy world:
- **Uniform exploration:** When exploring, sample ANY action uniformly from the 25-action grid
- **Local exploration:** When exploring, sample only NEIGHBORS ($\pm 1$ grid cell) of the current best action

### Part A --- Form Hypothesis (5 min)

Before running experiments, predict: *Which strategy converges faster?*

Write down the reasoning. The intuitive answer is "local exploration should be more efficient because it exploits structure near good solutions."

### Part B --- Cold Start Experiment (10 min)

Run both strategies from random initialization (Q=0 everywhere):

```python
from scripts.ch00.lab_solutions import exercise_0_2_action_geometry

results = exercise_0_2_action_geometry(
    n_episodes_cold=500,
    n_episodes_warmup=200,
    n_episodes_refine=300,
    n_runs=5,
    seed=42,
)
```

**Questions:**
1. Which strategy wins? By how much?
2. Does this match the hypothesis?

### Part C --- Diagnosis (10 min)

The code reports action coverage. Examine how many of the 25 actions each strategy explored.

**Questions:**
1. Why does local exploration explore fewer actions?
2. What does "cold start problem" mean in this context?
3. Why is the local agent doing "local refinement of garbage"?

### Part D --- Warm Start Experiment (10 min)

The code also runs a warm start experiment: first train with uniform for 200 episodes, then compare strategies for 300 more episodes.

**Questions:**
1. How does the gap between strategies change after warm start?
2. Why is local exploration now competitive?
3. When would local exploration actually *win*?

### Part E --- Synthesis (15 min)

Connect the findings to real RL algorithms:

1. **SAC** uses entropy regularization that naturally decays. How does this relate to the cold start problem?
2. **$\varepsilon$-greedy** schedules typically decay $\varepsilon$ from 0.9 $\rightarrow$ 0.05. Why?
3. **Optimistic initialization** (starting with high Q-values) is a common trick. How does it help with cold start?

**Deliverable:** Write a 1-paragraph guideline for choosing exploration strategies based on policy maturity.

**Time estimate:** 50 minutes total

---

## Exercise 0.3 --- Regret Analysis

**Goal:** Track cumulative regret and verify sublinear scaling.

```python
from scripts.ch00.lab_solutions import exercise_0_3_regret_analysis

results = exercise_0_3_regret_analysis(n_train=2000, seed=42)
```

**What to analyze:**
1. Is regret sublinear? (Does average regret per episode decrease?)
2. Fit the curve to $\text{Regret}(T) \approx C \cdot T^\alpha$. What is $\alpha$?
3. Compare to theory (stochastic finite-armed bandits): constant $\varepsilon$-greedy yields linear regret $\Theta(T)$. Decaying schedules can be sublinear (e.g., $\varepsilon_t \propto t^{-1/3}$ gives $O(T^{2/3})$ gap-independent bounds), while UCB/Thompson Sampling achieve $\tilde O(\sqrt{KT})$ worst-case regret.

**Time estimate:** 20 minutes

---

## Exercise 0.4 --- Constrained Q-Learning with CM2 Floor

**Goal:** Add profitability constraint $\mathbb{E}[\text{CM2} \mid x, a] \geq \tau$ and study the GMV-CM2 tradeoff.

In this toy exercise, we treat $\tau$ as an absolute CM2 floor per episode (currency units), not a margin-rate threshold.

```python
from scripts.ch00.lab_solutions import exercise_0_4_constrained_qlearning

results = exercise_0_4_constrained_qlearning(seed=42)
```

**What to analyze:**
1. Do we obtain a clean Pareto frontier? Why or why not?
2. What causes the high violation rates?
3. Propose an alternative approach (hint: Lagrangian relaxation, chance constraints)

**Connection:** This motivates the CMDP formalism (previewed in Remark 3.5.3 and developed in Appendix C) and its production instantiation as guardrails in Chapter 10.

**Time estimate:** 25 minutes

---

## Exercise 0.5 --- Bandit-Bellman Bridge (Conceptual)

**Goal:** Show that contextual bandit Q-learning is the $\gamma = 0$ case of MDP Q-learning.

```python
from scripts.ch00.lab_solutions import exercise_0_5_bandit_bellman_bridge

results = exercise_0_5_bandit_bellman_bridge()
```

**Theoretical derivation:**

1. Write the Bellman optimality equation for Q-values
2. Set $\gamma = 0$ and simplify
3. Show that the resulting update rule matches the bandit Q-update

**What to verify:**
1. Do the numerical tests pass?
2. What happens to the "bootstrap target" $r + \gamma \max_{a'} Q(s', a')$ when $\gamma = 0$?

**Connection to Chapter 11:** Multi-episode search requires $\gamma > 0$ because today's ranking affects tomorrow's return probability.

**Time estimate:** 15 minutes

---

## Summary: Exercise Time Budget

| Exercise | Time | Key Concept |
|----------|------|-------------|
| Lab 0.1 | 30 min | Q-learning on toy world |
| Ex 0.2 (stress) | 10 min | Reward weight sensitivity |
| Ex 0.1 | 20 min | Policy sensitivity to rewards |
| **Ex 0.2 (geometry)** | **50 min** | **Cold start problem** |
| Ex 0.3 | 20 min | Regret analysis |
| Ex 0.4 | 25 min | Constrained RL |
| Ex 0.5 | 15 min | Bandit-MDP connection |

**Total:** ~170 minutes (adjust based on depth of analysis)

---

## Running All Solutions

```bash
# Run all exercises
uv run python scripts/ch00/lab_solutions.py --all

# Run specific exercise
uv run python scripts/ch00/lab_solutions.py --exercise lab0.1
uv run python scripts/ch00/lab_solutions.py --exercise 0.2  # Action Geometry

# Interactive menu
uv run python scripts/ch00/lab_solutions.py
```


# Chapter 0 --- Lab Solutions

*Vlad Prytula*

These solutions demonstrate how theory meets practice in reinforcement learning. Every solution weaves mathematical analysis with runnable code, following the Application Mode principle: **mathematics and code in constant dialogue**.

All outputs shown are actual results from running the code with the specified seeds.

---

## Lab 0.1 --- Tabular Boost Search (Toy World)

**Goal:** Reproduce the $\geq 90\%$ of oracle guarantee using `scripts/ch00/toy_problem_solution.py`.

### Solution

```python
from scripts.ch00.toy_problem_solution import (
    TabularQLearning,
    discretize_action_space,
    run_learning_experiment,
    evaluate_policy,
    OraclePolicy,
)

# Configure experiment (parameters from exercises_labs.md)
actions = discretize_action_space(n_bins=5, a_min=-1.0, a_max=1.0)
print(f"Action space: {len(actions)} discrete templates (5x5 grid)")

agent = TabularQLearning(
    actions,
    epsilon_init=0.9,
    epsilon_decay=0.995,
    epsilon_min=0.05,
    learning_rate=0.15,
)

results = run_learning_experiment(
    agent,
    n_train=800,
    eval_interval=40,
    n_eval=120,
    seed=314,
)

# Compute oracle baseline
oracle = OraclePolicy(actions, n_eval=200, seed=314)
oracle_results = evaluate_policy(oracle, n_episodes=300, seed=314)
oracle_mean = oracle_results['mean_reward']

pct_oracle = 100 * results['final_mean'] / oracle_mean
print(f"Final mean reward: {results['final_mean']:.2f} (target >= 0.90 * oracle)")
print(f"Per-user reward: {results['final_per_user']}")
```

**Actual Output:**
```
Action space: 25 discrete templates (5x5 grid)
Oracle: Computing optimal actions via grid search...
  price_hunter: w*=(0.5, 0.0), Q*=17.80
  premium: w*=(-1.0, 1.0), Q*=19.02
  bulk_buyer: w*=(0.5, 1.0), Q*=12.88

Final mean reward: 16.13 (target >= 0.90 * oracle)
Per-user reward: {'price_hunter': 14.62, 'premium': 22.65, 'bulk_buyer': 11.02}

Oracle mean: 16.77
Percentage of oracle: 96.2%
Result: SUCCESS
```

### Analysis

**1. We achieve 96.2% of oracle performance---well above the 90% target.**

The Q-learning agent learns effective context-adaptive policies purely from interaction data.

**2. Per-User Performance Breakdown:**

| Segment | Q-Learning | Oracle | % of Optimal |
|---------|------------|--------|--------------|
| price_hunter | 14.62 | 17.80* | 82.1% |
| premium | 22.65 | 19.02* | 119.0%** |
| bulk_buyer | 11.02 | 12.88* | 85.6% |

*Oracle Q-values are per-action estimates; actual oracle mean across users is 16.77.

**Why does premium exceed oracle estimates?** The stochasticity in user interactions means different random seeds produce different outcomes. The agent found an action that happened to perform well on the evaluation seed.

**3. Learned Policy vs. Oracle Policy:**

| User Type | Learned Action | Oracle Action |
|-----------|---------------|---------------|
| price_hunter | (0.5, 0.5) | (0.5, 0.0) |
| premium | (-1.0, 0.0) | (-1.0, 1.0) |
| bulk_buyer | (-0.5, 0.5) | (0.5, 1.0) |

The learned actions differ from oracle because:
1. Q-table hasn't converged perfectly in 800 episodes
2. Stochastic rewards mean multiple actions have similar expected values
3. The $5 \times 5$ grid may not include the truly optimal continuous action

**Key Insight:** Even without matching the oracle's exact actions, Q-learning achieves near-oracle reward. This demonstrates the robustness of value-based learning.

---

## Exercise 0.2 (from exercises\_labs.md) --- Stress-Testing Reward Weights

**Goal:** Validate that oversized engagement weight $\delta$ inflates rewards despite unchanged GMV.

### Solution

```python
from scripts.ch00.toy_problem_solution import (
    USER_TYPES, compute_reward, rank_products, simulate_user_interaction,
)

# Exact parameters from exercises_labs.md
alpha, beta, delta = 0.6, 0.3, 0.3  # delta intentionally oversized
user = USER_TYPES["price_hunter"]
ranking = rank_products(0.8, 0.1)
interaction = simulate_user_interaction(user, ranking, seed=7)
reward = compute_reward(interaction, alpha=alpha, beta=beta, gamma=0.0, delta=delta)
print(f"Reward with delta={delta:.1f}: {reward:.2f}")
```

**Actual Output:**
```
Interaction: {'clicks': [5], 'purchases': [], 'n_clicks': 1, 'n_purchases': 0, 'gmv': 0.0, 'cm2': 0.0}
Reward with delta=0.3: 0.30
```

This matches the expected output in `exercises_labs.md`.

### Extended Analysis

Running 100 samples per user type with a "clickbait" ranking (high discount boost):

```
Standard weights (alpha=0.6, beta=0.3, delta=0.1):
  Ratio delta/alpha = 0.167
  price_hunter   : R=20.01, GMV=28.30, CM2=9.60, Clicks=1.56
  premium        : R=12.71, GMV=17.98, CM2=6.17, Clicks=0.76
  bulk_buyer     : R=11.35, GMV=16.19, CM2=5.12, Clicks=1.01

Oversized delta (alpha=0.6, beta=0.3, delta=0.3):
  Ratio delta/alpha = 0.500 (5x above guideline!)
  price_hunter   : R=20.32 (+1.6%), GMV=28.30, Clicks=1.56  <-- Same GMV, higher reward!
  premium        : R=12.86 (+1.2%), GMV=17.98, Clicks=0.76
  bulk_buyer     : R=11.55 (+1.8%), GMV=16.19, Clicks=1.01
```

**Key Observation:** With $\delta/\alpha = 0.50$, reward increases $\sim 1.5\%$ while GMV stays constant. The agent could learn to prioritize clicks over conversions---a form of engagement gaming.

**Guideline:** Keep $\delta/\alpha \leq 0.10$ to ensure GMV dominates the reward signal.

---

## Exercise 0.1 --- Reward Sensitivity Analysis

**Goal:** Compare learned policies under three reward configurations.

### Solution

```python
# Three configurations as specified in the exercise
configs = [
    ((1.0, 0.0, 0.0), "Pure GMV"),
    ((0.4, 0.5, 0.1), "Profit-focused"),
    ((0.5, 0.2, 0.3), "Engagement-heavy"),
]

# Run Q-learning for each configuration
for weights, label in configs:
    result = run_sensitivity_experiment(weights, label, n_train=1200, seed=42)
    print(f"{label} (alpha={weights[0]}, beta={weights[1]}, delta={weights[2]}):")
    print(f"  Learned policy: ...")
```

**Actual Output:**
```
Pure GMV (alpha=1.0, beta=0.0, delta=0.0):
  Final reward: 23.99
  Final GMV: 23.99
  Learned policy:
    price_hunter    -> w_discount=+1.0, w_quality=+0.0
    premium         -> w_discount=-1.0, w_quality=+1.0
    bulk_buyer      -> w_discount=+0.5, w_quality=+1.0

Profit-focused (alpha=0.4, beta=0.5, delta=0.1):
  Final reward: 14.03
  Final GMV: 24.24
  Learned policy:
    price_hunter    -> w_discount=+0.5, w_quality=+0.0
    premium         -> w_discount=-1.0, w_quality=+1.0
    bulk_buyer      -> w_discount=+0.5, w_quality=+1.0

Engagement-heavy (alpha=0.5, beta=0.2, delta=0.3):
  Final reward: 14.40
  Final GMV: 24.48
  Learned policy:
    price_hunter    -> w_discount=+1.0, w_quality=-1.0
    premium         -> w_discount=-1.0, w_quality=+1.0
    bulk_buyer      -> w_discount=+0.5, w_quality=+1.0
```

### Analysis

**Does the policy change?** Yes, for some user types:

| Configuration | price_hunter | premium | bulk_buyer |
|---------------|--------------|---------|------------|
| Pure GMV | $(+1.0, 0.0)$ | $(-1.0, +1.0)$ | $(+0.5, +1.0)$ |
| Profit-focused | $(+0.5, 0.0)$ | $(-1.0, +1.0)$ | $(+0.5, +1.0)$ |
| Engagement-heavy | $(+1.0, -1.0)$ | $(-1.0, +1.0)$ | $(+0.5, +1.0)$ |

**Key Observations:**

1. **premium users:** Policy is stable across all configurations at $(-1.0, +1.0)$---quality-heavy. This makes sense: premium users convert well on quality products regardless of reward weighting.

2. **price_hunter:** Shows the most variation:
   - Pure GMV: $(+1.0, 0.0)$ --- maximize discount, ignore quality
   - Profit-focused: $(+0.5, 0.0)$ --- moderate discount (high-margin products)
   - Engagement-heavy: $(+1.0, -1.0)$ --- extreme discount, actively penalize quality (clickbait risk!)

3. **bulk_buyer:** Stable at $(+0.5, +1.0)$---balanced approach works across configurations.

**The engagement-heavy case shows clickbait risk:** For price_hunter, the policy shifts to $(+1.0, -1.0)$, actively demoting quality products. This maximizes clicks but may hurt long-term user satisfaction.

---

## Exercise 0.2 --- Action Geometry and the Cold Start Problem

**Goal:** Understand how exploration strategy effectiveness depends on policy quality.

This exercise teaches a fundamental insight through a structured investigation. We start with a hypothesis, test it empirically, discover it's wrong, diagnose why, and then design an experiment that validates when the original intuition *does* hold.

---

### Part A --- The Hypothesis

Intuition suggests that **local exploration**---small perturbations around our current best action---should be more efficient than **uniform random sampling**. After all, once we find a good region of action space, why waste samples exploring far away?

```python
# Two exploration strategies:
# - Uniform: When exploring (with prob epsilon), sample ANY action uniformly
# - Local:   When exploring (with prob epsilon), sample NEIGHBORS of current best (+/-1 grid cell)
```

**Hypothesis:** Local exploration converges faster because it exploits structure near good solutions (gradient descent intuition).

Let's test this.

---

### Part B --- Cold Start Experiment

We train both agents from scratch (Q=0 everywhere) for 500 episodes.

```python
from scripts.ch00.lab_solutions import exercise_0_2_action_geometry

results = exercise_0_2_action_geometry(
    n_episodes_cold=500,
    n_episodes_warmup=200,
    n_episodes_refine=300,
    n_runs=5,
    seed=42,
)
```

**Actual Output (Cold Start):**
```
Starting BOTH agents from random initialization (Q=0 everywhere).
Training each for 500 episodes...

Results (averaged over 5 runs):
  Strategy       Final Reward        Std
  ------------ -------------- ----------
  Uniform               15.66      18.48
  Local                 10.33      15.43

  Winner: Uniform (by 34.0%)

  ** SURPRISE! Uniform exploration wins decisively.
      Our hypothesis was WRONG. But why?
```

---

### Part C --- Diagnosis: The Cold Start Problem

**Why does local exploration fail from cold start?**

The problem is **initialization**. With Q=0 everywhere:
- **Uniform agent**: Explores the ENTIRE action space randomly
- **Local agent**: Starts at action index 0 (the corner: $w=(-1,-1)$) and only explores NEIGHBORS of that corner!

The local agent is doing **"local refinement of garbage"**---there's no good region nearby to refine. It's stuck in a bad neighborhood.

**Action Coverage After 200 Episodes:**
```
  Uniform explored 18/25 actions (72%)
  Local explored   4/25 actions (16%)

  Local agent never discovered the optimal region!
```

This is the **COLD START PROBLEM**:
> Local exploration assumes we are already in a good basin.
> From random initialization, we are not.

---

### Part D --- Warm Start Experiment

If local exploration fails from cold start, when SHOULD it work?

**Answer:** After we've found a good region via global exploration!

**Experiment Design:**
1. Train with UNIFORM for 200 episodes (find good region)
2. Then continue training with each strategy for 300 more episodes
3. Compare which strategy refines better from this warm start

**Actual Output (Warm Start):**
```
Results (averaged over 5 runs, after 200 warmup episodes):
  Strategy       Final Reward        Std
  ------------ -------------- ----------
  Uniform               14.04      16.76
  Local                 14.58      17.10

  Winner: Local (by 3.7%)

  Local exploration is now COMPETITIVE (or wins)!
    Once we're in a good basin, local refinement works.
```

**Key Observation:** The gap between uniform and local *reverses* when starting from a warm policy. From cold start, uniform wins by 34%. From warm start, local actually *wins* by 3.7%! Local exploration works---and even excels---*once we are already in a good region*.

---

### Part E --- Synthesis: Adaptive Exploration

The key insight: **EXPLORATION STRATEGY SHOULD ADAPT TO POLICY MATURITY.**

| Training Phase | Recommended Strategy | Rationale |
|----------------|---------------------|-----------|
| Early (cold) | Uniform/global | Find good regions across action space |
| Late (warm) | Local/refined | Exploit structure within good regions |

**This is exactly what sophisticated algorithms implement:**

| Algorithm | Mechanism | Effect |
|-----------|-----------|--------|
| **SAC** | Entropy bonus $\alpha \cdot H(\pi)$ | Encourages broad exploration; decays naturally as policy sharpens |
| **PPO** | Decaying entropy coefficient | High entropy early (explore) $\rightarrow$ low late (exploit) |
| **$\varepsilon$-greedy** | $\varepsilon$ decays $(0.9 \rightarrow 0.05)$ | Global early, local late |
| **Boltzmann** | Temperature $\tau$ decays | High $\tau$ = uniform, low $\tau$ = local around best |

**Connection to Theory:**

The cold start problem explains why **optimistic initialization** (starting with high Q-values) helps---it forces global exploration before settling into local refinement. Starting with $Q=\infty$ everywhere means the agent must try everything before any action looks "best."

---

### Summary Table

| Experiment | Uniform | Local | Winner | Gap |
|------------|---------|-------|--------|-----|
| Cold start | 15.66 | 10.33 | Uniform | 34.0% |
| Warm start | 14.04 | 14.58 | **Local** | 3.7% |

**The same exploration strategy can WIN or LOSE depending on whether the policy is cold (random) or warm (trained).** In fact, the winner *reverses*: uniform dominates cold start, but local wins after warm-up!

This is not a bug---it's a fundamental insight about RL exploration.

---

### Practical Guideline

When designing exploration strategies, ask:

> "Is my policy already in a good region?"

- **If no** $\rightarrow$ Use global/uniform exploration first
- **If yes** $\rightarrow$ Local refinement is efficient

This principle applies beyond toy examples. In production RL:
- **Curriculum learning** starts with easier tasks (warm start for harder ones)
- **Transfer learning** initializes from pre-trained policies (warm start)
- **Reward shaping** guides early exploration toward good regions

---

## Exercise 0.3 --- Regret Analysis

**Goal:** Track cumulative regret and understand what it tells us about learning.

### Background: What Is Regret?

**Cumulative regret** measures total performance loss compared to an oracle:
$$\text{Regret}(T) = \sum_{t=1}^{T} \left( R^*_t - R_t \right)$$
where $R^*_t$ is the oracle's reward and $R_t$ is the agent's reward at episode $t$.

**Sublinear regret** means $\text{Regret}(T) = o(T)$, i.e., average regret per episode vanishes:
$$\frac{\text{Regret}(T)}{T} \to 0 \quad \text{as } T \to \infty$$

This confirms the agent is *learning*---eventually performing as well as the oracle.

### Solution

```python
# Cumulative regret: Regret(T) = Sum_{t=1}^T (R*_t - R_t)
# where R*_t is oracle reward and R_t is agent reward

# Run 2000 episodes with exponential decay + floor:
#   eps_t = max(eps_min, eps_0 * 0.998^t)
```

**Actual Output:**
```
======================================================================
Exercise 0.3: Regret Analysis
======================================================================

Computing oracle policy...
Oracle: Computing optimal actions via grid search...
  price_hunter: w*=(1.0, -0.5), Q*=17.46
  premium: w*=(-0.5, 1.0), Q*=19.22
  bulk_buyer: w*=(0.5, 1.0), Q*=15.73
Running regret experiment...

Regret Analysis Summary:
  Total episodes: 2000
  Final cumulative regret: 5682.1
  Average regret per episode: 2.841
  Regret growth slowing? True (avg regret: 7.210 early -> 2.841 late)

Empirical curve fitting (for illustration only):
  sqrt(T) model: Regret(T) ~ 127.1 * sqrt(T)
  Power model: Regret(T) ~ 79.1 * T^0.57

WARNING: Don't conflate empirical fits with asymptotic bounds!
  The exponent alpha=0.57 describes the learning TRANSIENT,
  not a fundamental asymptotic rate.

Theoretical expectations (for reference):
  Constant epsilon-greedy:              Theta(T) -- linear regret
  Epsilon-greedy with epsilon_min>0:    Theta(T) -- still linear (never stops exploring)
  Decaying epsilon (GLIE):              sublinear regret is possible (e.g., O(T^(2/3)) gap-independent)
  UCB / Thompson Sampling:             ~O(sqrt(KT log T)) worst-case (and O(log T) gap-dependent)

Our schedule uses exponential decay down to epsilon_min=0.020,
so asymptotically it behaves like constant-epsilon and has linear regret in theory.
The T^0.57 fit captures the transient, not the asymptote.
```

### Interpretation

**Does this run demonstrate no-regret?** Not by itself. The average regret per episode decreases over this window, which indicates learning, but with a nonzero exploration floor we should expect linear regret asymptotically.

### Theory-Practice Gap: Why Curve Fitting Is Misleading

**Caution:** Fitting a power law to 2000 points and claiming "regret scales as $O(T^{0.62})$" conflates two different things:

| Concept | What It Means |
|---------|---------------|
| **Empirical fit** | Regret $\approx c \cdot T^\alpha$ for *observed* data |
| **Asymptotic bound** | $\lim_{T\to\infty} \text{Regret}(T)/T^\alpha < \infty$ |

The empirical exponent $\alpha = 0.62$ could drift as $T \to \infty$. With finite samples, one can fit almost any functional form.

### What Should We Actually Expect?

Our implementation uses an exponential decay with an exploration floor:
$$
\varepsilon_t = \max(\varepsilon_{\min}, \varepsilon_0 \lambda^t).
$$
With $\varepsilon_{\min} > 0$, exploration never fully shuts off. In the classic stochastic bandit setting this implies **linear regret**: a constant fraction of rounds are intentionally random, hence suboptimal on average. We use this schedule in Chapter 0 for pragmatic stability in short runs, not for asymptotic optimality.

If we set $\varepsilon_{\min}=0$ (pure geometric decay), then $\sum_t \varepsilon_t < \infty$, but this still does **not** imply bounded regret: with nonzero probability the algorithm stops exploring before it has identified the optimal action and then exploits a suboptimal arm forever. For asymptotic guarantees one typically assumes a GLIE condition ($\varepsilon_t \to 0$ but $\sum_t \varepsilon_t = \infty$) or uses confidence-based exploration (UCB/Thompson Sampling).

| Exploration schedule | Typical guarantee (stochastic bandits) | Notes |
|---------------------|------------------------------------------|-------|
| Constant $\varepsilon$ | $\Theta(T)$ | Linear: explores forever |
| $\varepsilon_t = \max(\varepsilon_{\min}, \varepsilon_0 \lambda^t)$ with $\varepsilon_{\min}>0$ | $\Theta(T)$ | Asymptotically constant-$\varepsilon$ |
| GLIE schedules (e.g., $\varepsilon_t \propto t^{-1/3}$) | $O(T^{2/3})$ (gap-independent, example) | Requires $\varepsilon_t \to 0$ and $\sum_t \varepsilon_t = \infty$ |
| UCB / Thompson Sampling | $\tilde O(\sqrt{KT})$ (worst-case); $O(\log T)$ (gap-dependent) | Confidence-/uncertainty-driven |

**Common misconception:** "Constant $\varepsilon$-greedy gives $O(T^{2/3})$." This is **wrong**. Constant $\varepsilon$ gives *linear* regret $\Omega(\varepsilon T)$ because exploration continues forever. The classical $O(T^{2/3})$ bound corresponds to specific *decaying* schedules (e.g., $\varepsilon_t \propto t^{-1/3}$) or Explore-Then-Commit variants.

### What the Empirical Fit Actually Shows

The $T^{0.62}$ fit over 2000 episodes captures the **transient learning phase**:

1. **Early** ($t < 200$): High $\varepsilon$ $\rightarrow$ lots of exploration $\rightarrow$ high per-episode regret
2. **Middle** ($200 < t < 1000$): Q-values converging $\rightarrow$ regret growth slows
3. **Late** ($t > 1000$): $\varepsilon_t$ has decayed close to its floor $\varepsilon_{\min}$, so exploitation dominates but exploration never vanishes

The power-law fit interpolates this transition but doesn't reflect any fundamental asymptotic rate.

### The Honest Conclusion

**What we can say:**
- Regret growth slows over time $\rightarrow$ the agent is learning
- Average regret per episode decreases $\rightarrow$ converging toward oracle performance
- With a nonzero exploration floor, regret is still linear asymptotically (but with a much smaller slope than at the start)

**What we should NOT say:**
- "Regret scales as $O(T^{0.62})$" --- this conflates empirical fits with asymptotic bounds
- Comparisons to UCB/theoretical bounds without matching assumptions

**The key insight:** Curve-fitting describes transients. The asymptotic regret regime is determined by the exploration strategy (constant-$\varepsilon$ behavior yields $\Theta(T)$; confidence-based methods yield $\tilde O(\sqrt{T})$ under standard assumptions).

---

## Exercise 0.4 --- Constrained Q-Learning with CM2 Floor

**Goal:** Add profitability constraint $\mathbb{E}[\text{CM2} \mid x, a] \geq \tau$ and study the GMV--CM2 tradeoff.

### Solution

```python
class ConstrainedQLearning:
    """Q-learning with CM2 floor constraint.

    Maintains separate estimates for Q(x,a) (reward) and CM2(x,a) (contribution margin, currency units).
    Filters actions based on estimated CM2 feasibility.
    """
    def get_feasible_actions(self, user_name):
        return [a for a in range(n_actions) if self.CM2[(user_name, a)] >= self.tau]
```

**Actual Output:**
```
=== Exercise 0.4: Constrained Q-Learning ===

Pareto Frontier (GMV vs CM2):
--------------------------------------------------
tau= 0.0: GMV=23.73, CM2= 7.98, Violations=  0%
tau= 2.0: GMV=23.16, CM2= 8.02, Violations= 50%
tau= 4.0: GMV=22.50, CM2= 7.94, Violations= 50%
tau= 6.0: GMV=21.41, CM2= 6.49, Violations= 72%
tau= 8.0: GMV=23.38, CM2= 8.19, Violations= 58%
tau=10.0: GMV=23.03, CM2= 7.23, Violations= 72%
tau=12.0: GMV=20.17, CM2= 6.93, Violations= 66%

Analysis:
  Unconstrained GMV: 23.73
  Unconstrained CM2: 7.98
  Best CM2 at tau=8.0: CM2=8.19, GMV=23.38
```

### Theory-Practice Gap: Per-Episode Constraints Are Hard!

**The results don't show a clean Pareto frontier.** Why?

1. **High CM2 variance:** CM2 is 0 when no purchase occurs (common!), and can be $30+$ when a high-margin product sells. Per-episode CM2 is extremely noisy.

2. **Constraint satisfaction is probabilistic:** Even if $\mathbb{E}[\text{CM2} \mid x, a] \geq \tau$, individual episodes often violate the constraint due to variance.

3. **Optimistic initialization:** We initialize CM2 estimates at $10.0$ (optimistic). As estimates converge to true values, many actions become infeasible, leading to policy instability.

**Better Approaches (CMDP preview + duality):** See Remark 3.5.3 for the Bellman-operator view of Lagrangian relaxation, Appendix C for the full CMDP/duality framework, and Chapter 10 for production guardrails.

1. **Lagrangian relaxation:** Instead of hard constraints, penalize violations:
   $$\max_\pi \mathbb{E}[R] - \lambda(\tau - \mathbb{E}[\text{CM2}])$$

2. **Chance constraints:** Require $P(\text{CM2} \geq \tau) \geq 1 - \delta_c$ instead of expected value.

3. **Batch constraints:** Aggregate over episodes/users, not per-episode.

**Key Insight:** Single-episode CMDP constraints with high-variance outcomes require sophisticated handling. The simple primal feasibility approach shown here is educational but not production-ready.

---

## Exercise 0.5 --- Bandit-Bellman Bridge (Conceptual)

**Goal:** Show that contextual bandit Q-learning is the $\gamma = 0$ case of MDP Q-learning.

### Solution

**The Bellman optimality equation:**
$$V^*(s) = \max_a \left\{ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^*(s') \right\}$$

**Setting $\gamma = 0$:**
$$V^*(s) = \max_a R(s, a)$$

The future value term vanishes! The Q-function becomes:
$$Q^*(s, a) = R(s, a) = \mathbb{E}[\text{reward} \mid s, a]$$

This is exactly what our bandit Q-table estimates.

### Numerical Verification

```python
def bandit_update(Q, r, alpha):
    """Bandit: Q <- (1-alpha)Q + alpha*r"""
    return (1 - alpha) * Q + alpha * r

def mdp_update(Q, r, Q_next_max, alpha, gamma):
    """MDP: Q <- Q + alpha[r + gamma*max(Q') - Q]"""
    td_target = r + gamma * Q_next_max
    return Q + alpha * (td_target - Q)
```

**Actual Output:**
```
Test 1:
  Initial Q: 5.0, Reward: 7.0, alpha: 0.1
  Bandit update: 5.200000
  MDP update (gamma=0): 5.200000
  Difference: 0.00e+00
  PASSED

Test 2:
  Initial Q: 0.0, Reward: 10.0, alpha: 0.5
  Bandit update: 5.000000
  MDP update (gamma=0): 5.000000
  Difference: 0.00e+00
  PASSED

Test 3:
  Initial Q: -3.0, Reward: 2.0, alpha: 0.2
  Bandit update: -2.000000
  MDP update (gamma=0): -2.000000
  Difference: 4.44e-16  <-- Floating point precision
  PASSED

Test 4:
  Initial Q: 100.0, Reward: 50.0, alpha: 0.01
  Bandit update: 99.500000
  MDP update (gamma=0): 99.500000
  Difference: 0.00e+00
  PASSED

Verified: Bandit Q-update = MDP Q-update with gamma=0
```

### Implications

| Property | Contextual Bandit | Full MDP |
|----------|-------------------|----------|
| Horizon | 1 step | $T$ steps (or infinite) |
| State transitions | None | $s \rightarrow s'$ via $P(s' \mid s, a)$ |
| Update target | $r$ | $r + \gamma \max_{a'} Q(s', a')$ |
| Convergence | Stochastic approximation | Bellman contraction |

**For Chapter 11 (multi-episode search):** Today's ranking affects tomorrow's return probability. This requires $\gamma > 0$ and the full Bellman machinery.

---

## Summary: Theory--Practice Insights

These labs revealed important insights about RL in practice:

| Exercise | Key Discovery | Lesson |
|----------|--------------|--------|
| Lab 0.1 | 96.2% of oracle achieved | Q-learning works for small discrete action spaces |
| Ex 0.1 | Policy varies with reward weights | Engagement-heavy configs risk clickbait |
| **Ex 0.2** | **Cold start problem discovered** | **Exploration strategy must match policy maturity** |
| Ex 0.3 | Regret growth slows over time | Sublinear regret confirms learning; don't conflate empirical fits with $O(\cdot)$ bounds |
| Ex 0.4 | No clean Pareto frontier | Per-episode constraints need Lagrangian methods |
| Ex 0.5 | Bandit = MDP with $\gamma=0$ | Unified view of bandits and MDPs |

**Key Lessons:**

1. **Q-learning works well** for small discrete action spaces with clear structure
2. **Exploration strategy depends on context**:
   - Cold start $\rightarrow$ uniform/global exploration
   - Warm start $\rightarrow$ local refinement is competitive
   - This explains why $\varepsilon$-greedy decays, SAC uses entropy, etc.
3. **Per-episode constraints** with high-variance outcomes need careful handling (Lagrangian methods)
4. **Bandits are $\gamma=0$ MDPs**---understanding this connection is foundational for Chapter 11

**The Cold Start Problem (Ex 0.2) is the pedagogical highlight:** We started with a hypothesis (local exploration is more efficient), discovered it was wrong, diagnosed why (cold start), and then validated when the intuition *does* hold (warm start). This is honest empiricism in action.

---

## Running the Code

All solutions are in `scripts/ch00/lab_solutions.py`:

```bash
# Run all exercises
python scripts/ch00/lab_solutions.py --all

# Run specific exercise
python scripts/ch00/lab_solutions.py --exercise lab0.1
python scripts/ch00/lab_solutions.py --exercise 0.3

# Interactive menu
python scripts/ch00/lab_solutions.py
```

---

*End of Lab Solutions*


