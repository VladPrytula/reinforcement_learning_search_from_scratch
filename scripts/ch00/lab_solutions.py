"""
Chapter 0 Lab Solutions ‚Äî Complete Runnable Code

Author: Vlad Prytula

This module implements all lab exercises from Chapter 0, demonstrating the
seamless integration of mathematical theory and production-quality code.

Solutions included:
- Lab 0.1: Tabular Boost Search (reproduce ‚â•90% oracle)
- Exercise 0.2 (from exercises_labs.md): Stress-Testing Reward Weights
- Exercise 0.1: Reward Sensitivity Analysis
- Exercise 0.2: Action Geometry (Exploration Strategies)
- Exercise 0.3: Regret Analysis and Curve Fitting
- Exercise 0.4: Constrained Q-Learning with CM2 Floor
- Exercise 0.5: Bandit-Bellman Bridge Verification

Usage:
    python scripts/ch00/lab_solutions.py [--exercise N] [--all]

    --exercise N: Run only exercise N (e.g., 'lab0.1', '0.1', '0.3')
    --all: Run all exercises sequentially
    (default): Run interactive menu
"""

from __future__ import annotations

import argparse
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any

import numpy as np

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.ch00.toy_problem_solution import (
    PRODUCTS,
    USER_TYPES,
    OraclePolicy,
    TabularQLearning,
    compute_reward,
    discretize_action_space,
    evaluate_policy,
    rank_products,
    run_learning_experiment,
    simulate_user_interaction,
)


# =============================================================================
# Lab 0.1: Tabular Boost Search
# =============================================================================


def lab_0_1_tabular_boost_search(seed: int = 314, verbose: bool = True) -> dict:
    """
    Lab 0.1 Solution: Tabular Boost Search

    Reproduces the ‚â•90% oracle guarantee using Q-learning on the toy world.
    Uses exact parameters from exercises_labs.md.

    Returns:
        Dict with final_mean, per_user, oracle_mean, pct_oracle
    """
    if verbose:
        print("=" * 70)
        print("Lab 0.1: Tabular Boost Search (Toy World)")
        print("=" * 70)

    # Configure experiment (exact params from exercises_labs.md)
    actions = discretize_action_space(n_bins=5, a_min=-1.0, a_max=1.0)
    if verbose:
        print(f"Action space: {len(actions)} discrete templates (5√ó5 grid)")

    agent = TabularQLearning(
        actions,
        epsilon_init=0.9,
        epsilon_decay=0.995,
        epsilon_min=0.05,
        learning_rate=0.15,
    )

    results = run_learning_experiment(
        agent,
        n_train=800,
        eval_interval=40,
        n_eval=120,
        seed=seed,
    )

    # Compute oracle baseline
    oracle = OraclePolicy(actions, n_eval=200, seed=seed)
    oracle_results = evaluate_policy(oracle, n_episodes=300, seed=seed)
    oracle_mean = oracle_results["mean_reward"]

    pct_oracle = 100 * results["final_mean"] / oracle_mean

    if verbose:
        print(f"\nFinal mean reward: {results['final_mean']:.2f}")
        print(f"Per-user reward: {results['final_per_user']}")
        print(f"\nOracle mean: {oracle_mean:.2f}")
        print(f"Percentage of oracle: {pct_oracle:.1f}%")

        if pct_oracle >= 90:
            print(f"\n‚úì SUCCESS: Q-learning achieves ‚â•90% of oracle!")
        else:
            print(f"\n‚úó Below target: {pct_oracle:.1f}% < 90%")

        # Show learned policy
        print("\nLearned policy:")
        for user_name in USER_TYPES:
            best_idx = int(np.argmax([agent.Q[(user_name, i)] for i in range(len(actions))]))
            best_action = actions[best_idx]
            print(f"  {user_name:15s}: ({float(best_action[0]):+.1f}, {float(best_action[1]):+.1f})")

    return {
        "final_mean": results["final_mean"],
        "final_per_user": results["final_per_user"],
        "oracle_mean": oracle_mean,
        "pct_oracle": pct_oracle,
    }


# =============================================================================
# Exercise 0.2 (exercises_labs.md): Stress-Testing Reward Weights
# =============================================================================


def exercise_stress_test_reward_weights(seed: int = 7, verbose: bool = True) -> dict:
    """
    Exercise 0.2 (from exercises_labs.md): Stress-Testing Reward Weights

    Validates that oversized Œ¥ inflates reward despite unchanged GMV.
    """
    if verbose:
        print("=" * 70)
        print("Exercise 0.2 (exercises_labs.md): Stress-Testing Reward Weights")
        print("=" * 70)

    # Single interaction example (exact params from exercises_labs.md)
    alpha, beta, delta = 0.6, 0.3, 0.3
    user = USER_TYPES["price_hunter"]
    ranking = rank_products(0.8, 0.1)
    interaction = simulate_user_interaction(user, ranking, seed=seed)
    reward = compute_reward(interaction, alpha=alpha, beta=beta, gamma=0.0, delta=delta)

    if verbose:
        print(f"\nSingle Interaction (seed={seed}):")
        print(f"  Interaction: {interaction}")
        print(f"  Reward with delta={delta:.1f}: {reward:.2f}")

    # Extended analysis with multiple samples
    def analyze_config(alpha: float, beta: float, delta: float, n_samples: int = 100):
        results = {}
        for user_name, user_type in USER_TYPES.items():
            rewards, gmv_total, cm2_total, clicks_total = [], 0.0, 0.0, 0
            for i in range(n_samples):
                ranking = rank_products(w_discount=0.8, w_quality=0.1)
                interaction = simulate_user_interaction(
                    user_type, ranking, seed=seed + i + hash(user_name) % 10000
                )
                rewards.append(compute_reward(interaction, alpha, beta, 0.0, delta))
                gmv_total += interaction["gmv"]
                cm2_total += interaction["cm2"]
                clicks_total += interaction["n_clicks"]
            results[user_name] = {
                "mean_reward": np.mean(rewards),
                "mean_gmv": gmv_total / n_samples,
                "mean_cm2": cm2_total / n_samples,
                "mean_clicks": clicks_total / n_samples,
            }
        return results

    standard = analyze_config(0.6, 0.3, 0.1)
    oversized = analyze_config(0.6, 0.3, 0.3)

    if verbose:
        print(f"\nStandard weights (Œ±=0.6, Œ≤=0.3, Œ¥=0.1):")
        print(f"  Ratio Œ¥/Œ± = {0.1/0.6:.3f}")
        for user, data in standard.items():
            print(f"  {user:15s}: R={data['mean_reward']:.2f}, GMV={data['mean_gmv']:.2f}, "
                  f"CM2={data['mean_cm2']:.2f}, Clicks={data['mean_clicks']:.2f}")

        print(f"\nOversized Œ¥ (Œ±=0.6, Œ≤=0.3, Œ¥=0.3):")
        print(f"  Ratio Œ¥/Œ± = {0.3/0.6:.3f} (5√ó above guideline!)")
        for user, data in oversized.items():
            std_r = standard[user]["mean_reward"]
            pct = 100 * (data["mean_reward"] - std_r) / std_r if std_r else 0
            print(f"  {user:15s}: R={data['mean_reward']:.2f} ({pct:+.1f}%), "
                  f"GMV={data['mean_gmv']:.2f}, Clicks={data['mean_clicks']:.2f}")

    return {"single_reward": reward, "standard": standard, "oversized": oversized}


# =============================================================================
# Exercise 0.1: Reward Sensitivity Analysis
# =============================================================================


def exercise_0_1_reward_sensitivity(seed: int = 42, verbose: bool = True) -> dict:
    """
    Exercise 0.1: Reward Sensitivity Analysis

    Three reward configurations:
    (a) Pure GMV: (1.0, 0.0, 0.0)
    (b) Profit-focused: (0.4, 0.5, 0.1)
    (c) Engagement-heavy: (0.5, 0.2, 0.3)
    """
    if verbose:
        print("=" * 70)
        print("Exercise 0.1: Reward Sensitivity Analysis")
        print("=" * 70)

    def compute_reward_custom(interaction: dict, weights: tuple) -> float:
        alpha, beta, delta = weights
        return (alpha * interaction["gmv"] + beta * interaction["cm2"] +
                delta * interaction["n_clicks"])

    def run_experiment(weights: tuple, label: str, n_train: int = 1200):
        rng = np.random.default_rng(seed)
        actions = discretize_action_space(n_bins=5)
        agent = TabularQLearning(actions, epsilon_init=0.9, epsilon_decay=0.995,
                                  epsilon_min=0.05, learning_rate=0.12)
        rewards_hist, gmv_hist = [], []

        for ep in range(n_train):
            user_name = rng.choice(list(USER_TYPES.keys()))
            action = agent.select_action(user_name, rng)
            ranking = rank_products(action[0], action[1])
            interaction = simulate_user_interaction(USER_TYPES[user_name], ranking, seed=seed+ep)
            reward = compute_reward_custom(interaction, weights)
            agent.update(user_name, action, reward)
            rewards_hist.append(reward)
            gmv_hist.append(interaction["gmv"])

        learned_policy = {}
        for user_name in USER_TYPES:
            best_idx = int(np.argmax([agent.Q[(user_name, i)] for i in range(len(actions))]))
            learned_policy[user_name] = actions[best_idx]

        return {
            "label": label, "weights": weights, "learned_policy": learned_policy,
            "final_reward": np.mean(rewards_hist[-100:]),
            "final_gmv": np.mean(gmv_hist[-100:]),
        }

    configs = [
        ((1.0, 0.0, 0.0), "Pure GMV"),
        ((0.4, 0.5, 0.1), "Profit-focused"),
        ((0.5, 0.2, 0.3), "Engagement-heavy"),
    ]

    results = {}
    for weights, label in configs:
        result = run_experiment(weights, label)
        results[label] = result
        if verbose:
            print(f"\n{label} (Œ±={weights[0]}, Œ≤={weights[1]}, Œ¥={weights[2]}):")
            print(f"  Final reward: {result['final_reward']:.2f}")
            print(f"  Final GMV: {result['final_gmv']:.2f}")
            print("  Learned policy:")
            for user, action in result["learned_policy"].items():
                print(f"    {user:15s} ‚Üí w_discount={float(action[0]):+.1f}, "
                      f"w_quality={float(action[1]):+.1f}")

    return results


# =============================================================================
# Exercise 0.2: Action Geometry and the Cold Start Problem
# =============================================================================


def exercise_0_2_action_geometry(
    n_episodes_cold: int = 500,
    n_episodes_warmup: int = 200,
    n_episodes_refine: int = 300,
    n_runs: int = 5,
    seed: int = 42,
    verbose: bool = True,
) -> dict:
    """
    Exercise 0.2: Action Geometry and the Cold Start Problem.

    This exercise teaches a fundamental insight: exploration strategy effectiveness
    depends on policy quality. We discover this through a structured investigation:

    Part A: Form hypothesis (naive intuition: local exploration is more efficient)
    Part B: Cold start experiment (surprise! uniform wins)
    Part C: Diagnosis (cold start problem explained)
    Part D: Warm start experiment (now local wins/is competitive)
    Part E: Synthesis (adaptive exploration, connection to entropy regularization)

    This transforms what could be "our hypothesis was wrong" into a deliberate
    pedagogical journey about phase-dependent exploration strategies.
    """
    if verbose:
        print("=" * 70)
        print("Exercise 0.2: Action Geometry and the Cold Start Problem")
        print("=" * 70)

    actions = discretize_action_space(n_bins=5)
    n_actions = len(actions)
    n_bins = 5

    # -------------------------------------------------------------------------
    # Agent Definitions
    # -------------------------------------------------------------------------

    class UniformExplorationAgent:
        """Œµ-greedy agent with uniform random exploration."""

        def __init__(self):
            self.Q: dict = defaultdict(float)
            self.counts: dict = defaultdict(int)

        def select_action(self, user: str, rng, eps: float = 0.15) -> int:
            if rng.random() < eps:
                return int(rng.integers(n_actions))  # Uniform over ALL actions
            return int(np.argmax([self.Q[(user, a)] for a in range(n_actions)]))

        def update(self, user: str, idx: int, r: float, lr: float = 0.12):
            self.Q[(user, idx)] += lr * (r - self.Q[(user, idx)])
            self.counts[(user, idx)] += 1

        def get_best_actions(self) -> dict:
            """Return best action index for each user type."""
            return {
                user: int(np.argmax([self.Q[(user, a)] for a in range(n_actions)]))
                for user in USER_TYPES
            }

        def copy_from(self, other: "UniformExplorationAgent"):
            """Copy Q-values and counts from another agent (for warm start)."""
            self.Q = defaultdict(float, other.Q)
            self.counts = defaultdict(int, other.counts)

    class LocalExplorationAgent:
        """Œµ-greedy agent with LOCAL perturbation exploration (neighbors only)."""

        def __init__(self):
            self.Q: dict = defaultdict(float)
            self.counts: dict = defaultdict(int)

        def select_action(self, user: str, rng, eps: float = 0.15) -> int:
            best = int(np.argmax([self.Q[(user, a)] for a in range(n_actions)]))
            if rng.random() < eps:
                # Explore LOCALLY: perturb within ¬±1 grid cell
                i, j = best // n_bins, best % n_bins
                i = int(np.clip(i + rng.integers(-1, 2), 0, n_bins - 1))
                j = int(np.clip(j + rng.integers(-1, 2), 0, n_bins - 1))
                return i * n_bins + j
            return best

        def update(self, user: str, idx: int, r: float, lr: float = 0.12):
            self.Q[(user, idx)] += lr * (r - self.Q[(user, idx)])
            self.counts[(user, idx)] += 1

        def get_best_actions(self) -> dict:
            return {
                user: int(np.argmax([self.Q[(user, a)] for a in range(n_actions)]))
                for user in USER_TYPES
            }

        def copy_from(self, other):
            """Copy Q-values and counts from another agent (for warm start)."""
            self.Q = defaultdict(float, other.Q)
            self.counts = defaultdict(int, other.counts)

    # -------------------------------------------------------------------------
    # Helper: Run training episodes
    # -------------------------------------------------------------------------

    def train_agent(agent, n_episodes: int, rng, seed_offset: int = 0) -> list:
        """Train agent for n_episodes, return reward history."""
        rewards = []
        for ep in range(n_episodes):
            user = rng.choice(list(USER_TYPES.keys()))
            idx = agent.select_action(user, rng)
            action = actions[idx]
            interaction = simulate_user_interaction(
                USER_TYPES[user],
                rank_products(action[0], action[1]),
                seed=seed + ep + seed_offset,
            )
            r = compute_reward(interaction)
            agent.update(user, idx, r)
            rewards.append(r)
        return rewards

    # =========================================================================
    # Part A: Form Hypothesis
    # =========================================================================

    if verbose:
        print("\n" + "‚îÄ" * 70)
        print("üìù PART A ‚Äî Hypothesis")
        print("‚îÄ" * 70)
        print("""
Intuition suggests that local exploration‚Äîsmall perturbations around
our current best action‚Äîshould be more efficient than uniform random
sampling. After all, once we find a good region of action space, why
waste samples exploring far away?

  ‚Ä¢ Uniform exploration: When exploring, sample ANY action uniformly
  ‚Ä¢ Local exploration:   When exploring, sample NEIGHBORS of current best

Hypothesis: Local exploration converges faster because it exploits
            structure near good solutions (gradient descent intuition).

Let's test this.
""")

    # =========================================================================
    # Part B: Cold Start Experiment
    # =========================================================================

    if verbose:
        print("‚îÄ" * 70)
        print("üßä PART B ‚Äî Cold Start Experiment")
        print("‚îÄ" * 70)
        print(f"\nStarting BOTH agents from random initialization (Q=0 everywhere).")
        print(f"Training each for {n_episodes_cold} episodes...\n")

    cold_uniform_results = []
    cold_local_results = []

    for run in range(n_runs):
        rng = np.random.default_rng(seed + run)

        # Fresh agents (cold start)
        uniform_agent = UniformExplorationAgent()
        local_agent = LocalExplorationAgent()

        # Train both
        u_rewards = train_agent(uniform_agent, n_episodes_cold, rng, seed_offset=run * 10000)
        l_rewards = train_agent(local_agent, n_episodes_cold, rng, seed_offset=run * 10000 + 50000)

        cold_uniform_results.append(u_rewards)
        cold_local_results.append(l_rewards)

    cold_u_arr = np.array(cold_uniform_results)
    cold_l_arr = np.array(cold_local_results)

    cold_u_final = cold_u_arr[:, -100:].mean()
    cold_l_final = cold_l_arr[:, -100:].mean()
    cold_u_std = cold_u_arr[:, -100:].std()
    cold_l_std = cold_l_arr[:, -100:].std()

    if verbose:
        print("Results (averaged over {} runs):".format(n_runs))
        print(f"  {'Strategy':<12} {'Final Reward':>14} {'Std':>10}")
        print(f"  {'-'*12} {'-'*14} {'-'*10}")
        print(f"  {'Uniform':<12} {cold_u_final:>14.2f} {cold_u_std:>10.2f}")
        print(f"  {'Local':<12} {cold_l_final:>14.2f} {cold_l_std:>10.2f}")
        print()

        winner = "Uniform" if cold_u_final > cold_l_final else "Local"
        margin = abs(cold_u_final - cold_l_final) / max(cold_u_final, cold_l_final) * 100
        print(f"  Winner: {winner} (by {margin:.1f}%)")

        if cold_u_final > cold_l_final:
            print("""
  ‚ö†Ô∏è  SURPRISE! Uniform exploration wins decisively.
      Our hypothesis was WRONG. But why?
""")

    # =========================================================================
    # Part C: Diagnosis ‚Äî The Cold Start Problem
    # =========================================================================

    if verbose:
        print("‚îÄ" * 70)
        print("üîç PART C ‚Äî Diagnosis: The Cold Start Problem")
        print("‚îÄ" * 70)
        print("""
Why does local exploration fail from cold start?

The problem is INITIALIZATION. With Q=0 everywhere:
  ‚Ä¢ Uniform agent: Explores the ENTIRE action space randomly
  ‚Ä¢ Local agent:   Starts at action index 0 (the corner: w=(-1,-1))
                   and only explores NEIGHBORS of that corner!

The local agent is doing "local refinement of garbage"‚Äîthere's no
good region nearby to refine. It's stuck in a bad neighborhood.

This is the COLD START PROBLEM:
  Local exploration assumes you're already in a good basin.
  From random initialization, you're not.

Let's visualize which actions each agent tried (one run):
""")

        # Show action coverage for one run
        rng = np.random.default_rng(seed)
        uniform_demo = UniformExplorationAgent()
        local_demo = LocalExplorationAgent()
        train_agent(uniform_demo, 200, rng, seed_offset=99999)
        train_agent(local_demo, 200, rng, seed_offset=99998)

        # Count unique actions explored
        u_explored = set(k[1] for k in uniform_demo.counts.keys())
        l_explored = set(k[1] for k in local_demo.counts.keys())

        print(f"  After 200 episodes:")
        print(f"    Uniform explored {len(u_explored)}/{n_actions} actions ({100*len(u_explored)/n_actions:.0f}%)")
        print(f"    Local explored   {len(l_explored)}/{n_actions} actions ({100*len(l_explored)/n_actions:.0f}%)")
        print()
        print("  Local agent never discovered the optimal region!")

    # =========================================================================
    # Part D: Warm Start Experiment
    # =========================================================================

    if verbose:
        print("\n" + "‚îÄ" * 70)
        print("üî• PART D ‚Äî Warm Start Experiment")
        print("‚îÄ" * 70)
        print(f"""
If local exploration fails from cold start, when SHOULD it work?

Answer: After we've found a good region via global exploration!

Experiment design:
  1. Train with UNIFORM for {n_episodes_warmup} episodes (find good region)
  2. Then continue training with each strategy for {n_episodes_refine} more episodes
  3. Compare which strategy refines better from this warm start
""")

    warm_uniform_results = []
    warm_local_results = []

    for run in range(n_runs):
        rng = np.random.default_rng(seed + run + 1000)

        # Phase 1: Warm up with uniform exploration
        warmup_agent = UniformExplorationAgent()
        train_agent(warmup_agent, n_episodes_warmup, rng, seed_offset=run * 20000)

        # Phase 2: Continue with each strategy from warm start
        uniform_agent = UniformExplorationAgent()
        local_agent = LocalExplorationAgent()

        # Copy learned Q-values (warm start)
        uniform_agent.copy_from(warmup_agent)
        local_agent.copy_from(warmup_agent)

        # Train both from warm start
        u_rewards = train_agent(uniform_agent, n_episodes_refine, rng, seed_offset=run * 20000 + 10000)
        l_rewards = train_agent(local_agent, n_episodes_refine, rng, seed_offset=run * 20000 + 15000)

        warm_uniform_results.append(u_rewards)
        warm_local_results.append(l_rewards)

    warm_u_arr = np.array(warm_uniform_results)
    warm_l_arr = np.array(warm_local_results)

    warm_u_final = warm_u_arr[:, -100:].mean()
    warm_l_final = warm_l_arr[:, -100:].mean()
    warm_u_std = warm_u_arr[:, -100:].std()
    warm_l_std = warm_l_arr[:, -100:].std()

    if verbose:
        print("Results (averaged over {} runs, after {} warmup episodes):".format(n_runs, n_episodes_warmup))
        print(f"  {'Strategy':<12} {'Final Reward':>14} {'Std':>10}")
        print(f"  {'-'*12} {'-'*14} {'-'*10}")
        print(f"  {'Uniform':<12} {warm_u_final:>14.2f} {warm_u_std:>10.2f}")
        print(f"  {'Local':<12} {warm_l_final:>14.2f} {warm_l_std:>10.2f}")
        print()

        winner = "Uniform" if warm_u_final > warm_l_final else "Local"
        margin = abs(warm_u_final - warm_l_final) / max(warm_u_final, warm_l_final) * 100
        print(f"  Winner: {winner} (by {margin:.1f}%)")

        if warm_l_final >= warm_u_final * 0.95:  # Local is competitive or wins
            print("""
  ‚úì Local exploration is now COMPETITIVE (or wins)!
    Once we're in a good basin, local refinement works.
""")
        else:
            print("""
  Note: Local exploration improved significantly from warm start,
  even if uniform still edges ahead in this small action space.
""")

    # =========================================================================
    # Part E: Synthesis ‚Äî Adaptive Exploration
    # =========================================================================

    if verbose:
        print("‚îÄ" * 70)
        print("üéØ PART E ‚Äî Synthesis: Adaptive Exploration")
        print("‚îÄ" * 70)
        print("""
The key insight: EXPLORATION STRATEGY SHOULD ADAPT TO POLICY MATURITY.

Optimal approach:
  ‚Ä¢ Early training:  Uniform/global exploration (find good regions)
  ‚Ä¢ Late training:   Local exploration (refine within good regions)

This is exactly what sophisticated algorithms implement:

  ‚Ä¢ SAC (Soft Actor-Critic):
    Entropy bonus Œ±¬∑H(œÄ) encourages broad exploration early.
    As policy improves, entropy naturally decreases ‚Üí local refinement.

  ‚Ä¢ PPO (Proximal Policy Optimization):
    Entropy coefficient often DECAYED during training.
    High entropy early (explore) ‚Üí low entropy late (exploit).

  ‚Ä¢ Œµ-greedy schedules:
    Œµ decreases over time (e.g., 0.9 ‚Üí 0.05).
    Same principle: global early, local late.

  ‚Ä¢ Boltzmann exploration:
    Temperature œÑ decreases over training.
    High œÑ = uniform, low œÑ = local around best action.

CONNECTION TO THEORY:
  The cold start problem explains why "optimistic initialization"
  (starting with high Q-values) helps‚Äîit forces global exploration
  before settling into local refinement.

PRACTICAL GUIDELINE:
  When designing exploration strategies, ask:
  "Is my policy already in a good region?"
  If yes ‚Üí local refinement. If no ‚Üí global exploration first.
""")

    # =========================================================================
    # Summary Results
    # =========================================================================

    results = {
        "cold_start": {
            "uniform": {"mean": cold_u_final, "std": cold_u_std},
            "local": {"mean": cold_l_final, "std": cold_l_std},
            "winner": "uniform" if cold_u_final > cold_l_final else "local",
        },
        "warm_start": {
            "uniform": {"mean": warm_u_final, "std": warm_u_std},
            "local": {"mean": warm_l_final, "std": warm_l_std},
            "winner": "uniform" if warm_u_final > warm_l_final else "local",
        },
        "insight": "Exploration strategy effectiveness depends on policy quality",
        "cold_uniform_history": cold_u_arr,
        "cold_local_history": cold_l_arr,
        "warm_uniform_history": warm_u_arr,
        "warm_local_history": warm_l_arr,
    }

    if verbose:
        print("‚îÄ" * 70)
        print("üìä SUMMARY")
        print("‚îÄ" * 70)
        print(f"""
  | Experiment   | Uniform      | Local        | Winner  |
  |--------------|--------------|--------------|---------|
  | Cold start   | {cold_u_final:>10.2f}   | {cold_l_final:>10.2f}   | {results['cold_start']['winner']:<7} |
  | Warm start   | {warm_u_final:>10.2f}   | {warm_l_final:>10.2f}   | {results['warm_start']['winner']:<7} |

Key lesson: The same exploration strategy can WIN or LOSE depending
on whether the policy is cold (random) or warm (trained).

This is not a bug‚Äîit's a fundamental insight about RL exploration.
""")

    return results


# =============================================================================
# Exercise 0.3: Regret Analysis
# =============================================================================


def exercise_0_3_regret_analysis(
    n_train: int = 2000, seed: int = 42, verbose: bool = True
) -> dict:
    """
    Exercise 0.3: Track cumulative regret and verify sublinear scaling.
    """
    if verbose:
        print("=" * 70)
        print("Exercise 0.3: Regret Analysis")
        print("=" * 70)

    rng = np.random.default_rng(seed)
    actions = discretize_action_space(n_bins=5)

    if verbose:
        print("Computing oracle policy...")
    oracle = OraclePolicy(actions, n_eval=200, seed=seed)

    agent = TabularQLearning(actions, epsilon_init=1.0, epsilon_decay=0.998,
                              epsilon_min=0.02, learning_rate=0.1)

    cumulative_regret = []
    total_regret = 0.0

    if verbose:
        print("Running regret experiment...")

    for ep in range(n_train):
        user_name = rng.choice(list(USER_TYPES.keys()))
        user_type = USER_TYPES[user_name]

        # Oracle reward
        oracle_action = oracle.select_action(user_name, rng)
        oracle_ranking = rank_products(oracle_action[0], oracle_action[1])
        r_oracle = compute_reward(simulate_user_interaction(user_type, oracle_ranking, seed=seed+ep))

        # Agent reward
        agent_action = agent.select_action(user_name, rng)
        agent_ranking = rank_products(agent_action[0], agent_action[1])
        r_agent = compute_reward(simulate_user_interaction(user_type, agent_ranking, seed=seed+ep))
        agent.update(user_name, agent_action, r_agent)

        total_regret += max(0, r_oracle - r_agent)
        cumulative_regret.append(total_regret)

    regrets = np.array(cumulative_regret)
    T = len(regrets)
    episodes = np.arange(1, T + 1)

    # Curve fitting
    try:
        from scipy.optimize import curve_fit

        def power_model(t, C, alpha):
            return C * np.power(t, alpha)

        popt, _ = curve_fit(power_model, episodes, regrets, p0=[10, 0.5],
                            bounds=([0, 0], [10000, 1.5]))
        C_power, alpha_power = popt
    except Exception:
        C_power = regrets[-1] / np.sqrt(T)
        alpha_power = 0.5

    C_sqrt = regrets[-1] / np.sqrt(T)

    if verbose:
        print(f"\nRegret Analysis Summary:")
        print(f"  Total episodes: {T}")
        print(f"  Final cumulative regret: {regrets[-1]:.1f}")
        print(f"  Average regret per episode: {regrets[-1]/T:.3f}")
        print(f"  Is regret sublinear? {regrets[-1]/T < regrets[T//10]/(T//10)}")
        print(f"\nCurve fitting:")
        print(f"  ‚àöT model: Regret(T) ‚âà {C_sqrt:.1f} √ó ‚àöT")
        print(f"  Power model: Regret(T) ‚âà {C_power:.1f} √ó T^{alpha_power:.2f}")
        print(f"\nTheory comparison:")
        print(f"  Œµ-greedy (constant Œµ): O(T^0.67)")
        print(f"  Œµ-greedy (decaying Œµ): O(‚àöT log T) ‚Üí Œ± ‚âà 0.50")
        print(f"  Empirical Œ±: {alpha_power:.2f}")

    return {"regrets": regrets, "C_sqrt": C_sqrt, "C_power": C_power, "alpha": alpha_power}


# =============================================================================
# Exercise 0.4: Constrained Q-Learning with CM2 Floor
# =============================================================================


def exercise_0_4_constrained_qlearning(seed: int = 42, verbose: bool = True) -> dict:
    """
    Exercise 0.4: Add CM2 floor constraint and study GMV-CM2 tradeoff.

    Key finding: Per-episode constraints with high-variance outcomes are hard!
    """
    if verbose:
        print("=" * 70)
        print("Exercise 0.4: Constrained Q-Learning with CM2 Floor")
        print("=" * 70)

    class ConstrainedAgent:
        def __init__(self, actions, tau: float):
            self.actions = actions
            self.n_actions = len(actions)
            self.tau = tau
            self.Q: dict = defaultdict(float)
            self.CM2: dict = defaultdict(lambda: 10.0)  # Optimistic init

        def select_action(self, user: str, rng, eps: float = 0.15) -> int:
            feasible = [a for a in range(self.n_actions)
                        if self.CM2[(user, a)] >= self.tau]
            if not feasible:
                feasible = list(range(self.n_actions))
            if rng.random() < eps:
                return int(rng.choice(feasible))
            return feasible[int(np.argmax([self.Q[(user, a)] for a in feasible]))]

        def update(self, user: str, idx: int, r: float, cm2: float, lr: float = 0.1):
            self.Q[(user, idx)] += lr * (r - self.Q[(user, idx)])
            self.CM2[(user, idx)] += lr * (cm2 - self.CM2[(user, idx)])

    def run_experiment(tau: float, n_train: int = 1200):
        rng = np.random.default_rng(seed)
        actions = discretize_action_space(n_bins=5)
        agent = ConstrainedAgent(actions, tau)
        gmv_hist, cm2_hist = [], []

        for ep in range(n_train):
            user = rng.choice(list(USER_TYPES.keys()))
            idx = agent.select_action(user, rng)
            action = actions[idx]
            interaction = simulate_user_interaction(
                USER_TYPES[user], rank_products(action[0], action[1]), seed=seed+ep
            )
            agent.update(user, idx, compute_reward(interaction), interaction["cm2"])
            gmv_hist.append(interaction["gmv"])
            cm2_hist.append(interaction["cm2"])

        return {
            "tau": tau,
            "gmv": np.mean(gmv_hist[-200:]),
            "cm2": np.mean(cm2_hist[-200:]),
            "violations": sum(1 for c in cm2_hist[-200:] if c < tau) / 200,
        }

    if verbose:
        print("\nPareto Frontier (GMV vs CM2):")
        print("-" * 50)

    results = []
    for tau in [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0]:
        r = run_experiment(tau)
        results.append(r)
        if verbose:
            print(f"tau={tau:4.1f}: GMV={r['gmv']:5.2f}, CM2={r['cm2']:5.2f}, "
                  f"Violations={r['violations']*100:4.0f}%")

    if verbose:
        print("\nNote: Per-episode CM2 constraints don't produce clean Pareto frontier")
        print("due to high variance (CM2=0 when no purchase, CM2>>0 when purchase).")
        print("See Chapter 3, ¬ß3.6 for Lagrangian methods that handle this better.")

    return {"pareto": results}


# =============================================================================
# Exercise 0.5: Bandit-Bellman Bridge
# =============================================================================


def exercise_0_5_bandit_bellman_bridge(verbose: bool = True) -> dict:
    """
    Exercise 0.5: Verify that bandit Q-update equals MDP Q-update with Œ≥=0.
    """
    if verbose:
        print("=" * 70)
        print("Exercise 0.5: Bandit-Bellman Bridge")
        print("=" * 70)

    def bandit_update(Q: float, r: float, alpha: float) -> float:
        return (1 - alpha) * Q + alpha * r

    def mdp_update(Q: float, r: float, Q_next_max: float, alpha: float, gamma: float) -> float:
        return Q + alpha * (r + gamma * Q_next_max - Q)

    tests = [
        {"Q": 5.0, "r": 7.0, "alpha": 0.1},
        {"Q": 0.0, "r": 10.0, "alpha": 0.5},
        {"Q": -3.0, "r": 2.0, "alpha": 0.2},
        {"Q": 100.0, "r": 50.0, "alpha": 0.01},
    ]

    all_passed = True
    for i, tc in enumerate(tests):
        Q_b = bandit_update(tc["Q"], tc["r"], tc["alpha"])
        Q_m = mdp_update(tc["Q"], tc["r"], 1000.0, tc["alpha"], 0.0)
        diff = abs(Q_b - Q_m)
        passed = diff < 1e-10
        all_passed = all_passed and passed

        if verbose:
            print(f"\nTest {i+1}:")
            print(f"  Initial Q: {tc['Q']}, Reward: {tc['r']}, Œ±: {tc['alpha']}")
            print(f"  Bandit update: {Q_b:.6f}")
            print(f"  MDP update (Œ≥=0): {Q_m:.6f}")
            print(f"  Difference: {diff:.2e}")
            print(f"  {'‚úì PASSED' if passed else '‚úó FAILED'}")

    if verbose:
        print("\n" + "=" * 70)
        print("‚úì Verified: Bandit Q-update = MDP Q-update with Œ≥=0" if all_passed
              else "‚úó Verification failed!")
        print("=" * 70)

    return {"all_passed": all_passed}


# =============================================================================
# Main Entry Point
# =============================================================================


def run_all_exercises(verbose: bool = True) -> dict:
    """Run all exercises sequentially."""
    print("\n" + "=" * 70)
    print("CHAPTER 0 LAB SOLUTIONS ‚Äî COMPLETE RUN")
    print("=" * 70 + "\n")

    results = {
        "lab_0_1": lab_0_1_tabular_boost_search(verbose=verbose),
        "stress_test": exercise_stress_test_reward_weights(verbose=verbose),
        "ex_0_1": exercise_0_1_reward_sensitivity(verbose=verbose),
        "ex_0_2": exercise_0_2_action_geometry(verbose=verbose),
        "ex_0_3": exercise_0_3_regret_analysis(verbose=verbose),
        "ex_0_4": exercise_0_4_constrained_qlearning(verbose=verbose),
        "ex_0_5": exercise_0_5_bandit_bellman_bridge(verbose=verbose),
    }

    print("\n" + "=" * 70)
    print("ALL EXERCISES COMPLETED")
    print("=" * 70)

    return results


def main():
    """Main entry point with argument parsing."""
    parser = argparse.ArgumentParser(description="Chapter 0 Lab Solutions")
    parser.add_argument("--all", action="store_true", help="Run all exercises")
    parser.add_argument("--exercise", type=str,
                        help="Run specific exercise (e.g., 'lab0.1', '0.1', 'stress')")

    args = parser.parse_args()

    exercise_map = {
        "lab0.1": lab_0_1_tabular_boost_search,
        "lab01": lab_0_1_tabular_boost_search,
        "stress": exercise_stress_test_reward_weights,
        "0.1": exercise_0_1_reward_sensitivity,
        "01": exercise_0_1_reward_sensitivity,
        "0.2": exercise_0_2_action_geometry,
        "02": exercise_0_2_action_geometry,
        "0.3": exercise_0_3_regret_analysis,
        "03": exercise_0_3_regret_analysis,
        "0.4": exercise_0_4_constrained_qlearning,
        "04": exercise_0_4_constrained_qlearning,
        "0.5": exercise_0_5_bandit_bellman_bridge,
        "05": exercise_0_5_bandit_bellman_bridge,
    }

    if args.all:
        run_all_exercises()
    elif args.exercise:
        key = args.exercise.lower().replace("_", "").replace("-", "")
        if key in exercise_map:
            exercise_map[key]()
        else:
            print(f"Unknown exercise: {args.exercise}")
            print(f"Available: lab0.1, stress, 0.1, 0.2, 0.3, 0.4, 0.5")
            sys.exit(1)
    else:
        # Interactive menu
        print("\nCHAPTER 0 LAB SOLUTIONS")
        print("=" * 40)
        print("1. Lab 0.1  - Tabular Boost Search")
        print("2. Stress   - Reward Weight Sensitivity")
        print("3. Ex 0.1   - Reward Configurations")
        print("4. Ex 0.2   - Exploration Strategies")
        print("5. Ex 0.3   - Regret Analysis")
        print("6. Ex 0.4   - Constrained Q-Learning")
        print("7. Ex 0.5   - Bandit-Bellman Bridge")
        print("A. All      - Run everything")
        print()

        choice = input("Select (1-7 or A): ").strip().lower()
        choice_map = {
            "1": lab_0_1_tabular_boost_search,
            "2": exercise_stress_test_reward_weights,
            "3": exercise_0_1_reward_sensitivity,
            "4": exercise_0_2_action_geometry,
            "5": exercise_0_3_regret_analysis,
            "6": exercise_0_4_constrained_qlearning,
            "7": exercise_0_5_bandit_bellman_bridge,
            "a": run_all_exercises,
        }
        if choice in choice_map:
            choice_map[choice]()
        else:
            print(f"Invalid choice: {choice}")


if __name__ == "__main__":
    main()
