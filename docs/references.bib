@book{folland:real_analysis:1999,
  author    = {Gerald B. Folland},
  title     = {Real Analysis: Modern Techniques and Their Applications},
  edition   = {2nd},
  publisher = {Wiley},
  year      = {1999}
}

@book{durrett:probability:2019,
  author    = {Rick Durrett},
  title     = {Probability: Theory and Examples},
  edition   = {5th},
  publisher = {Cambridge University Press},
  year      = {2019}
}

@inproceedings{craswell:cascade:2008,
  author    = {Nick Craswell and Onno Zoeter and Michael Taylor and Bill Ramsey},
  title     = {An Experimental Comparison of Click Position-Bias Models},
  booktitle = {Proceedings of the International Conference on Web Search and Data Mining (WSDM)},
  year      = {2008}
}

@inproceedings{chapelle:position_bias:2009,
  author    = {Olivier Chapelle and Ya Zhang},
  title     = {A Simple Bayesian Method for Learning to Rank with Position Bias},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2009}
}

@inproceedings{wang:position_bias_contextual:2016,
  author    = {Xuanhui Wang and Nadav Golbandi and Michael Bendersky and Donald Metzler},
  title     = {Position Bias Estimation for Unbiased Learning to Rank in Personal Search},
  booktitle = {Proceedings of the International Conference on Web Search and Data Mining (WSDM)},
  year      = {2016}
}

% --- Policy Gradient Methods (Chapter 8) ---

@article{williams:simple_statistical:1992,
  author    = {Ronald J. Williams},
  title     = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  journal   = {Machine Learning},
  volume    = {8},
  number    = {3--4},
  pages     = {229--256},
  year      = {1992}
}

@inproceedings{sutton:policy_gradient:2000,
  author    = {Richard S. Sutton and David McAllester and Satinder Singh and Yishay Mansour},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2000}
}

@inproceedings{schulman:high_dimensional:2015,
  author    = {John Schulman and Philipp Moritz and Sergey Levine and Michael I. Jordan and Pieter Abbeel},
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2016}
}

@article{schulman:proximal_policy:2017,
  author    = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {arXiv preprint arXiv:1707.06347},
  year      = {2017}
}

@inproceedings{haarnoja:soft_actor_critic:2018,
  author    = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2018}
}

@inproceedings{chen:top_k_reinforce:2019,
  author    = {Minmin Chen and Alex Beutel and Paul Covington and Sagar Jain and Francoise Belletti and Ed H. Chi},
  title     = {Top-K Off-Policy Correction for a REINFORCE Recommender System},
  booktitle = {Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM)},
  pages     = {456--464},
  year      = {2019},
  note      = {YouTube's production REINFORCE recommender}
}

@article{ouyang:training_language:2022,
  author    = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and others},
  title     = {Training Language Models to Follow Instructions with Human Feedback},
  journal   = {arXiv preprint arXiv:2203.02155},
  year      = {2022},
  note      = {InstructGPT / RLHF methodology}
}
