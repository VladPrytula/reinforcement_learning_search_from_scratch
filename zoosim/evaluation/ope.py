"""
Off-Policy Evaluation (OPE) Module.

This module implements estimators for evaluating a target policy π_e using
logged data generated by a behavior policy π_b. It corresponds to Chapter 9
of the textbook.

Estimators implemented:
- IPS (Inverse Propensity Scoring)
- SNIPS (Self-Normalized IPS, also known as Weighted IS)
- PDIS (Per-Decision IS)
- DR (Doubly Robust)
- FQE (Fitted Q Evaluation)
"""

import numpy as np
import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple, Protocol, Union
from abc import ABC, abstractmethod

# --- Data Structures ---

@dataclass
class Transition:
    """Single transition (s, a, r, s')."""
    state: Dict[str, Any]  # Observation dict from env.step()
    action: np.ndarray     # Action vector
    reward: float
    next_state: Dict[str, Any]
    done: bool
    propensity: float      # π_b(a|s), logged

@dataclass
class Trajectory:
    """Complete episode trajectory."""
    transitions: List[Transition]
    return_: float         # Cached return G = Σ γ^t r_t
    length: int

    @property
    def importance_weight(self) -> float:
        """
        Compute trajectory-level IS weight.
        Note: This requires the policy context to be passed in, so we
        don't implement it as a property that computes itself without args.
        This is a placeholder for the concept.
        """
        return 0.0 # Logic moved to estimators

# --- Interfaces ---

class Policy(ABC):
    """Abstract policy interface for OPE."""

    @abstractmethod
    def act(self, state: Dict[str, Any]) -> np.ndarray:
        """Select action for given state."""
        pass

    @abstractmethod
    def prob(self, action: np.ndarray, state: Dict[str, Any]) -> float:
        """Return probability π(a|s)."""
        pass

class QFunction(Protocol):
    """Protocol for Q-functions used in DR and FQE."""
    
    def value(self, state: Dict[str, Any], policy: Policy) -> float:
        """Compute V(s) = E_{a~π(s)}[Q(s, a)]."""
        ...

    def q(self, state: Dict[str, Any], action: np.ndarray) -> float:
        """Compute Q(s, a)."""
        ...
        
    def forward(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:
        """Batch forward pass for training."""
        ...
        
    def parameters(self):
        """Return iterator over parameters."""
        ...

# --- Data Collection ---

def epsilon_greedy_logger(
    env: Any,
    behavior_policy: Policy,
    epsilon: float = 0.05,
    num_episodes: int = 1000,
    seed: int = 42
) -> List[Trajectory]:
    """
    Collect logged data with ε-greedy exploration.
    
    Args:
        env: The environment (Gym-like interface)
        behavior_policy: The policy π_b to execute
        epsilon: Probability of random action
        num_episodes: Number of episodes to collect
        seed: Random seed
        
    Returns:
        List of Trajectory objects
    """
    rng = np.random.default_rng(seed)
    dataset = []
    
    for _ in range(num_episodes):
        # Gym v0.26+ reset returns (obs, info)
        obs_res = env.reset()
        if isinstance(obs_res, tuple):
            obs = obs_res[0]
        else:
            obs = obs_res
            
        done = False
        transitions = []
        cumulative_reward = 0.0
        
        step_idx = 0
        while not done:
            # Epsilon-greedy selection
            # Note: We assume behavior_policy.act gives the 'greedy' or 'intended' action
            
            # Determine dimensionality
            greedy_action = behavior_policy.act(obs)
            action_dim = greedy_action.shape[0]
            
            # Uniform density over [-1, 1]^k is (1/2)^k
            uniform_density = (0.5 ** action_dim)
            
            if rng.random() < epsilon:
                # Random action
                if hasattr(env, 'action_space'):
                    action = env.action_space.sample()
                    if not isinstance(action, np.ndarray):
                        action = np.array(action)
                else:
                    action = rng.uniform(-1, 1, size=action_dim)
                
                # Mixture propensity
                pi_b_prob = behavior_policy.prob(action, obs)
                propensity = (1 - epsilon) * pi_b_prob + epsilon * uniform_density
            else:
                action = greedy_action
                pi_b_prob = behavior_policy.prob(action, obs)
                propensity = (1 - epsilon) * pi_b_prob + epsilon * uniform_density
                
            # Gym v0.26+ step returns (obs, reward, terminated, truncated, info)
            step_res = env.step(action)
            if len(step_res) == 5:
                next_obs, reward, terminated, truncated, _ = step_res
                done = terminated or truncated
            elif len(step_res) == 4:
                next_obs, reward, done, _ = step_res
            else:
                raise ValueError(f"Env step returned {len(step_res)} values, expected 4 or 5.")
            
            trans = Transition(
                state=obs,
                action=action,
                reward=reward,
                next_state=next_obs,
                done=done,
                propensity=propensity
            )
            transitions.append(trans)
            cumulative_reward += reward
            obs = next_obs
            step_idx += 1
            
        dataset.append(Trajectory(transitions=transitions, return_=cumulative_reward, length=step_idx))
        
    return dataset

# --- Estimators ---

def ips_estimate(
    dataset: List[Trajectory],
    pi_eval: Policy,
    pi_behavior: Optional[Policy] = None,
    gamma: float = 0.95,
    clip_weights: Optional[float] = None
) -> Tuple[float, Dict[str, Any]]:
    """
    Importance Sampling (IPS) estimator.
    Implements [EQ-9.10].
    """
    n = len(dataset)
    weights = []
    returns = []

    for traj in dataset:
        # Compute importance weight w(τ) = ∏_t π_e(a_t|s_t) / π_b(a_t|s_t)
        w = 1.0
        discounted_return = 0.0
        
        for i, trans in enumerate(traj.transitions):
            if pi_behavior is not None:
                prob_b = pi_behavior.prob(trans.action, trans.state)
            else:
                prob_b = trans.propensity  # Use logged propensity

            prob_e = pi_eval.prob(trans.action, trans.state)
            
            if prob_b < 1e-10:
                ratio = 0.0
            else:
                ratio = prob_e / prob_b
                
            w *= ratio
            discounted_return += (gamma ** i) * trans.reward

        if clip_weights is not None:
            w = min(w, clip_weights)

        weights.append(w)
        returns.append(discounted_return)

    weights = np.array(weights)
    returns = np.array(returns)

    estimate = np.mean(weights * returns)

    sum_weights_sq = np.sum(weights ** 2)
    effective_n = (np.sum(weights) ** 2) / sum_weights_sq if sum_weights_sq > 0 else 0.0
    
    diagnostics = {
        "weights_mean": np.mean(weights),
        "weights_std": np.std(weights),
        "weights_max": np.max(weights),
        "effective_sample_size": effective_n,
        "variance_estimate": np.var(weights * returns) / n
    }

    return estimate, diagnostics

def snips_estimate(
    dataset: List[Trajectory],
    pi_eval: Policy,
    pi_behavior: Optional[Policy] = None,
    gamma: float = 0.95,
    clip_weights: Optional[float] = None
) -> Tuple[float, Dict[str, Any]]:
    """
    Self-Normalized Importance Sampling (SNIPS).
    Implements [EQ-9.13].
    """
    weights = []
    returns = []

    for traj in dataset:
        w = 1.0
        discounted_return = 0.0
        
        for i, trans in enumerate(traj.transitions):
            prob_b = trans.propensity if pi_behavior is None else pi_behavior.prob(trans.action, trans.state)
            prob_e = pi_eval.prob(trans.action, trans.state)
            
            if prob_b < 1e-10:
                ratio = 0.0
            else:
                ratio = prob_e / prob_b
                
            w *= ratio
            discounted_return += (gamma ** i) * trans.reward

        if clip_weights is not None:
            w = min(w, clip_weights)

        weights.append(w)
        returns.append(discounted_return)

    weights = np.array(weights)
    returns = np.array(returns)

    sum_weights = np.sum(weights)
    if sum_weights > 0:
        estimate = np.sum(weights * returns) / sum_weights
    else:
        estimate = 0.0

    sum_weights_sq = np.sum(weights ** 2)
    diagnostics = {
        "weights_sum": sum_weights,
        "weights_mean": np.mean(weights),
        "weights_max": np.max(weights),
        "effective_sample_size": (sum_weights ** 2) / sum_weights_sq if sum_weights_sq > 0 else 0.0
    }

    return estimate, diagnostics

def pdis_estimate(
    dataset: List[Trajectory],
    pi_eval: Policy,
    pi_behavior: Optional[Policy] = None,
    gamma: float = 0.95
) -> Tuple[float, Dict[str, Any]]:
    """
    Per-Decision Importance Sampling (PDIS).
    Implements [EQ-9.16].
    """
    total = 0.0
    n = len(dataset)

    for traj in dataset:
        rho_product = 1.0
        traj_val = 0.0
        
        for t, trans in enumerate(traj.transitions):
            prob_b = trans.propensity if pi_behavior is None else pi_behavior.prob(trans.action, trans.state)
            prob_e = pi_eval.prob(trans.action, trans.state)
            
            if prob_b < 1e-10:
                rho_t = 0.0
            else:
                rho_t = prob_e / prob_b
                
            rho_product *= rho_t
            traj_val += (gamma ** t) * rho_product * trans.reward
            
        total += traj_val

    estimate = total / n

    diagnostics = {
        "num_trajectories": n,
        "avg_traj_length": np.mean([len(traj.transitions) for traj in dataset])
    }

    return estimate, diagnostics

def dr_estimate(
    dataset: List[Trajectory],
    pi_eval: Policy,
    q_function: QFunction,
    pi_behavior: Optional[Policy] = None,
    gamma: float = 0.95
) -> Tuple[float, Dict[str, Any]]:
    """
    Doubly Robust (DR) estimator.
    Implements [EQ-9.25].
    """
    estimates = []

    for traj in dataset:
        s_0 = traj.transitions[0].state
        v_0 = q_function.value(s_0, pi_eval)

        correction = 0.0
        rho_product = 1.0

        for t, trans in enumerate(traj.transitions):
            prob_b = trans.propensity if pi_behavior is None else pi_behavior.prob(trans.action, trans.state)
            prob_e = pi_eval.prob(trans.action, trans.state)
            
            if prob_b < 1e-10:
                rho_t = 0.0
            else:
                rho_t = prob_e / prob_b
                
            rho_product *= rho_t

            q_val = q_function.q(trans.state, trans.action)
            
            if not trans.done:
                v_next = q_function.value(trans.next_state, pi_eval)
            else:
                v_next = 0.0

            td_error = trans.reward + gamma * v_next - q_val
            correction += (gamma ** t) * rho_product * td_error

        estimates.append(v_0 + correction)

    estimate = np.mean(estimates)

    diagnostics = {
        "model_baseline_mean": np.mean([q_function.value(traj.transitions[0].state, pi_eval) for traj in dataset]),
        "correction_mean": np.mean([est - q_function.value(traj.transitions[0].state, pi_eval) for traj, est in zip(dataset, estimates)]),
        "estimates_std": np.std(estimates)
    }

    return estimate, diagnostics

# --- FQE & Model Helpers ---

class SimpleMLP(nn.Module):
    """Simple MLP for Q-function if Ensemble is not used."""
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 128]):
        super().__init__()
        layers = []
        input_dim = state_dim + action_dim
        
        for h in hidden_dims:
            layers.append(nn.Linear(input_dim, h))
            layers.append(nn.ReLU())
            input_dim = h
        
        layers.append(nn.Linear(input_dim, 1))
        self.net = nn.Sequential(*layers)
        
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        x = torch.cat([state, action], dim=-1)
        return self.net(x).squeeze(-1)

def fqe(
    dataset: List[Transition], 
    pi_eval: Policy,
    gamma: float = 0.95,
    num_iterations: int = 100,
    model_class: str = "ensemble",
    hidden_dims: List[int] = [128, 128],
    learning_rate: float = 1e-3
) -> Tuple[QFunction, Dict[str, Any]]:
    """
    Fitted Q Evaluation (FQE).
    """
    from zoosim.policies.q_ensemble import QEnsembleRegressor
    
    sample_state = dataset[0].state
    # Robust state feature extraction
    if isinstance(sample_state, dict) and "features" in sample_state:
        state_dim = sample_state["features"].shape[0]
        get_state_fn = lambda s: s["features"]
    else:
        # Assume it is already a vector
        state_dim = sample_state.shape[0]
        get_state_fn = lambda s: s

    action_dim = dataset[0].action.shape[0]

    q_func: QFunction
    if model_class == "ensemble":
        raw_q = QEnsembleRegressor(state_dim, action_dim, config=None) # Use default config
        
        class QEnsembleWrapper:
            def __init__(self, model):
                self.model = model
            
            def value(self, state: Dict[str, Any], policy: Policy) -> float:
                s_feat = torch.FloatTensor(get_state_fn(state))
                act = policy.act(state)
                act_t = torch.FloatTensor(act)
                # predict expects numpy batch
                s_np = s_feat.numpy()[None, :]
                a_np = act_t.numpy()[None, :]
                mean, _ = self.model.predict(s_np, a_np)
                return mean[0]

            def q(self, state: Dict[str, Any], action: np.ndarray) -> float:
                s_feat = torch.FloatTensor(get_state_fn(state))
                act_t = torch.FloatTensor(action)
                s_np = s_feat.numpy()[None, :]
                a_np = act_t.numpy()[None, :]
                mean, _ = self.model.predict(s_np, a_np)
                return mean[0]
                
            def forward(self, states, actions):
                # Training usually done via update_batch in QEnsembleRegressor
                # But here we want custom FQE loop?
                # QEnsembleRegressor has `_models`
                # Let's just implement manual forward for training
                x = torch.cat([states, actions], dim=1)
                preds_list = []
                for model in self.model._models:
                    preds = model(x).squeeze(-1)
                    preds_list.append(preds)
                # Return list of preds for loss calc?
                # Or return stack
                return torch.stack(preds_list, dim=0) 
                
            def parameters(self):
                # Iterate over all models
                params = []
                for model in self.model._models:
                    params.extend(list(model.parameters()))
                return iter(params)
        
        q_func = QEnsembleWrapper(raw_q)
        
    elif model_class == "mlp":
        raw_mlp = SimpleMLP(state_dim, action_dim, hidden_dims)
        
        class MLPWrapper:
            def __init__(self, model):
                self.model = model
                
            def value(self, state, policy):
                s_feat = torch.FloatTensor(get_state_fn(state))
                act = policy.act(state)
                act_t = torch.FloatTensor(act)
                with torch.no_grad():
                    return self.model(s_feat, act_t).item()
                    
            def q(self, state, action):
                s_feat = torch.FloatTensor(get_state_fn(state))
                act_t = torch.FloatTensor(action)
                with torch.no_grad():
                    return self.model(s_feat, act_t).item()

            def forward(self, states, actions):
                return self.model(states, actions)
                
            def parameters(self):
                return self.model.parameters()
        
        q_func = MLPWrapper(raw_mlp)
    else:
        raise ValueError(f"Unknown model_class: {model_class}")

    optimizer = torch.optim.Adam(q_func.parameters(), lr=learning_rate)
    losses = []

    states_np = np.array([get_state_fn(t.state) for t in dataset])
    actions_np = np.array([t.action for t in dataset])
    rewards_np = np.array([t.reward for t in dataset])
    next_states_np = np.array([get_state_fn(t.next_state) for t in dataset])
    dones_np = np.array([float(t.done) for t in dataset])
    
    states = torch.FloatTensor(states_np)
    actions = torch.FloatTensor(actions_np)
    rewards = torch.FloatTensor(rewards_np)
    next_states = torch.FloatTensor(next_states_np)
    dones = torch.FloatTensor(dones_np)

    for k in range(num_iterations):
        with torch.no_grad():
            next_acts_list = [pi_eval.act({ "features": next_states_np[i] } if isinstance(dataset[0].state, dict) else next_states_np[i]) for i in range(len(next_states))]
            next_actions = torch.FloatTensor(np.array(next_acts_list))
            
            # Target calculation
            # If ensemble, we get (n_ensemble, N)
            next_q_values = q_func.forward(next_states, next_actions)
            
            if model_class == "ensemble":
                # Average over ensemble for target (or keep distribution? simplified: average)
                next_q_mean = next_q_values.mean(dim=0)
                targets = rewards + gamma * (1 - dones) * next_q_mean
                # Expand targets to (n_ensemble, N) for training
                targets = targets.unsqueeze(0).expand_as(next_q_values)
            else:
                targets = rewards + gamma * (1 - dones) * next_q_values

        q_pred = q_func.forward(states, actions)
        loss = torch.mean((q_pred - targets) ** 2)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

    diagnostics = {
        "final_loss": losses[-1],
        "losses": losses,
        "num_iterations": num_iterations
    }

    return q_func, diagnostics


# --- Policy Wrappers ---

class QEnsemblePolicy(Policy):
    """Wrapper for Q-ensemble agent from Chapter 7."""

    def __init__(self, q_ensemble_agent, exploration_noise: float = 0.0):
        self.agent = q_ensemble_agent
        self.exploration_noise = exploration_noise

    def act(self, state: Dict[str, Any]) -> np.ndarray:
        # Check dimensionality and type
        if isinstance(state, dict):
            s_vec = state["features"]
        else:
            s_vec = state
            
        # Call select_action
        return self.agent.select_action(s_vec)

    def prob(self, action: np.ndarray, state: Dict[str, Any]) -> float:
        if self.exploration_noise == 0.0:
            optimal_action = self.act(state)
            if np.allclose(action, optimal_action, atol=1e-3):
                return 1.0
            else:
                return 1e-10
        else:
            optimal_action = self.act(state)
            diff = action - optimal_action
            log_prob = -0.5 * np.sum((diff / self.exploration_noise) ** 2)
            denom = ((2 * np.pi * self.exploration_noise ** 2) ** (len(action) / 2))
            return np.exp(log_prob) / denom

class REINFORCEPolicy(Policy):
    """Wrapper for REINFORCE agent from Chapter 8."""

    def __init__(self, reinforce_agent):
        self.agent = reinforce_agent

    def act(self, state: Dict[str, Any]) -> np.ndarray:
        if isinstance(state, dict):
            s_vec = state["features"]
        else:
            s_vec = state
            
        s_tensor = torch.as_tensor(s_vec, dtype=torch.float32, device=self.agent.device)
        with torch.no_grad():
            dist = self.agent.policy(s_tensor)
            action = dist.sample()
        return action.cpu().numpy()

    def prob(self, action: np.ndarray, state: Dict[str, Any]) -> float:
        if isinstance(state, dict):
            s_vec = state["features"]
        else:
            s_vec = state
            
        s_tensor = torch.as_tensor(s_vec, dtype=torch.float32, device=self.agent.device)
        action_tensor = torch.as_tensor(action, dtype=torch.float32, device=self.agent.device)
        
        with torch.no_grad():
            dist = self.agent.policy(s_tensor)
            log_prob = dist.log_prob(action_tensor).sum()
        return torch.exp(log_prob).item()
